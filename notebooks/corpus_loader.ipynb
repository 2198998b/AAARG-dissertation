{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading/Processing Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "# from tqdm.notebook import tqdm\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import warnings\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta File Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"/nfs/trects-kba2014-filtered\" # directory of corpus of gzipped html files\n",
    "topics_path = corpus_path + \"/test-topics.xml\"\n",
    "doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "topic_tags = ['id', 'title', 'description', 'start','end','query','type'] # topic fields\n",
    "test_file_addr = corpus_path + \"/1/2012-02-22-15.gz\"\n",
    "proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "csv_dir = proj_dir + '/' + 'load_data'\n",
    "# csv file addresses\n",
    "corp_csv_name = 'corpus_loaded.csv.gz'\n",
    "corp_csv_path = csv_dir + '/' + corp_csv_name\n",
    "topics_csv_name = 'topics_loaded.csv.gz'\n",
    "topics_csv_path = csv_dir + '/' + topics_csv_name\n",
    "# nugget/update dataframes\n",
    "nugget_dir = \"/nfs/TemporalSummarization/ts13/results\"\n",
    "updates_sampled_path = nugget_dir + \"/updates_sampled.tsv\"\n",
    "nuggets_path = nugget_dir + \"/nuggets.tsv\"\n",
    "nug_matches_path = nugget_dir + \"/matches.tsv\"\n",
    "# saving nugget and update files\n",
    "nugget_csv = 'nugget_df.csv.gz'\n",
    "update_csv = 'update_df.csv.gz'\n",
    "nugget_csv_path = csv_dir + '/' + nugget_csv\n",
    "update_csv_path = csv_dir + '/' + update_csv\n",
    "# supervised input/labels\n",
    "# supervised_csv = 'supervised_df.csv.gz'\n",
    "# supervised_csv_path = csv_dir + '/' + supervised_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePathHandler:\n",
    "    \"\"\"Paths will be in the format:\n",
    "    \n",
    "    proj_dir/dataset_dir/corpus_name/file_purpose/instance_identifier+split_identifier+sfile_type\n",
    "    \"\"\"\n",
    "    def __init__(self, proj_dir, dataset_dir=\"dataset\", compression='gzip'):\n",
    "        self.proj_dir = proj_dir\n",
    "        self.dataset_dir = proj_dir + '/' + dataset_dir\n",
    "        self.create_dir_if_not_exists(self.dataset_dir)\n",
    "        self.path_df_path = self.dataset_dir + '/' + 'file_path_df.csv.gz'\n",
    "        self.corpus_sources_pickle_path = self.dataset_dir + '/' + 'corpus_sources.pickle'\n",
    "        self.compression = compression\n",
    "        self.file_purposes = [\"topics\", \"corpus\", \"nuggets\", \"embed_labels\", \"updates\"]\n",
    "        self.path_df_cols = [\"corpus_name\", \"file_purpose\", \"split_identifier\", \"num_splits\",\n",
    "                            \"instance_identifier\", \"file_type\", \"path\", \"exists\"]\n",
    "        \n",
    "        # load meta files\n",
    "        self.load_corpus_sources()\n",
    "        self.load_path_df()\n",
    "\n",
    "\n",
    "    def paths_in_corpus_name(self, corpus_name, selection=None):\n",
    "        def get_identity_paths(ident_df, exists=True):\n",
    "            ident_paths = list(ident_df[ident_df['exists']==exists]['relative_path'])\n",
    "            if len(ident_paths) == 0:\n",
    "                ident_paths = None\n",
    "            return ident_paths\n",
    "        \n",
    "        if selection is None:  # get all file purposes if none selected\n",
    "            selection = self.file_purposes\n",
    "        \n",
    "        paths = {}\n",
    "        name_df = self.path_df[self.path_df['corpus_name'] == corpus_name]\n",
    "        for file_purpose in selection:  # loop for each file purpose/stage of generation\n",
    "            purp_df = name_df[name_df['file_purpose'] == file_purpose]\n",
    "            idents = purp_df['instance_identifier'].unique()  # segregate any unique identifiers/instances\n",
    "            if len(idents) == 0:\n",
    "                # case where corpus_name or file_purpose not in paths_df\n",
    "                # will create same dict with None entries\n",
    "                idents.append(None)  \n",
    "            for ident in idents:\n",
    "                ident_df = purp_df[purp_df['instance_identifier'] == ident]\n",
    "                paths[file_purpose][ident]['exists'] = get_identity_paths(ident_df, exists=True)\n",
    "                paths[file_purpose][ident]['not_exists'] = get_identity_paths(ident_df, exists=False)\n",
    "        return paths\n",
    "        \n",
    "    \n",
    "    def get_path(self, corpus_name, file_purpose, inst_identifier, file_type, add_path=True, exists=False,\n",
    "                split_identifier=None, num_splits=None):\n",
    "        # do check here make sure filename compatible, or elsewhere\n",
    "        path = self.dataset_dir + '/' + corpus_name + '/' + file_purpose + '/' + inst_identifier\n",
    "        if split_identifier is not None:\n",
    "            path += '_' + split_identifier\n",
    "        path += file_type\n",
    "        \n",
    "        if add_path:\n",
    "            self.add_path_to_df(corpus_name, file_purpose, split_identifier, num_splits, inst_identifier,\n",
    "                            file_type, path, exists, save=True)\n",
    "        return path\n",
    "            \n",
    "    def add_path_to_df(self, corpus_name, file_purpose, split_identifier, num_splits, inst_identifier,\n",
    "                       file_type, path, exists, save=True):\n",
    "        if not (self.path_df['path'] == path).any():  # check if row exists\n",
    "            # create appropriate dir if needed\n",
    "            new_dir_path = self.dataset_dir + '/' + corpus_name + '/' + file_purpose\n",
    "            self.create_dir_if_not_exists(new_dir_path)\n",
    "            # add to path_df\n",
    "            row = {\"corpus_name\":corpus_name, \"file_purpose\":file_purpose, \"split_identifier\":split_identifier,\n",
    "                  \"num_splits\":num_splits, \"inst_identifier\":inst_identifier, \"file_type\":file_type,\n",
    "                  \"path\":path, \"exists\":exists}\n",
    "            self.path_df = self.path_df.append(row, ignore_index=True)\n",
    "#             row = [corpus_name, file_purpose, split_identifier, num_splits, inst_identifier, file_type,\n",
    "#                   path, exists]\n",
    "#             self.path_df.loc[len(self.path_df)] = row\n",
    "            if save:  # save new path_df\n",
    "                self.save_path_df()\n",
    "        else:\n",
    "            warnings.warn(\"Path already exists in dataframe: \" + str(path))\n",
    "            \n",
    "    def update_path_exists(self, path, save=True):\n",
    "        self.path_df.loc[self.path_df['path'] == path, 'exists'] = True\n",
    "        if save:\n",
    "            self.save_path_df()\n",
    "        \n",
    "\n",
    "    def create_dir_if_not_exists(self, dir_path, warn=True):\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "            warnings.warn(\"Created new directory at \" + str(dir_path))\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def search_path_df(self, search_dict, df_slice=None):\n",
    "        if df_slice is None:\n",
    "            df_slice = self.path_df\n",
    "        for col_name, value in search_dict.items():\n",
    "            df_slice = df_slice[col_name == value]\n",
    "        return df_slice\n",
    "    \n",
    "    def path_exists(self, path):\n",
    "        return os.path.exists(path)\n",
    "    \n",
    "    def source_dict_correct(self, source_dict):\n",
    "        false_paths = []\n",
    "        for path_type, path in source_dict.items():\n",
    "            if path_type == \"corpus_name\":  # dict entry not a path, don't check\n",
    "                continue\n",
    "            if not self.path_exists(path):\n",
    "                false_paths.append(str(path_type) + \" does not exist at \" + str(path))\n",
    "        if len(false_paths) > 0:\n",
    "            error_str = \"\\n\".join(false_paths)\n",
    "            raise FileNotExistsError(error_str)\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def create_corpus_source_dict(self, corpus_name, dir_path, topics_file_path, nuggets_file_path):\n",
    "        s_dict = {\"corpus_name\":corpus_name, \"dir_path\":dir_path, \n",
    "                  \"topics_path\":topics_file_path, \"nuggets_path\":nuggets_file_path}\n",
    "        self.source_dict_correct(s_dict)\n",
    "        return p_dict\n",
    "    \n",
    "    def add_corpus_source(self, corpus_source_dict, overwrite=False):\n",
    "        \"\"\"Add a corpus directory to load from and its meta files\"\"\"\n",
    "        # check paths exist\n",
    "        self.source_dict_correct(corpus_source_dict)\n",
    "        corpus_name = copy.deepcopy(corpus_source_dict[\"corpus_name\"])\n",
    "        # store new entry\n",
    "        if corpus_name in self.corpus_sources and overwrite==False:\n",
    "            warnings.warn(str(corpus_name) + \" is already present in corpus source dictionary. \\n Proceeding with dict entry\")\n",
    "        else:\n",
    "            del corpus_source_dict[\"corpus_name\"]  # remove corpus_name from dict to add as a key\n",
    "            self.corpus_sources[corpus_name] = corpus_source_dict\n",
    "            # create folder for outputting new source files\n",
    "            new_corpus_dir = self.dataset_dir + '/' + corpus_name\n",
    "            self.create_dir_if_not_exists(new_corpus_dir)\n",
    "            self.save_corpus_sources()\n",
    "            \n",
    "    def get_corpus_sources(self, corpus_names=None):\n",
    "        \"\"\"Retrieve file paths from corpus_load dicts\n",
    "        Parameters:\n",
    "            corpus_names: list of corpus names retrieve, if None then retrieve all\n",
    "        \n",
    "        Returns:\n",
    "            A dictionary where keys are the corpus names and values are target file paths\n",
    "        \"\"\"\n",
    "        if corpus_names is None:\n",
    "            corpus_names = self.corpus_sources.keys()\n",
    "        corpus_paths = {}\n",
    "        for name in corpus_names:\n",
    "            corpus_paths[name] = self.corpus_sources[name]\n",
    "        return corpus_paths\n",
    "    \n",
    "    def save_corpus_sources(self):\n",
    "        with open(self.corpus_sources_pickle_path, 'wb') as handle:\n",
    "            pickle.dump(self.corpus_sources, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def load_corpus_sources(self):\n",
    "        if os.path.exists(self.corpus_sources_pickle_path):\n",
    "            with open(self.corpus_sources_pickle_path, 'rb') as handle:\n",
    "                self.corpus_sources = pickle.load(handle)\n",
    "            return True\n",
    "        else:\n",
    "            self.corpus_sources = {} # create empty dictionary\n",
    "            return False\n",
    "        \n",
    "    def save_path_df(self):\n",
    "        self.path_df.to_csv(self.path_df_path, compression=self.compression)\n",
    "        \n",
    "    def load_path_df(self):\n",
    "        \"\"\"File containing info about file paths to systematically load files\"\"\"\n",
    "        if os.path.exists(self.path_df_path):\n",
    "            self.path_df = pd.read_csv(self.path_df_path, compression=self.compression)\n",
    "            return True\n",
    "        else:\n",
    "            self.path_df = pd.DataFrame(columns=self.path_df_cols)  # create empty dataframe\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_path_f = FilePathHandler(proj_dir)\n",
    "# del test_path_f.corpus_sources['tr14_init_filtered']\n",
    "# print(test_path_f.corpus_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def supervised_path_generator(identifier, base=\"supervised_df\", handle=\".csv.gz\"):\n",
    "# #     fn = base + \"_\" + identifier + handle\n",
    "# #     path = csv_dir + '/' + fn\n",
    "# #     return path\n",
    "\n",
    "# def create_dir_if_not_exists(dir_path, warn=True):\n",
    "#     if not os.path.exists(dir_path):\n",
    "#         os.makedirs(dir_path)\n",
    "#         warnings.warn(\"Created new directory at \" + str(base_dir))\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# def save_path_generator(corpus_dir, file_purpose_dir, file_name, file_type, identifier=None, part=None):\n",
    "#     proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "#     base_dir = proj_dir + '/' + 'dataset'  # base folder for storing corpus files\n",
    "#     create_dir_if_not_exists(base_dir)\n",
    "#     # select appropriate corpus directory (e.g. trects-filtered-2014)\n",
    "#     path = base_dir + '/' + corpus_dir\n",
    "#     create_dir_if_not_exists(path)\n",
    "#     # ensure is from pre-selected file_purposes\n",
    "#     file_purposes = [\"corpus\", \"nuggets\", \"topics\", \"embed_labels\", \"updates\"]\n",
    "#     if file_purpose_dir not in file_purposes:\n",
    "#         raise ValueError(\"File purpose must be in defined file purposes\")\n",
    "#     path += '/' + file_purpose_dir\n",
    "#     create_dir_if_not_exists(path)\n",
    "    \n",
    "#     # create file name\n",
    "#     path += '/' + file_name\n",
    "#     if identifier is not None:\n",
    "#         path += '_' + identifier\n",
    "#     if part is not None:\n",
    "#         path += '_' + part\n",
    "#     path += file_type\n",
    "    \n",
    "#     return path\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markup Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and get beautifulsoup object from markup file\n",
    "def open_markup_file(addr, gz=True, xml=False, verbose=False):\n",
    "    markup = None\n",
    "    f = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(addr)\n",
    "\n",
    "    if gz:\n",
    "        f = gzip.open(addr)\n",
    "        if verbose:\n",
    "            print(\"gz file opened\")\n",
    "            print(\"first line: \" + str(f.readline())\n",
    "    else:\n",
    "        f = open(addr)\n",
    "        \n",
    "    if xml == False:\n",
    "        markup = bs(f, \"lxml\")  # using lxml parser for speed\n",
    "    else:\n",
    "        markup = bs(f, \"xml\")\n",
    "        \n",
    "    f.close()\n",
    "    return markup\n",
    "\n",
    "\n",
    "# parse markup and return 2D list [entry:tags]\n",
    "def parse_markup(markup, entry_list, tag_list, find_tag, topic_id=None):\n",
    "    for e in markup.find_all(find_tag):\n",
    "        entry = OrderedDict.fromkeys(tag_list)\n",
    "        if topic_id is not None:\n",
    "            entry['topic_id'] = topic_id\n",
    "        for c in e.children:  # children use direct children, descendants uses all\n",
    "            if c.name in entry:\n",
    "                entry[c.name] = str(c.string)\n",
    "            elif c.name is None and c.string != '\\n':  # inner body of <doc> tag\n",
    "                entry['text'] = str(c.string)\n",
    "        entry_list.append(list(entry.values()))\n",
    "        \n",
    "            \n",
    "# recursively find gz html files from a directory address\n",
    "def search_dir(path):    \n",
    "    # separate the subdirectories and html files \n",
    "    # (help maintain sequential order of insertion)\n",
    "    gz_paths = []\n",
    "    for f in os.scandir(path):\n",
    "        if os.path.splitext(f.path)[-1].lower() == \".gz\":\n",
    "            gz_paths.append(f.path)\n",
    "    \n",
    "    return gz_paths\n",
    "\n",
    "\n",
    "def list_to_dataframe(markup_list, tags):\n",
    "    return pd.DataFrame(markup_list, columns=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(df):\n",
    "    seen = set()\n",
    "    seen_twice = set()\n",
    "    for docid in df['docid']:\n",
    "        if docid not in seen:\n",
    "            seen.add(docid)\n",
    "        else:\n",
    "            seen_twice.add(docid)\n",
    "    return seen_twice\n",
    "\n",
    "def file_exists(path):\n",
    "    return os.path.isfile(path)\n",
    "\n",
    "def load_df_control(saved_path, load_func, save=True, force_reload=False, compression='gzip', \n",
    "                    name=None, verbose=True, path_handler=None):\n",
    "    df = None\n",
    "    if name is not None and verbose:\n",
    "        print(\"Loading \" + name)\n",
    "    if not file_exists(saved_path) or force_reload:\n",
    "        df = load_func()\n",
    "        if verbose:\n",
    "            print(\"df loaded\")\n",
    "        if save:\n",
    "            df.to_csv(saved_path, compression=compression)\n",
    "            if path_handler is not None:\n",
    "                path_handler.update_path_exists(saved_path)\n",
    "            if verbose:\n",
    "                print(\"saved at: \" + str(saved_path))\n",
    "    else:\n",
    "        df = pd.read_csv(saved_path, compression=compression)\n",
    "        if verbose:\n",
    "            print(\"loaded from file\")\n",
    "    if verbose:\n",
    "        print(display(df[0:4]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes from Corpus Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topics into dataframe\n",
    "def __load_topics(path):\n",
    "    topics_list = []\n",
    "    parse_markup(open_markup_file(path, gz=False, xml=True), \n",
    "                    topics_list, topic_tags, \"event\")\n",
    "    df = list_to_dataframe(topics_list, topic_tags)\n",
    "    df['id'] = pd.to_numeric(df['id'])\n",
    "    return df\n",
    "\n",
    "def load_topics(saved_path, load_path=None, save=True, force_reload=False, verbose=True, path_handler=None):\n",
    "    topics = load_df_control(saved_path, lambda: __load_topics(load_path), \n",
    "                             save=save, force_reload=force_reload, name=\"topics\", verbose=verbose, path_handler=path_handler)\n",
    "    return topics\n",
    "\n",
    "# topics = load_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Corpus Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all formatted gzipped html files into dataframe\n",
    "\n",
    "def __load_corpus(corpus_dir, doc_tags=None, topic_ids=None, split_every=None, split_start_doc=None):\n",
    "    if doc_tags is None:\n",
    "        doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "    df = pd.DataFrame(columns=doc_tags)\n",
    "    \n",
    "    for topic_id in topic_ids:\n",
    "        print(\"Loading topic \" + str(topic_id) + \"...\")\n",
    "        topic_list = []\n",
    "        topic_path = corpus_dir + '/' + str(topic_id)\n",
    "        gz_paths = search_dir(topic_path)\n",
    "        \n",
    "        if split_every is not None and split_start_doc is not None:\n",
    "            end_split = split_start_doc + split_every\n",
    "            if end_split >= len(gz_paths):  # last section\n",
    "                end_split = len(gz_paths) - 1\n",
    "            gz_paths = gz_paths[split_start_doc:end_split]\n",
    "        \n",
    "        print(\"creating corpus df for topic \" + str(topic_id) + \" starting at file no. \" + str(split_start_doc)\n",
    "             + \" splitting every \" + str(split_every) + \" docs\")\n",
    "        for gz_path in tqdm(gz_paths, position=0, leave=True):\n",
    "            parse_markup(open_markup_file(gz_path, verbose=False),\n",
    "                             topic_list, doc_tags, \"doc\", topic_id=topic_id)\n",
    "        topic_df = list_to_dataframe(topic_list, doc_tags)\n",
    "        df = df.append(topic_df)\n",
    "    df['epoch'] = pd.to_numeric(df['epoch'])\n",
    "    return df\n",
    "\n",
    "def load_corpus(save_path, corpus_dir=None, doc_tags=None, topic_ids=None, split_every=None, split_start_doc=None,\n",
    "                save=True, force_reload=False, verbose=True, path_handler=None):\n",
    "    \n",
    "    corpus = load_df_control(save_path, \n",
    "                             lambda: __load_corpus(corpus_dir, doc_tags=doc_tags, \n",
    "                                                   topic_ids=topic_ids, split_every=split_every,\n",
    "                                                   split_start_doc=split_start_doc), \n",
    "                             save=save, force_reload=force_reload, name=\"corpus\", verbose=verbose, path_handler=path_handler)\n",
    "    if verbose:\n",
    "        print(\"Corpus loaded succesfully: \" + str(len(corpus)) + \" documents loaded.\")\n",
    "    return corpus\n",
    "\n",
    "# corpus = load_corpus(doc_tags=doc_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuggets (Evaluation Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_nugget_df():\n",
    "    \"\"\"Dataframe containing nugget data and its appearances in corpus\"\"\"\n",
    "    def create_entry(row, reg_cols, multi_col_vals=None):\n",
    "        entry_dict = {}\n",
    "        for col in reg_cols:\n",
    "            entry_dict[col] = row[col]\n",
    "        if multi_cols is not None:\n",
    "            for k,v in multi_col_vals.items():\n",
    "                entry_dict[k] = v\n",
    "        return entry_dict\n",
    "    nuggets_tsv = pd.read_csv(nuggets_path, \"\\t\")\n",
    "    entry_list = []\n",
    "    reg_cols = ['query_id', 'nugget_id', 'importance', 'nugget_len', 'nugget_text']\n",
    "    multi_cols = ['docid', 'streamid', 'epoch', 'yyyymmddhh']  # multiindex cols\n",
    "    num_cols = ['query_id', 'importance', 'nugget_len', 'epoch']\n",
    "    \n",
    "    pbar = tqdm(total=len(nuggets_tsv), position=0, leave=True)\n",
    "    for index, row in nuggets_tsv.iterrows():\n",
    "        # find where nugget appears in text\n",
    "        nug_text = row['nugget_text']\n",
    "        topic_id = 0\n",
    "        try:\n",
    "            topic_id = int(row['query_id'])  # make sure pattern match in correct topic\n",
    "        except ValueError:\n",
    "            pbar.update()\n",
    "            continue  # topic_id is unknown string in tsv file, e.g. \"TS13.07\"\n",
    "        appears = corpus[corpus['topic_id'] == topic_id]\n",
    "        appears = appears[appears['text'].str.contains(re.escape(nug_text))]  # make sure no accidental regex pattern\n",
    "        \n",
    "        # gather information on docs it appears in\n",
    "        dups = find_duplicates(appears)  # get docids where nugget appears\n",
    "        for docid in dups:\n",
    "            upd = appears[appears['docid'] == docid]  # get docs with this docid\n",
    "            for i, r in upd.iterrows():  # gather info on each doc with this docid (e.g. streamid, epoch etc.)\n",
    "                multi_col_vals = {}\n",
    "                for multi_col in multi_cols:\n",
    "                    multi_col_vals[multi_col] = r[multi_col]\n",
    "                entry = create_entry(row, reg_cols, multi_col_vals=multi_col_vals)\n",
    "                entry_list.append(entry)\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "    \n",
    "    # form multi-index nugget dataframe\n",
    "    reg_cols.extend(multi_cols)  # get new multiindex order\n",
    "    nugget_df = pd.DataFrame(entry_list)\n",
    "    nugget_df[num_cols] = nugget_df[num_cols].apply(pd.to_numeric, errors='coerce', axis=1)  # convert appropriate cols to numerical values\n",
    "    nugget_df.rename(columns={'query_id':'topic_id'}, inplace=True)  # topic_id matches other dataframes\n",
    "    return nugget_df\n",
    "\n",
    "def load_nugget_df(save=True, force_reload=False, verbose=True):\n",
    "    nugget_df = load_df_control(nugget_csv_path, create_nugget_df, \n",
    "                                save=save, force_reload=force_reload, name=\"nugget_df\", verbose=verbose)\n",
    "    return nugget_df\n",
    "\n",
    "# nugget_df = load_nugget_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Dataframe (Temporal Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_update_df():\n",
    "    \"\"\"Data Frame containing information about docs which have updates/multiple instances in corpus\"\"\"\n",
    "    def create_entry(row, col_tags):\n",
    "        entry = {}\n",
    "        for col in col_tags:\n",
    "            entry[col] = row[col]\n",
    "        return entry\n",
    "    \n",
    "    col_tags = ['docid', 'streamid', 'epoch', 'yyyymmddhh', 'zulu']\n",
    "    entry_list = []\n",
    "    dups = find_duplicates(corpus)\n",
    "    for docid in tqdm(dups, position=0, leave=True):\n",
    "        d = corpus[corpus['docid'] == docid]\n",
    "        for index, row in d.iterrows():\n",
    "            entry = create_entry(row, col_tags)\n",
    "            entry_list.append(entry)\n",
    "             \n",
    "    update_df = pd.DataFrame(entry_list)\n",
    "    update_df = update_df.set_index(col_tags)\n",
    "    return update_df\n",
    "\n",
    "def load_update_df(save=True, force_reload=False, verbose=True):\n",
    "    update_df = load_df_control(update_csv_path, create_update_df, \n",
    "                                save=save, force_reload=force_reload, name=\"update_df\", verbose=verbose)\n",
    "    return update_df\n",
    "\n",
    "# update_df = load_update_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Input Data (Embeddings) / Labels (Model Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess corpus into cleaned sentences\n",
    "# create sentence embeddings of corpus text\n",
    "# create embeddings from where nuggets appear in article\n",
    "# match them together in df\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedTrainingGenerator:\n",
    "    \"\"\"Currently not matching nuggets to correct sentence_id\"\"\"\n",
    "    def __init__(self, spacy_model_selector=\"en_core_web_sm\"):\n",
    "        self.sent_model = self.init_sent_model()\n",
    "        self.spacy_model_selector = spacy_model_selector\n",
    "        self.nlp = None\n",
    "        \n",
    "    def generate(self, corpus_df, nugget_df, topic_ids=None, save=True, force_reload=False, verbose=True):\n",
    "        if topic_ids is not None:\n",
    "            self.supervised_df = {}\n",
    "            for topic_id in topic_ids:\n",
    "                print(\"Processing topic \" + str(topic_id))\n",
    "                t_corpus = corpus_df[corpus_df['topic_id'] == topic_id]\n",
    "                t_nugget = nugget_df[nugget_df['topic_id'] == topic_id]\n",
    "                t_path = supervised_path_generator(\"topic\" + str(topic_id))\n",
    "                self.supervised_df[topic_id] = load_df_control(t_path, \n",
    "                                                lambda: self.__generate(t_corpus, t_nugget), \n",
    "                                                save=save, force_reload=force_reload, \n",
    "                                                verbose=verbose, name=\"supervised_df\" + str(topic_id))\n",
    "                self.sent_model = self.init_sent_model()\n",
    "        else:\n",
    "            # maybe check here load different csv with different topic_ids\n",
    "            self.supervised_df = load_df_control(supervised_csv_path, \n",
    "                                                lambda: self.__generate(corpus_df, nugget_df), \n",
    "                                                save=save, force_reload=force_reload, \n",
    "                                                verbose=verbose, name=\"supervised_df\")\n",
    "        return self.supervised_df\n",
    "    \n",
    "    def __generate(self, corpus_df, nugget_df):\n",
    "        # create df each row being a sentence, it's embedding, sent_id, is_nugget, nugget_text, topic_id, streamid, docid etc.\n",
    "        supervised = []\n",
    "        for index, article in tqdm_notebook(corpus_df.iterrows(), total=corpus_df.shape[0], position=0, leave=True):\n",
    "            # preprocess sentences\n",
    "            sentences = self.preprocess_text(article['text'])\n",
    "            sent_ids, sentences, embeddings = self.sent_embeddings(sentences)\n",
    "            \n",
    "            # if nuggets in article, get the index of the sentence\n",
    "            streamid = article['streamid']\n",
    "            article_nugs = self.nugget_matching_sent(streamid, nugget_df, sentences)\n",
    "            \n",
    "            # create dictionary for later creating dataframe\n",
    "            for sent_id, sent, emb in zip(sent_ids, sentences, embeddings):\n",
    "                t_id = article['topic_id']\n",
    "                epoch = article['epoch']\n",
    "                is_nugget = False\n",
    "                nugget_text = None\n",
    "                nugget_id = None\n",
    "                \n",
    "                nug_dict_index = None\n",
    "                # check if nugget\n",
    "                try:\n",
    "                    nug_dict_index = article_nugs['sent_id'].index(sent_id)  # throws if not in list\n",
    "                    is_nugget = True\n",
    "                    nugget_text = article_nugs['nugget_text'][nug_dict_index]\n",
    "                    nugget_id = article_nugs['nugget_id'][nug_dict_index]\n",
    "                except ValueError:\n",
    "                    pass # current sentence is not nugget\n",
    "                \n",
    "                s_dict = {\"topic_id\":t_id, \"streamid\":streamid, \"epoch\":epoch, \"sent_id\":sent_id, \n",
    "                          \"sentence\":sent, \"embedding\":emb, \"is_nugget\":is_nugget, \n",
    "                          \"nugget_id\":nugget_id, \"nugget_text\":nugget_text}\n",
    "                supervised.append(s_dict)\n",
    "                \n",
    "        supervised_df = pd.DataFrame(supervised)\n",
    "        return supervised_df\n",
    "                \n",
    "                \n",
    "    def nugget_matching_sent(self, streamid, nugget_df, sentences):\n",
    "        # find sent index of where nugget appears in text\n",
    "        matches = {\"sent_id\":[], \"nugget_text\":[], \"nugget_id\":[]}\n",
    "        for index, nug in self.nuggets_in_article(streamid, nugget_df).iterrows():\n",
    "            match = None\n",
    "            nug_text = nug['nugget_text']\n",
    "            for i in range(len(sentences)):\n",
    "                match = None\n",
    "                if sentences[i] in nug_text:\n",
    "                    match = i  # only take first appearance in article if multiple exist\n",
    "                    break\n",
    "            if match is not None:\n",
    "                matches[\"sent_id\"].append(match)\n",
    "                matches[\"nugget_text\"].append(nug_text)\n",
    "                matches['nugget_id'].append(nug['nugget_id'])\n",
    "        return matches\n",
    "        \n",
    "                \n",
    "    def nuggets_in_article(self, streamid, nugget_df):\n",
    "        # find streamid in nugget_df\n",
    "        nug_rows = nugget_df[nugget_df['streamid'] == streamid]\n",
    "        return nug_rows\n",
    "                \n",
    "            \n",
    "    def preprocess_text(self, text, use_spacy=False):\n",
    "        # remove first char if \\n\n",
    "#         if text[:1] == \"\\n\":\n",
    "#             text = text[1:]\n",
    "        \n",
    "        sentences = None\n",
    "        if spacy:\n",
    "            if self.nlp is None:\n",
    "                self.nlp = spacy.load(self.spacy_model_selector)\n",
    "            text = self.nlp(text)\n",
    "            sentences = list(text.sents)\n",
    "            sentences = [s.text for s in sentences if len(s) != 0]\n",
    "        else:  # split by newline\n",
    "            sentences = text.splitlines()\n",
    "        return sentences\n",
    "        \n",
    "    def sent_embeddings(self, sentences):\n",
    "        # use sentence-transformers embeddings\n",
    "        result = self.sent_model.encode(sentences, show_progress_bar=False)\n",
    "        sent_ids = []\n",
    "        tokens = []  # sentences as text\n",
    "        embeddings = []\n",
    "        for i, (tok, emb) in enumerate(zip(sentences,result)):\n",
    "            sent_ids.append(i)\n",
    "            tokens.append(tok)\n",
    "            embeddings.append(emb)\n",
    "#         embeddings = np.stack(embeddings)\n",
    "        return sent_ids, tokens, embeddings\n",
    "#         # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
    "#         self.normed_embeddings = (embeddings.T / (embeddings**2).sum(axis=1) ** 0.5).T\n",
    "\n",
    "    def init_sent_model(self):\n",
    "        sent_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "        return sent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super_gen = SupervisedTrainingGenerator()\n",
    "# super_df = super_gen.generate(corpus, nugget_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(display_df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sli = pd.DataFrame.copy(super_gen.supervised_df[1][0:5], deep=True)\n",
    "# # testees = sli['nugget_text']\n",
    "# # testees[2] = \"poop\"\n",
    "# # sli['nugget_text'] = testees\n",
    "# # print(sli['nugget_text'].unique())\n",
    "# e_df_test = pd.DataFrame(columns=sli.columns)\n",
    "# print(list(e_df_test[e_df_test['epoch']==None]['topic_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the Larger Trects Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrectsFilter:\n",
    "    def __init__(self):\n",
    "        self.base_dir = '/nfs/trects-kba2014'\n",
    "        self.updates_dir = \"/nfs/TemporalSummarization/ts14/results\"\n",
    "        self.updates_csv_path = self.updates_dir + '/' + \"updates_sampled.extended.tsv\"  # using extended version\n",
    "        self.save_dir = '/nfs/trects-kba2014-filtered-mine'\n",
    "        self.proc_history_path = self.save_dir + '/' + 'process_history.pickle'\n",
    "        self.proc_history = None\n",
    "#         self.parser = etree.HTMLParser()\n",
    "#         self.streamid_strainer = SoupStrainer(\"streamid\")  # quick search for streamid tags\n",
    "        \n",
    "        \n",
    "    def create_filtered_dataset(self, force_reload=False, verbose=True):\n",
    "        \"\"\" Outline of Process\n",
    "        1. Find streamids\n",
    "            1.1 open updates_sampled.tsv file (or updates_sampled.extended.tsv)\n",
    "            1.2 Scrape update_id column\n",
    "            1.3 transform into streamid (drop last hyphenated numbers (these are sentenceids))\n",
    "            1.4 Put streamids into datastructure for comparing (e.g. set)\n",
    "        2. Create a new directory for each topic folder there is in target dir\n",
    "        3. Opening up documents\n",
    "            3.1 Go for each topic folder\n",
    "            3.2 Open up each document\n",
    "            3.3 Parse into html tree\n",
    "            3.4 if streamid matches, store locally in memory buffer\n",
    "            3.5 when buffer is size of however many docs are in other html.gz files, print those docs to a new file\n",
    "            3.6 name this file something like a number, not dates like other files, save with same file extension/format\n",
    "        4. Quick test\n",
    "            4.1 Open up a topicid using load corpus\n",
    "        5. Final check\n",
    "            5.1 Add in final check that will only run this if it hasn't already been generated\n",
    "        \"\"\"\n",
    "        # get streamids for docs that we will filter for\n",
    "        self.streamids = self.get_streamids()\n",
    "\n",
    "        # get topicids from folder names\n",
    "        topic_ids = [int(tid) for tid in os.listdir(self.base_dir) if tid.isdigit()]\n",
    "        topic_ids.sort()\n",
    "        \n",
    "        # create dir to save filtered corpus to\n",
    "        self.create_dir(self.save_dir)\n",
    "        \n",
    "        # load history of files already processed if exists\n",
    "        self.load_process_history_dict(topic_ids)\n",
    "        \n",
    "        \n",
    "        for topic_id in tqdm(topic_ids, position=0, leave=True):\n",
    "            # create save directory\n",
    "            topic_dir = self.base_dir + '/' + str(topic_id)\n",
    "            self.create_dir(topic_dir)\n",
    "            \n",
    "            # get paths for files in target topic dir\n",
    "            gz_paths = search_dir(topic_dir)\n",
    "            \n",
    "            # remove already processed files\n",
    "            if not force_reload:\n",
    "                gz_paths = [x for x in gz_paths if x not in self.proc_history[topic_id]]\n",
    "            \n",
    "            # process each file\n",
    "            for gz_path in tqdm(gz_paths, position=1, leave=True):\n",
    "                # get file markup\n",
    "                markup = open_markup_file(gz_path, verbose=verbose)  # 50MB file proving hard for beautifulsoup\n",
    "                # get docs in file that are in streamids\n",
    "                matches = self.retrieve_matching_docs(markup, verbose=verbose)\n",
    "                save_path = self.get_file_save_path(topic_id, gz_path)\n",
    "                # write file and save results\n",
    "                self.write_docs_to_file(matches, save_path, verbose=verbose)\n",
    "                self.proc_history[topic_id].add(gz_path)\n",
    "                self.save_process_history_dict()\n",
    "        print(\"Finished filtering corpus\")\n",
    "        \n",
    "    def save_process_history_dict(self):\n",
    "        with open(self.proc_history_path, 'wb') as handle:\n",
    "            pickle.dump(self.proc_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    def load_process_history_dict(self, topic_ids):\n",
    "        if os.path.exists(self.proc_history_path):\n",
    "            with open(self.proc_history_path, 'rb') as handle:\n",
    "                self.proc_history = pickle.load(handle)\n",
    "            return True\n",
    "        else:\n",
    "            self.proc_history = self.create_process_history_dict(topic_ids)\n",
    "            return False\n",
    "        \n",
    "    def create_process_history_dict(self, topic_ids):\n",
    "        \"\"\"Create a dictionary to keep track of what files have already been searched\"\"\"\n",
    "        proc_history = {}\n",
    "        for topic_id in topic_ids:\n",
    "            proc_history[int(topic_id)] = set()  # sets have faster indexing\n",
    "        return proc_history\n",
    "\n",
    "                \n",
    "    def get_file_save_path(self, topic_id, gz_path):\n",
    "        filename = self.get_filename_from_gz_path(gz_path)\n",
    "        save_path = self.save_dir + '/' + str(topic_id) + '/' + filename\n",
    "        return save_path\n",
    "        \n",
    "                \n",
    "    def get_filename_from_gz_path(self, gz_path):\n",
    "        split = gz_path.split(\"/\")\n",
    "        filename = split[-1]\n",
    "        return filename  # return with file extension on\n",
    "                \n",
    "\n",
    "    def write_docs_to_file(self, doc_list, save_path, verbose=True):\n",
    "        # transform docs into string\n",
    "        if len(doc_list) > 0:  # don't write empty files\n",
    "            out = \"\\n\".join(list(map(str, doc_list)))\n",
    "            # write\n",
    "            with gzip.open(save_path, \"wt\") as f:\n",
    "                f.write(out)\n",
    "                if verbose:\n",
    "                    print(\"File written to: \" + str(save_path))\n",
    "        \n",
    "            \n",
    "    def retrieve_matching_docs(self, markup, verbose=False):\n",
    "        \"\"\"Retrieve docs with matching streamids from markup\"\"\"\n",
    "        matches = []\n",
    "        doc_count = 0\n",
    "        match_count = 0\n",
    "        for doc in markup.find_all(\"doc\"):\n",
    "            d_streamid = str(doc.find(\"streamid\").string)\n",
    "            if d_streamid in self.streamids:  # matching doc\n",
    "                matches.append(doc)\n",
    "                match_count += 1\n",
    "            doc_count +=1\n",
    "        if verbose:\n",
    "            print(\"doc count: \" + str(doc_count) + \"\\nmatch_count: \" + str(match_count))\n",
    "        return matches\n",
    "\n",
    "    def get_streamids(self):\n",
    "        # read tsv file\n",
    "        updates_csv = pd.read_csv(self.updates_csv_path, \"\\t\")\n",
    "        # take column with streamids\n",
    "        updateids = list(updates_csv['update_id'])\n",
    "        streamids = set()  # can do lookups in constant time\n",
    "        for updateid in updateids:\n",
    "            streamid = self.parse_streamid(updateid)\n",
    "            streamids.add(streamid)\n",
    "        return streamids\n",
    "        \n",
    "    def parse_streamid(self, updateid):\n",
    "        \"\"\"Convert updateid in format: epoch-docid-sentid into epoch-docid\"\"\"\n",
    "        split = updateid.split(\"-\")\n",
    "        split = split[:-1]  # remove sentid from end\n",
    "        streamid = \"-\".join(split)\n",
    "        return streamid\n",
    "    \n",
    "    def create_dir(self, dir_path):\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "            print(\"Created new directory at \" + str(dir_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/45 [00:00<?, ?it/s]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/trects-kba2014/1/2012-02-27-00.gz\n",
      "gz file opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/241 [06:28<25:53:30, 388.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc count: 15823\n",
      "match_count:2\n",
      "File written to: /nfs/trects-kba2014-filtered-mine/1/2012-02-27-00.gz\n",
      "/nfs/trects-kba2014/1/2012-02-26-15.gz\n",
      "gz file opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 2/241 [09:13<21:20:00, 321.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc count: 7763\n",
      "match_count:23\n",
      "File written to: /nfs/trects-kba2014-filtered-mine/1/2012-02-26-15.gz\n",
      "/nfs/trects-kba2014/1/2012-02-26-17.gz\n",
      "gz file opened\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-bdaf492ad704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrectsfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrectsFilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrectsfilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_filtered_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-3ed08e457be4>\u001b[0m in \u001b[0;36mcreate_filtered_dataset\u001b[0;34m(self, force_reload, verbose)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgz_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgz_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;31m# get file markup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_markup_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgz_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 50MB file proving hard for beautifulsoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;31m# get docs in file that are in streamids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_matching_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-55a8aa151970>\u001b[0m in \u001b[0;36mopen_markup_file\u001b[0;34m(addr, gz, xml, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# open as html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_replacement_characters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m              self.builder.prepare_markup(\n\u001b[0;32m--> 345\u001b[0;31m                  markup, from_encoding, exclude_encodings=exclude_encodings)):\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bs4/builder/_lxml.py\u001b[0m in \u001b[0;36mprepare_markup\u001b[0;34m(self, markup, user_specified_encoding, exclude_encodings, document_declared_encoding)\u001b[0m\n\u001b[1;32m    184\u001b[0m         detector = EncodingDetector(\n\u001b[1;32m    185\u001b[0m             markup, try_encodings, is_html, exclude_encodings)\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_declared_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bs4/dammit.py\u001b[0m in \u001b[0;36mencodings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_usable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtried\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bs4/dammit.py\u001b[0m in \u001b[0;36mchardet_dammit\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m#import chardet.constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#chardet.constants._debug = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chardet/__init__.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(byte_str)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUniversalDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chardet/universaldetector.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLatin1Prober\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                     self.result = {'encoding': prober.charset_name,\n\u001b[1;32m    213\u001b[0m                                    \u001b[0;34m'confidence'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_confidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chardet/charsetgroupprober.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chardet/sbcharsetprober.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keep_english_letter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mbyte_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_international_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chardet/charsetprober.py\u001b[0m in \u001b[0;36mfilter_international_words\u001b[0;34m(buf)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# If the last character in the word is a marker, replace it with a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trectsfilter = TrectsFilter()\n",
    "trectsfilter.create_filtered_dataset(verbose=True, force_reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zulu>2012-02-27T00:55:00.000000Z</zulu>\n",
      "<zulu>2012-02-27T00:39:00.000000Z</zulu>\n"
     ]
    }
   ],
   "source": [
    "# test1_path = \"/nfs/trects-kba2014-filtered-mine/1/2012-02-27-00.gz\"\n",
    "# test2_path = \"/nfs/trects-kba2014-filtered-mine/1/2012-02-26-15.gz\"\n",
    "# test1_markup = open_markup_file(test1_path)\n",
    "# for doc in test1_markup.find_all(\"doc\"):\n",
    "#     print(doc.find(\"zulu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Generate and Load a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusGenerator:\n",
    "    def __init__(self, proj_dir, corpus_split_step=200):\n",
    "        self.path_handler = FilePathHandler(proj_dir)\n",
    "        # [\"topics\", \"corpus\", \"nuggets\", \"embed_labels\", \"updates\"]\n",
    "        self.file_purposes = self.path_handler.file_purposes\n",
    "        self.corpus_split_step = corpus_split_step\n",
    "        self.topic_dfs = {}  # dict of topic dfs per corpus_name\n",
    "        \n",
    "    def generate(self, selection=None, corpus_names=None, new_corpuses=None, \n",
    "             force_reload=False, save=True, verbose=True):\n",
    "        \n",
    "        # add new corpuses to load\n",
    "        if new_corpuses is not None:\n",
    "            for new_corpus in new_corpuses:\n",
    "                self.path_handler.add_corpus_source(new_corpus, overwrite=True)\n",
    "        # get corpus paths to load from (if corpus_names is None loads all)\n",
    "        self.corpus_sources = self.path_handler.get_corpus_sources(corpus_names=corpus_names)\n",
    "        corpus_names = self.corpus_sources.keys()\n",
    "        \n",
    "        if selection is None:  # if none do all\n",
    "            selection = self.file_purposes\n",
    "        \n",
    "        for corpus_name in corpus_names:\n",
    "            print(\"corpus_name generate loop: \" + str(corpus_name))\n",
    "            for select in selection:\n",
    "                if select == \"topics\":\n",
    "                    # create topics_df csv\n",
    "                    \n",
    "                    self.topic_dfs[corpus_name] = self.get_topic_df(corpus_name, save=save, force_reload=force_reload, \n",
    "                                                      verbose=verbose, add_path=True)\n",
    "                elif select == \"corpus\":\n",
    "                    # create corpus df csvs\n",
    "                    self.corpus_splitter(corpus_name, force_reload=force_reload, verbose=False)\n",
    "                \n",
    "#                 elif select == \"nuggets\":\n",
    "#                     # create nuggets df\n",
    "#                     # need to edit create_nuggets to not use local df file\n",
    "#                     # need to iteratively load corpus to process nuggets\n",
    "                    \n",
    "                    \n",
    "    def corpus_splitter(self, corpus_name, force_reload=False, verbose=True):\n",
    "        # split by topic and then every 200 html gz files, then parse together in loading\n",
    "        # add check for what's been done already (i.e. check current topics, if all splits taken place)\n",
    "        \n",
    "        # if not exists load topics\n",
    "        if self.topic_dfs is None or corpus_name not in self.topic_dfs:\n",
    "            self.topic_dfs = self.get_topic_df(corpus_name)\n",
    "        \n",
    "        corpus_dir = self.corpus_sources[corpus_name][\"dir_path\"]\n",
    "        print(\"corpus_dir:\" + str(corpus_dir))\n",
    "            \n",
    "        for topic_id in self.topic_dfs[corpus_name]['id'].unique():\n",
    "            # check if path exists\n",
    "            p_df = self.path_handler.path_df\n",
    "            p_df = p_df[p_df['file_purpose'] == \"corpus\"]\n",
    "            t_df_paths = p_df[p_df['instance_identifier'] == str(topic_id)]\n",
    "            t_df_paths = t_df_paths[t_df_paths['exists'] == True]  # only concerned with created files\n",
    "            start_split = 0\n",
    "            num_split = 0\n",
    "            if len(t_df_paths) == 0 or force_reload:  # not yet processed\n",
    "                start_split = 0\n",
    "            else:\n",
    "                # check if all splits been processed\n",
    "                num_split = t_df_paths['num_splits'][0]  # ensure same num_splits is inputted into path_df\n",
    "                if len(t_df_paths) < num_split:  # not counting from zero\n",
    "                    break\n",
    "                # get start point if partway through\n",
    "                start_split = max(list(map(int, list(t_df_paths['split_indentifier']))))\n",
    "            \n",
    "            t_dir = corpus_dir + '/' + str(topic_id)\n",
    "            num_files = len(search_dir(t_dir))\n",
    "            # create split indexes to feed to load_corpus\n",
    "            splits = [start_split]\n",
    "            add = splits[-1] + self.corpus_split_step\n",
    "            while add < num_files:\n",
    "                splits.append(add)\n",
    "                add = splits[-1] + self.corpus_split_step\n",
    "            \n",
    "            if start_split == 0:  \n",
    "                num_splits = len(splits)  # for inputting into path_df\n",
    "            \n",
    "            # create corpus_df files\n",
    "            for split_num in splits:\n",
    "                # get save path\n",
    "                save_path = self.path_handler.get_path(corpus_name, \"corpus\", str(topic_id), \".csv.gz\",\n",
    "                                        split_identifier=str(split_num), num_splits=num_splits, add_path=True)\n",
    "                \n",
    "                load_corpus(save_path, corpus_dir=corpus_dir, topic_ids=[topic_id], \n",
    "                            split_every=self.corpus_split_step, split_start_doc=split_num, \n",
    "                            save=True, force_reload=force_reload, \n",
    "                            verbose=verbose, path_handler=self.path_handler)\n",
    "                \n",
    "\n",
    "    def get_topic_df(self, corpus_name, save=True, force_reload=False, verbose=True, add_path=False):\n",
    "        load_path = self.corpus_sources[corpus_name][\"topics_path\"]\n",
    "        save_path = self.path_handler.get_path(corpus_name, \"topics\", \"topics_df\", \".csv.gz\", add_path=add_path)\n",
    "        \n",
    "        topic_df = load_topics(save_path, load_path=load_path, save=save, force_reload=force_reload, \n",
    "                               verbose=verbose, path_handler=self.path_handler)\n",
    "        return topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_name generate loop: tr13_filtered\n",
      "Loading topics\n",
      "loaded from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: UserWarning: Path already exists in dataframe: /nfs/proj-repo/AAARG-dissertation/dataset/tr13_filtered/topics/topics_df.csv.gz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>query</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012 Buenos Aires Rail Disaster</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2012_Buenos_Aires...</td>\n",
       "      <td>1329910380</td>\n",
       "      <td>1330774380</td>\n",
       "      <td>buenos aires train crash</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2012 Pakistan garment factory fires</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2012_Pakistan_gar...</td>\n",
       "      <td>1347368400</td>\n",
       "      <td>1348232400</td>\n",
       "      <td>pakistan factory fire</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2012 Aurora shooting</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2012_Aurora_shooting</td>\n",
       "      <td>1342766280</td>\n",
       "      <td>1343630280</td>\n",
       "      <td>colorado shooting</td>\n",
       "      <td>shooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Wisconsin Sikh temple shooting</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...</td>\n",
       "      <td>1344180300</td>\n",
       "      <td>1345044300</td>\n",
       "      <td>sikh temple shooting</td>\n",
       "      <td>shooting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id                                title  \\\n",
       "0           0   1      2012 Buenos Aires Rail Disaster   \n",
       "1           1   2  2012 Pakistan garment factory fires   \n",
       "2           2   3                 2012 Aurora shooting   \n",
       "3           3   4       Wisconsin Sikh temple shooting   \n",
       "\n",
       "                                         description       start         end  \\\n",
       "0  http://en.wikipedia.org/wiki/2012_Buenos_Aires...  1329910380  1330774380   \n",
       "1  http://en.wikipedia.org/wiki/2012_Pakistan_gar...  1347368400  1348232400   \n",
       "2  http://en.wikipedia.org/wiki/2012_Aurora_shooting  1342766280  1343630280   \n",
       "3  http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...  1344180300  1345044300   \n",
       "\n",
       "                      query      type  \n",
       "0  buenos aires train crash  accident  \n",
       "1     pakistan factory fire  accident  \n",
       "2         colorado shooting  shooting  \n",
       "3      sikh temple shooting  shooting  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: UserWarning: Path already exists in dataframe: /nfs/proj-repo/AAARG-dissertation/dataset/tr13_filtered/corpus/1_0.csv.gz\n",
      "  1%|          | 2/200 [00:00<00:15, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "corpus_dir:/nfs/trects-kba2013-filtered\n",
      "Loading topic 1...\n",
      "creating corpus df for topic 1 starting at file no. 0 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:08<00:00, 22.40it/s]\n",
      "  8%|         | 3/40 [00:00<00:01, 27.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 1...\n",
      "creating corpus df for topic 1 starting at file no. 200 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:00<00:00, 52.44it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 2...\n",
      "creating corpus df for topic 2 starting at file no. 0 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [03:47<00:00,  1.14s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 2...\n",
      "creating corpus df for topic 2 starting at file no. 200 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:24<00:00,  1.66it/s]\n",
      "  2%|         | 3/200 [00:00<00:08, 22.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 3...\n",
      "creating corpus df for topic 3 starting at file no. 0 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [01:45<00:00,  1.89it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 3...\n",
      "creating corpus df for topic 3 starting at file no. 200 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:14<00:00,  2.81it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 4...\n",
      "creating corpus df for topic 4 starting at file no. 0 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [02:08<00:00,  1.56it/s]\n",
      "  2%|         | 1/40 [00:00<00:06,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 4...\n",
      "creating corpus df for topic 4 starting at file no. 200 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:32<00:00,  1.22it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 5...\n",
      "creating corpus df for topic 5 starting at file no. 0 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [01:05<00:00,  3.03it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 5...\n",
      "creating corpus df for topic 5 starting at file no. 200 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:11<00:00,  3.56it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 6...\n",
      "creating corpus df for topic 6 starting at file no. 0 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:45<00:00,  4.42it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 6...\n",
      "creating corpus df for topic 6 starting at file no. 200 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:13<00:00,  3.03it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 7...\n",
      "creating corpus df for topic 7 starting at file no. 0 splitting every 200 docs\n",
      "Loading topic 8...\n",
      "creating corpus df for topic 8 starting at file no. 0 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:18<00:00, 10.90it/s]\n",
      "  8%|         | 3/40 [00:00<00:01, 19.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 8...\n",
      "creating corpus df for topic 8 starting at file no. 200 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:02<00:00, 16.45it/s]\n",
      "  0%|          | 1/200 [00:00<00:32,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 9...\n",
      "creating corpus df for topic 9 starting at file no. 0 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:07<00:00, 27.62it/s]\n",
      "  8%|         | 3/40 [00:00<00:01, 19.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 9...\n",
      "creating corpus df for topic 9 starting at file no. 200 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:01<00:00, 23.36it/s]\n",
      "  0%|          | 1/200 [00:00<00:23,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 10...\n",
      "creating corpus df for topic 10 starting at file no. 0 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [01:11<00:00,  2.79it/s]\n",
      "  5%|         | 2/40 [00:00<00:02, 13.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 10...\n",
      "creating corpus df for topic 10 starting at file no. 200 splitting every 200 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:06<00:00,  6.28it/s]\n"
     ]
    }
   ],
   "source": [
    "proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "tr13_filtered_dict = { \"corpus_name\":\"tr13_filtered\",\n",
    "                        \"dir_path\":\"/nfs/trects-kba2013-filtered\", \n",
    "                      \"topics_path\":\"/nfs/trects-kba2013-filtered/test-topics.xml\", \n",
    "                      \"nuggets_path\":\"/nfs/TemporalSummarization/ts13/results/nuggets.tsv\"}\n",
    "\n",
    "\n",
    "corp_gen = CorpusGenerator(proj_dir)\n",
    "\n",
    "\n",
    "corp_gen.generate(new_corpuses=[tr13_filtered_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CorpusLoader:\n",
    "#     def __init__(self):\n",
    "#         self.path_handler = FilePathHandler()\n",
    "#         self.file_purposes = self.path_handler.file_purposes\n",
    "        \n",
    "        \n",
    "#     def load(self, selection=None, corpus_names=None, new_corpuses=None, force_reload=False, save=True, \n",
    "#              create_only=False, verbose=True):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#             selection: the data to load (e.g. corpus/nuggets), if None then load options\n",
    "#             corpus_names: list of corpus names to load\n",
    "#             new_corpuses: list of dicts of paths with keys {\"dir_path\", \"topics_path\", \"nuggets_path\"}\n",
    "#             force_reload: force rebuild corpus files from original files\n",
    "#             save: save built corpus files\n",
    "#             create_only: only generate missing corpus files, do not load into ram\n",
    "#         \"\"\"\n",
    "#         \"\"\"Put flags in here to control process too\n",
    "#         Steps:\n",
    "#         1. Load corpus from gz html files\n",
    "#         2. Load topics from topics file\n",
    "#         3. Create nuggets_df from nuggets file\n",
    "#         4. Create embeddings from nuggets and corpus\n",
    "#         \"\"\"\n",
    "#         # add new corpuses to load\n",
    "#         if new_corpuses is not None:\n",
    "#             for new_corpus in new_corpuses:\n",
    "#                 self.path_handler.add_corpus(new_corpus, overwrite=False)\n",
    "#         # get corpus paths to load from (if corpus_names is None loads all)\n",
    "#         corpus_sources = self.path_handler.get_corpus_sources(corpus_names=corpus_names)\n",
    "#         corpus_names = corpus_sources.keys()\n",
    "        \n",
    "#         # get paths for generated files in corpus\n",
    "#         if selection is None:  # if None selection get all\n",
    "#             selection = self.file_purposes\n",
    "#         corpus_names_paths = {}\n",
    "#         for corpus_name in corpus_names:\n",
    "#             name_paths = self.path_handler.paths_in_corpus_name(corpus_name, selection=selection)\n",
    "#             corpus_names_paths[corpus_name] = name_paths\n",
    "        \n",
    "#         # go through each selected corpus_name\n",
    "#         for corpus_name, corpus_paths in corpus_names_paths.items():\n",
    "#             # go through selected tasks\n",
    "#             for select in selection:  # maybe add tqdm here?\n",
    "#                 # for each identifier\n",
    "#                 # may behave differently when no entries in paths_df\n",
    "#                 for identifier, ident_paths in corpus_paths[select]:\n",
    "#                     exists = ident_paths['exists']\n",
    "#                     not_exists = ident_paths['not_exists']\n",
    "#                     if exists is None or force_reload:  # no paths loaded\n",
    "#                         # create all appropriate files\n",
    "# #                         not_exists = self.path_handler.convert_relative_path(not_exists)\n",
    "#                         self.select_load_func(select, paths=not_exists, save=save, \n",
    "#                                     force_reload=force_reload, verbose=verbose, create_only=create_only)\n",
    "#                     else:\n",
    "#                         if not_exists is None:  # only exists has paths\n",
    "#                             if create_only:\n",
    "#                                 # change this to something better\n",
    "#                                 warnings.warn(\"There are no new files to create\")\n",
    "#                             else:\n",
    "# #                                 exists = self.path_handler.convert_relative_path(exists)\n",
    "#                                 self.select_load_func(select, paths=exists, save=save, \n",
    "#                                     force_reload=force_reload, verbose=verbose, create_only=create_only)\n",
    "#                         else:  # both have paths\n",
    "#                             # need method to load/generate incomplete missing parts\n",
    "#                             # self.path_handler.convert_relative_paths(...) dont forget\n",
    "    \n",
    "#     def fix_partially_missing_paths(self, selection):\n",
    "#         \"\"\"Function to organise missing paths from partially-saved/generated dataset\"\"\"\n",
    "#         if selection == \"topics\":\n",
    "#             raise ValueError(\"There can only be one topics file for a corpus\")\n",
    "    \n",
    "#     def select_load_func(self, selection, paths=None, save=True, force_reload=False, \n",
    "#                              verbose=True, create_only=False, **identifiers):\n",
    "#         # [\"topics\", \"corpus\", \"nuggets\", \"embed_labels\", \"updates\"]\n",
    "#         if paths is None:\n",
    "#             # create paths appropriately, add to paths_df\n",
    "#             # also check paths/behaviour is correct in each load_func/after\n",
    "#             # also need to account for other identifiers\n",
    "#         if selection == \"topics\":\n",
    "#             # should only need to be one meta topics_df\n",
    "#             topic_dfs = []\n",
    "#             # need to add save paths into paths_df\n",
    "#             topic_df = load_topics(paths[0], save=save, force_reload=force_reload, verbose=verbose)\n",
    "                \n",
    "#         elif selection == \"corpus\":\n",
    "            \n",
    "#         elif selection == \"nuggets\":\n",
    "            \n",
    "#         elif selection == \"embed_labels\":\n",
    "            \n",
    "#         elif selection == \"updates\":\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

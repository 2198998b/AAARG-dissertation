{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Generating Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_threads = 32\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = str(num_threads)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ipynb.fs\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import kerastuner as kt\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from kerastuner.tuners import Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"gpus:\")\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTuner:\n",
    "    def __init__(self, save_dir, save_name, input_shape, tuning_iterations=2, max_epochs=15, reduction_factor=3,\n",
    "                 batch_size=32, force_reload=False, output_dims=1):\n",
    "        \"\"\"Can save using project_name param, if overwrite false then will reload where it started\n",
    "        In Tuner Class documentation\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_dims = output_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.models = []\n",
    "        self.tuner = Hyperband(self.build_model, \n",
    "                          objective='mean_squared_error', \n",
    "                          max_epochs=max_epochs,\n",
    "                          hyperband_iterations=tuning_iterations,\n",
    "                          factor=reduction_factor,  # keras-tuner default is 3\n",
    "                          directory=save_dir,\n",
    "                          project_name=save_name,\n",
    "                          overwrite=force_reload,\n",
    "                          tune_new_entries = True,\n",
    "                          allow_new_entries = True,\n",
    "                           distribution_strategy=tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()))\n",
    "\n",
    "        \n",
    "    def build_model(self, hp):\n",
    "        model = Sequential()\n",
    "        # specify input layer to ensure correct input shape\n",
    "        ilayer = InputLayer(input_shape=(self.input_shape,), \n",
    "                            batch_size=self.batch_size, \n",
    "                            name='input_layer')\n",
    "        model.add(ilayer)\n",
    "        \n",
    "        # add hidden layers\n",
    "        for i in range(hp.Int('num_hidden_layers', min_value=1, max_value=6)):\n",
    "            model.add(Dense(units=hp.Int('hidden_units_' + str(i),\n",
    "                                        min_value=32, max_value=2048, step=32),\n",
    "                            activation='tanh',\n",
    "                           name='hidden_layer_' + str(i)))\n",
    "            \n",
    "        # add output layer\n",
    "        model.add(Dense(units=self.output_dims, \n",
    "                        activation='tanh'))\n",
    "        \n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=0.0001, max_value=0.1))      \n",
    "        \n",
    "        model.compile(optimizer=opt, loss='huber', metrics=['mean_squared_error'])  # add metrics here\n",
    "        \n",
    "        self.models.append(model)\n",
    "        return model\n",
    "    \n",
    "    def search(self, batch_generator, best_model_dir):\n",
    "        \"\"\"Find optimal model given dataset\n",
    "        \"\"\"\n",
    "        self.tuner.search(x=batch_generator, verbose=1, use_multiprocessing=False, workers=num_threads)\n",
    "        best_models = self.tuner.get_best_models(num_models=5)\n",
    "        if best_model_dir is not None:\n",
    "            for i in range(len(best_models)):\n",
    "                print(\"Saving best model number: \" + str(i))\n",
    "                save_path = os.path.join(best_model_dir, str(i))\n",
    "                best_models[i].save(save_path)\n",
    "            hyperparams = self.tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        best_model_path = os.path.join(best_model_dir, str(0))\n",
    "        return best_models[0], hyperparams. best_model_path\n",
    "    \n",
    "\n",
    "# from collections import OrderedDict\n",
    "# from collections import deque\n",
    "\n",
    "class BatchGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Class to load in dataset that is too large to load into memory at once\n",
    "    \n",
    "    Do check in class before to make sure all X lists and y lists are same length\n",
    "    \n",
    "    https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size, num_batches):\n",
    "        if batch_size is None:\n",
    "            self.batch_size = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "#         self.shuffle = False  # make sure linear progression through dataset for sake of memory efficiency\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return self.num_batches\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one batch of data\"\"\"\n",
    "        inputs = self.load_samples(self.X, idx)\n",
    "        labels = self.load_samples(self.y, idx)\n",
    "        return inputs, labels\n",
    "    \n",
    "    \n",
    "    def load_samples(self, path, idx):\n",
    "        samples = path[idx]\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .defs.corpus_loader import PathRetriever, load_embeddings, load_topics, read_df_file_type, save_df_file_type\n",
    "from .defs.corpus_loader import convert_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_input_params(path_ret, corpus_names, nested_dirs, col_labels, input_col=None):\n",
    "    \"\"\"Helper function to resolve the selection of input params that determine what data to load/generate\"\"\"\n",
    "    # resolve corpus_names\n",
    "    if corpus_names is None:\n",
    "        corpus_names = path_ret.get_corpus_names()\n",
    "        if len(corpus_names) == 0:\n",
    "            raise Exception(\"There are no corpuses to load from\")\n",
    "    # resolve col_labels\n",
    "    if col_labels is None:  # our columns to generate files for\n",
    "        col_labels = default_col_labels.copy()\n",
    "        if input_col is not None:\n",
    "            col_labels.append(input_col)\n",
    "    # resolve nested_dirs\n",
    "    if type(nested_dirs) != dict:  # if output gets passed through again\n",
    "        nested_dict = {}\n",
    "        for corpus_name in corpus_names:  # get the nested dir for each corpus name\n",
    "            nested_dict[corpus_name] = path_ret.get_nested_dirs(corpus_name, \"embeddings\")\n",
    "            if nested_dirs is not None:\n",
    "                # add only selected nested_dirs for this corpus_name\n",
    "                nested_dict[corpus_name] = [x for x in nested_dict[corpus_name] if x in nested_dirs]\n",
    "        nested_dirs = nested_dict\n",
    "    # make sure there is at least one entry in nested_dict\n",
    "    empty_dirs = [len(x) == 0 for x in nested_dirs.values()]  # get if empty for each item\n",
    "    if all(empty_dirs):\n",
    "        raise Exception(\"There are no nested_dirs matching the selection\")\n",
    "    return corpus_names, nested_dirs, col_labels\n",
    "\n",
    "def corpus_name_topic_ids(path_retriever, corpus_name):\n",
    "    topic_path = path_retriever.get_topic_path(corpus_name, verbose=False)\n",
    "    topic_df = load_topics(topic_path, verbose=False)\n",
    "    topic_ids = list(topic_df['id'].unique())\n",
    "    return topic_ids\n",
    "\n",
    "def find_combinations(path_df, corpus_names, nested_dirs, col_labels, add_topics=False, col_labels_as_list=False,\n",
    "                      as_tuples=True, force_reload=False, path_retriever=None, batch_size=None, file_type=None,\n",
    "                     exists_only=False):\n",
    "    \"\"\"Find the combinations that have not been generated/trained already in path_df\n",
    "    \n",
    "    Tuple ordering: (corpus_name, nested_dir, col_label/[col_labels], **topic_id**)\n",
    "    \"\"\"\n",
    "    if exists_only:\n",
    "        path_df = path_df[path_df['exists'] == True]  # checking of path_df is only concerned with existing files\n",
    "    if batch_size is not None:\n",
    "        path_df = path_df[path_df['batch_size'] == batch_size]\n",
    "    if file_type is not None:\n",
    "        path_df = path_df[path_df['file_type'] == file_type]\n",
    "    topic_ids = {}\n",
    "    if add_topics:  # find topic_ids for each corpus\n",
    "        for corpus_name in corpus_names:\n",
    "            if path_retriever is not None:\n",
    "                topic_ids[corpus_name] = corpus_name_topic_ids(path_retriever, corpus_name)\n",
    "            else:\n",
    "                raise Exception(\"If add_topics is True then path_retriever must be set to an instance of PathRetriever\")\n",
    "    # get possible combinations\n",
    "    combinations = []\n",
    "    for corpus_name in corpus_names:\n",
    "        for nested_dir in nested_dirs[corpus_name]:\n",
    "            combo_path = path_df[(path_df['corpus_name'] == corpus_name)\n",
    "                                    & (path_df['nested_dir'] == nested_dir)]\n",
    "            combo = [corpus_name, nested_dir]\n",
    "            if add_topics:  # create permutations with topic_ids\n",
    "                topic_combo_dict = defaultdict(list)\n",
    "                for label in col_labels:\n",
    "                    for topic_id in topic_ids[corpus_name]:  # check if label exists for topic_id\n",
    "                        topic_path = combo_path[(combo_path['col_label'] == label)\n",
    "                                               & (combo_path['topic_id'] == topic_id)]\n",
    "                        if len(topic_path) == 0 or force_reload:\n",
    "                            topic_combo_dict[topic_id].append(label)\n",
    "                topic_combos = []\n",
    "                for topic_id, labels in topic_combo_dict.items():\n",
    "                    topic_combos = []\n",
    "                    if col_labels_as_list:  # add single tuple with all missing col_labels for topic_id\n",
    "                        topic_combo = copy.deepcopy(combo)\n",
    "                        topic_combo.append(labels)\n",
    "                        topic_combo.append(topic_id)\n",
    "                        topic_combos.append(topic_combo)\n",
    "                    else:\n",
    "                        for label in labels:  # add a tuple for each missing col_label for topic_id\n",
    "                            topic_combo = copy.deepcopy(combo)\n",
    "                            topic_combo.append(topic_id)\n",
    "                            topic_combos.append(topic_combo)\n",
    "                    combinations.extend(topic_combos)\n",
    "            else:  # create permutations with col_labels only\n",
    "                label_combos = []\n",
    "                add_labels = None\n",
    "                if not force_reload:  # find which col_labels don't exist already\n",
    "                    exist_labels = list(combo_path['col_label'].unique())\n",
    "                    add_labels = [x for x in col_labels if x not in exist_labels]\n",
    "                else:\n",
    "                    add_labels = copy.deepcopy(col_labels)  # force_reload add all labels\n",
    "                if col_labels_as_list:  # add single tuple\n",
    "                    label_combo = copy.deepcopy(combo)\n",
    "                    label_combo.append(add_labels)\n",
    "                    label_combos.append(label_combo)\n",
    "                else:\n",
    "                    for add_label in add_labels:  # add tuple for each col_label\n",
    "                        label_combo = copy.deepcopy(combo)\n",
    "                        label_combo.append(add_label)\n",
    "                        label_combos.append(label_combo)\n",
    "                combinations.extend(label_combos)\n",
    "                \n",
    "    if as_tuples:\n",
    "        combinations = [tuple(x) for x in combinations]\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemmapGenerator:\n",
    "    def __init__(self, proj_dir):\n",
    "        if proj_dir is None:\n",
    "            self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "        else:\n",
    "            self.proj_dir = proj_dir\n",
    "        self.default_file_type = \".hdf\"\n",
    "        self.path_ret = PathRetriever(proj_dir)\n",
    "        self.path_df_cols = ['corpus_name', 'nested_dir', 'col_label', 'path', 'dtype', 'vector_len', \n",
    "                             'total_nums', 'offset_step', 'topic_ids', 'complete']\n",
    "        self.dataset_dir = self.path_ret.path_handler.dataset_dir\n",
    "        self.sample_dir = os.path.join(self.dataset_dir, \"samples\")\n",
    "        self.path_df_path = os.path.join(self.dataset_dir, \"memmap_paths.hdf\")\n",
    "        self.path_df = self.load_path_df()\n",
    "        self.order = 'C'\n",
    "        \n",
    "        \n",
    "    def create_maps(self, corpus_name, nested_dir, col_labels, topic_ids, verbose=True, force_reload=False):\n",
    "        # check if already completed\n",
    "        path_slice = self.slice_path_df(corpus_name, nested_dir, topic_ids)\n",
    "        emb_paths = self.path_ret.get_embedding_paths(corpus_name, nested_dir, \n",
    "                                    file_type=self.default_file_type, verbose=False, \n",
    "                                    return_dir_path=False, topic_ids=topic_ids)\n",
    "        emb_paths = list(emb_paths['path'])\n",
    "        # load partial information on maps that need completed\n",
    "        meta_dict = self.create_meta_dict(path_slice, corpus_name, nested_dir, col_labels, \n",
    "                                          self.topic_ids_str(topic_ids), force_reload=force_reload)\n",
    "        \n",
    "        if len(meta_dict) > 0:\n",
    "            if verbose:\n",
    "                print(\"Creating memmaps for \" + str(\", \".join(col_labels)) + \"\\nwith topics: \" + str(topic_ids))\n",
    "            for emb_path in tqdm_notebook(emb_paths):\n",
    "                # get the cols that haven't been loaded for this path\n",
    "                # scrape data from dataframe\n",
    "                label_data = self.scrape_col_data(emb_path, meta_dict.keys())\n",
    "                # add data to memmap\n",
    "                for col_label, data in label_data.items():\n",
    "                    col_dict = meta_dict[col_label]\n",
    "                    if not col_dict['initialised']:\n",
    "                        col_dict['dtype'] = data.dtype\n",
    "                        ndim = data.ndim\n",
    "                        if ndim == 1:  # 1d\n",
    "                            col_dict['vector_len'] = 1\n",
    "                        elif ndim == 2:  # 2d\n",
    "                            col_dict['vector_len'] = data.shape[1]\n",
    "                        else:\n",
    "                            raise Exception(\"Too many dimensions: \" + str(data.shape))\n",
    "                        col_dict['offset_step'] = data.dtype.itemsize\n",
    "                        col_dict['initialised'] = True\n",
    "                        \n",
    "                        \n",
    "                    # load meta_dict vars, save hashing time\n",
    "                    total_nums = col_dict['total_nums']\n",
    "                    offset_step = col_dict['offset_step']\n",
    "                    path = col_dict['path']\n",
    "                    dtype = col_dict['dtype']\n",
    "\n",
    "                    # add data to map\n",
    "                    flat = data.ravel()\n",
    "                    num_to_add = len(flat)\n",
    "                    \n",
    "                    memmap = None\n",
    "                    if total_nums != 0:  # write to existing file\n",
    "                        memmap = np.memmap(path, dtype=dtype, mode='r+', offset=0, \n",
    "                                       order=self.order, shape=(total_nums + num_to_add,))\n",
    "                    else:  # create new file\n",
    "                        memmap = np.memmap(path, dtype=dtype, mode='w+', offset=0, \n",
    "                                       order=self.order, shape=(num_to_add,))\n",
    "                    \n",
    "                    memmap[total_nums:total_nums+num_to_add] = flat[:]\n",
    "                    if not np.array_equal(memmap[total_nums:total_nums+num_to_add], flat):\n",
    "                        print(\"memmap: \" + str(memmap[total_nums:total_nums+num_to_add]))\n",
    "                        print(\"flat: \" + str(flat))\n",
    "                        raise Exception(\"Memmap and flat not equal\")\n",
    "                    \n",
    "                    memmap.flush()\n",
    "\n",
    "                    # update fields\n",
    "                    col_dict['total_nums'] += num_to_add\n",
    "                    \n",
    "            for col_label, meta in meta_dict.items():\n",
    "                self.update_path_df_entry(meta['path'], col_label, meta['dtype'], meta['vector_len'],\n",
    "                         meta['offset_step'], meta['total_nums'])\n",
    "\n",
    "            if verbose:\n",
    "                print(display(path_slice))\n",
    "            print(\"Completed creating memmaps\")\n",
    "        else:\n",
    "            print(\"Already loaded \" + str(col_labels))\n",
    "            \n",
    "    def update_path_df_entry(self, path, col_label, dtype, vector_len, offset_step, total_nums):\n",
    "        mask = (self.path_df['path'] == path) & (self.path_df['col_label'] == col_label)\n",
    "        change_cols = ['dtype', 'vector_len', 'offset_step', 'total_nums', 'complete']\n",
    "        self.path_df.loc[mask, change_cols] = dtype, vector_len, offset_step, total_nums, True\n",
    "        self.save_path_df()\n",
    "        \n",
    "            \n",
    "    def add_path_df_entry(self, corpus_name, nested_dir, col_label, path, topic_ids, return_row_dict=False):\n",
    "        row = {'corpus_name':corpus_name, 'nested_dir':nested_dir, 'col_label':col_label, \n",
    "               'path':path, 'dtype':None, 'vector_len':np.nan, 'total_nums':0, \n",
    "               'offset_step':0, 'topic_ids':topic_ids, 'complete':False}\n",
    "        self.path_df = self.path_df.append(row, ignore_index=True)\n",
    "        self.save_path_df()\n",
    "        if return_row_dict:\n",
    "            return row\n",
    "        \n",
    "    def create_meta_dict(self, path_slice, corpus_name, nested_dir, col_labels, topic_ids,\n",
    "                        force_reload=False):\n",
    "        meta_dict = {}\n",
    "        for col_label in col_labels:\n",
    "            col_slice = path_slice[path_slice['col_label'] == col_label]\n",
    "            if len(col_slice) > 0:\n",
    "                if len(col_slice) == 1:\n",
    "                    complete = list(col_slice['complete'])[0]\n",
    "                    if not complete or force_reload:\n",
    "                        # add previous values\n",
    "                        col_slice = col_slice.to_dict(orient='list')\n",
    "                        col_slice['path'][0]\n",
    "                        row_dict = {\"dtype\":col_slice['dtype'][0], \"path\":col_slice['path'][0], \n",
    "                                    \"vector_len\":col_slice['vector_len'][0], \n",
    "                                    \"offset_step\":col_slice['offset_step'][0], \"total_nums\":0, # set to 0 to restart\n",
    "                                    \"initialised\":False, \"completed\":False}  \n",
    "                        meta_dict[col_label] = row_dict\n",
    "                else:\n",
    "                    print(display(col_slice))\n",
    "                    raise Exception(\"Multiple entries in path_df\")\n",
    "            else:\n",
    "                # add to path df\n",
    "                row_dict = self.add_path_df_entry(corpus_name, nested_dir, col_label,\n",
    "                                                 self.generate_new_map_path(col_label),\n",
    "                                                 topic_ids, return_row_dict=True)\n",
    "                row_dict['initialised'] = False\n",
    "                meta_dict[col_label] = row_dict\n",
    "        return meta_dict\n",
    "        \n",
    "    def load_memmap(self, corpus_name, nested_dir, topic_ids, col_label, batch_size=None,\n",
    "                   return_vector_len=False):\n",
    "        path_slice = self.slice_path_df(corpus_name, nested_dir, topic_ids)\n",
    "        col_slice = path_slice[path_slice['col_label'] == col_label]\n",
    "        if len(col_slice) == 1:\n",
    "            col_dict = col_slice.to_dict(orient='list')\n",
    "            dtype = col_dict['dtype'][0]\n",
    "            vector_len = int(col_dict['vector_len'][0])\n",
    "            total_nums = int(col_dict['total_nums'][0])\n",
    "            path = col_dict['path'][0]\n",
    "            \n",
    "            shape = None\n",
    "            num_items = int(total_nums / vector_len)\n",
    "            if batch_size is not None:\n",
    "                num_batches = math.floor(num_items / batch_size)\n",
    "                shape = (num_batches, batch_size, vector_len)\n",
    "            else:\n",
    "                shape = (num_items, vector_len)\n",
    "            memmap = np.memmap(path, dtype=dtype, mode='r', shape=shape, order=self.order)\n",
    "            if return_vector_len:\n",
    "                return memmap, vector_len\n",
    "            return memmap\n",
    "        else:\n",
    "            print(display(path_slice))\n",
    "            raise Exception(str(len(path_slice)) + \" entries for \")\n",
    "    \n",
    "    def slice_path_df(self, corpus_name, nested_dir, topic_ids):\n",
    "        topic_id_str = topic_ids\n",
    "        if type(topic_id_str) != str:\n",
    "            topic_id_str = self.topic_ids_str(topic_ids)\n",
    "            \n",
    "        mask = (self.path_df['corpus_name'] == corpus_name) & (self.path_df['nested_dir'] == nested_dir) & (self.path_df['topic_ids'] == topic_id_str)\n",
    "        path_slice = self.path_df.loc[mask]\n",
    "        return path_slice\n",
    "        \n",
    "    def topic_ids_str(self, topic_ids):\n",
    "        if type(topic_ids) != str:\n",
    "            sort = sorted(topic_ids)\n",
    "            sort = [str(x) for x in sort]\n",
    "            string = \",\".join(sort)\n",
    "            return string\n",
    "        else:\n",
    "            raise Exception(str(topic_ids) + \" is already type str\")\n",
    "        \n",
    "    def save_path_df(self):\n",
    "        save_df_file_type(self.path_df, self.path_df_path, verbose=False)\n",
    "                \n",
    "    def load_path_df(self):\n",
    "        if os.path.exists(self.path_df_path):\n",
    "            path_df = read_df_file_type(self.path_df_path, verbose=True)\n",
    "        else:\n",
    "            path_df = pd.DataFrame(columns=self.path_df_cols)\n",
    "            print(\"memmap path df created from scratch\")\n",
    "        return path_df\n",
    "        \n",
    "    def incompleted_col_labels(self, path_slice, col_labels):\n",
    "        incompleted = []\n",
    "        for col_label in col_labels:\n",
    "            col_slice = path_slice[path_slice['col_label'] == col_label]\n",
    "            if len(col_slice) > 0:\n",
    "                if len(col_slice) == 1:\n",
    "                    complete = list(col_slice['complete'])[0]\n",
    "                    if not complete:\n",
    "                        incompleted.append(col_label)\n",
    "                else:\n",
    "                    print(display(col_slice))\n",
    "                    raise Exception(\"Multiple entries in path_df\")\n",
    "            else:\n",
    "                incompleted.append(col_label)\n",
    "        return incompleted\n",
    "            \n",
    "    def generate_new_map_path(self, col_label):\n",
    "        # putting topic_ids in filename too long, use count instead\n",
    "        count = len(self.path_df)\n",
    "        base = str(count) + \"_\" + str(col_label)\n",
    "        if not os.path.exists(self.sample_dir):\n",
    "            os.makedirs(self.sample_dir)\n",
    "        mappath = os.path.join(self.sample_dir, base + \".memmap\")\n",
    "        return mappath          \n",
    "            \n",
    "    def scrape_col_data(self, emb_path, col_labels):\n",
    "        # setup return variables\n",
    "        labels = {}\n",
    "        emb_df = load_embeddings(emb_path, verbose=False)\n",
    "        for col_label in col_labels:\n",
    "            if col_label not in emb_df.columns:\n",
    "                raise ValueError(\"Target label \" + str(col_label) + \" is not in file at \" + str(emb_path))\n",
    "            # collect label values from df\n",
    "            labs = np.array(list(emb_df[col_label]))\n",
    "            labels[col_label] = labs\n",
    "        return labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPathHandler:\n",
    "    def __init__(self, proj_dir='/nfs/proj-repo/AAARG-dissertation', base_dir_name=\"tuning_models\",\n",
    "                verbose=True):\n",
    "        self.proj_dir = proj_dir\n",
    "        self.base_dir_name = base_dir_name\n",
    "        self.model_dir_path = os.path.join(self.proj_dir, self.base_dir_name)\n",
    "        self.df_name = \"nn_path_df.hdf\"\n",
    "        self.df_path = os.path.join(self.model_dir_path, self.df_name)\n",
    "        self.df_cols = [\"corpus_name\", \"nested_dir\", \"X_col\", \"y_col\", \"tuner_dir\", \n",
    "                                \"tuner_name\", \"best_hyperparams\", \"batch_size\", \"best_model_path\",\n",
    "                               \"input_param_text_path\", \"redundancy_threshold\"]\n",
    "        self.verbose = verbose\n",
    "        self.df = self.load_df(verbose=verbose)\n",
    "        \n",
    "    def load_df(self, verbose=True):\n",
    "        if os.path.exists(self.df_path):\n",
    "            df = read_df_file_type(self.df_path, verbose=verbose)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=self.df_cols)\n",
    "            if verbose:\n",
    "                print(\"model path df created from scratch\")\n",
    "        return df\n",
    "    \n",
    "    def save_df(self, verbose=False):\n",
    "        save_df_file_type(self.df, self.df_path, verbose=verbose)\n",
    "    \n",
    "    def add_path(self, corpus_name, nested_dir, X_col, y_col, tuner_dir, tuner_name,\n",
    "                          best_hyperparams, batch_size, best_model_path, input_param_text_path, verbose=True):\n",
    "        # check if exists in dataframe\n",
    "        df = self.df\n",
    "        mask = (df['corpus_name']==corpus_name)&(df['nested_dir']==nested_dir)&(df['X_col']==X_col)&(df['y_col']==y_col)&(df['tuner_dir']==tuner_dir)&(df['tuner_name']==tuner_name)&(df['batch_size']==batch_size)\n",
    "        exist_slice = df.loc[mask]\n",
    "        if len(exist_slice) == 1:  # update existing row\n",
    "            print(\"Saving to existing row on nn_path_df\")\n",
    "            self.df.loc[mask, self.df_cols] = corpus_name, nested_dir, X_col, y_col, tuner_dir, tuner_name, best_hyperparams, batch_size, best_model_path, input_param_text_path\n",
    "        else:  # append new row\n",
    "            print(\"Appending new row to nn_path_df\")\n",
    "            row = {\"corpus_name\":corpus_name, \"nested_dir\":nested_dir, \"X_col\":X_col,\n",
    "                  \"y_col\":y_col, \"tuner_dir\":tuner_dir, \"tuner_name\":tuner_name,\n",
    "                  \"best_hyperparams\":best_hyperparams, \"batch_size\":batch_size, \"best_model_path\":best_model_path,\n",
    "                  \"input_param_text_path\":input_param_text_path, \"redundancy_threshold\":np.nan}\n",
    "            self.df = self.nn_path_df.append(row, ignore_index=True)\n",
    "        # save new entry\n",
    "        self.save_df(verbose=verbose)\n",
    "        \n",
    "    def generate_nn_save_path(self, corpus_name, nested_dir, X_col, y_col, batch_size, create_dir=True):\n",
    "        col_dir = str(X_col) + \"_\" + str(y_col) + \"_\" + str(int(batch_size))\n",
    "        dir_list = [self.model_dir_path, corpus_name, nested_dir, col_dir]\n",
    "        # combine directories to form path of subdirectories, create dirs if necessary\n",
    "        dir_path = None\n",
    "        for cur_dir in dir_list:\n",
    "            if dir_path is None:  # first iteration\n",
    "                dir_path = dir_list[0]\n",
    "            else:\n",
    "                dir_path = os.path.join(dir_path, cur_dir)\n",
    "            if not os.path.exists(dir_path) and create_dir:\n",
    "                os.makedirs(dir_path)\n",
    "        # generate name\n",
    "        save_name = \"tuner_proj\"\n",
    "        return dir_path, save_name\n",
    "    \n",
    "    def update_redundancy_threshold(self, corpus_name, nested_dir, X_col, y_col, batch_size,\n",
    "                                   redundancy_threshold, verbose=True):\n",
    "        if \"redundancy_threshold\" not in self.df.columns:\n",
    "            self.df['redundancy_threshold'] = np.nan\n",
    "        \n",
    "        mask = self.create_df_mask(corpus_name, nested_dir, X_col, y_col, batch_size)\n",
    "        self.df.loc[mask, ['redundancy_threshold']] = redundancy_threshold\n",
    "        self.save_df(verbose=verbose)\n",
    "        \n",
    "    def create_df_mask(self, corpus_name, nested_dir, X_col, y_col, batch_size):\n",
    "        mask = (self.df['corpus_name']==corpus_name)&(self.df['nested_dir']==nested_dir)&(self.df['X_col']==X_col)&(self.df['y_col']==y_col)&(self.df['batch_size']==batch_size)\n",
    "        return mask\n",
    "        \n",
    "    def load_best_model(self, corpus_name, nested_dir, X_col, y_col, batch_size):\n",
    "        mask = self.create_df_mask(corpus_name, nested_dir, X_col, y_col, batch_size)\n",
    "        tuner_instance = self.df.loc[mask]\n",
    "        best_model_path = list(tuner_instance['best_model_path'])[0]\n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "class NNTrainer:\n",
    "    def __init__(self, proj_dir='/nfs/proj-repo/AAARG-dissertation', nn_base_save_dir_name=None):\n",
    "        self.proj_dir = proj_dir\n",
    "        self.nn_base_save_dir_name = nn_base_save_dir_name\n",
    "        if self.nn_base_save_dir_name is None:\n",
    "            self.nn_base_save_dir_name = \"tuning_models\"\n",
    "        self.nn_base_save_dir_path = os.path.join(self.proj_dir, self.nn_base_save_dir_name)\n",
    "        self.nn_path_df_name = \"nn_path_df.hdf\"\n",
    "        self.nn_path_df_path = os.path.join(self.nn_base_save_dir_path, self.nn_path_df_name)\n",
    "        self.nn_path_df_cols = [\"corpus_name\", \"nested_dir\", \"X_col\", \"y_col\", \"tuner_dir\", \n",
    "                                \"tuner_name\", \"best_hyperparams\", \"batch_size\", \"best_model_path\",\n",
    "                               \"input_param_text_path\"]\n",
    "        self.default_test_topics = [1,2,3,4,5,6,8,9,10]\n",
    "        self.default_train_ratio = 0.8\n",
    "        self.min_train_ratio = 0.5\n",
    "    \n",
    "    def train(self, corpus_name, nested_dir, topic_ids, X_col, y_col, tuning_iterations=5, max_epochs=15,\n",
    "              reduction_factor=3, batch_size=32, force_reload=False, verbose=True):\n",
    "        \"\"\"\n",
    "        1. Generate Data if needed\n",
    "        2. Determine combinations to try\n",
    "        3. Load combination\n",
    "        4. Train network on it\n",
    "        5. Generate summary on test topics\n",
    "        5. Save tuned network, metrics, database entries\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "        self.nn_path_df = self.load_nn_path_df(verbose=verbose)\n",
    "        \n",
    "        if verbose:\n",
    "            print(display(self.nn_path_df))\n",
    "            print(\"\")\n",
    "        \n",
    "        # generate data\n",
    "        mmap_gen = MemmapGenerator(self.proj_dir)\n",
    "        mmap_gen.create_maps(corpus_name, nested_dir, [X_col, y_col], topic_ids, verbose=verbose,\n",
    "                                         force_reload=False)  # setting to False for debug\n",
    "\n",
    "        # get paths for inputs and total_len of samples\n",
    "        X_map, vector_len = mmap_gen.load_memmap(corpus_name, nested_dir, topic_ids, X_col, \n",
    "                                                        batch_size=batch_size, return_vector_len=True)\n",
    "        # get paths for labels\n",
    "        y_map = mmap_gen.load_memmap(corpus_name, nested_dir, topic_ids, y_col, batch_size=batch_size,\n",
    "                                           return_vector_len=False)\n",
    "\n",
    "        # create a generator to feed NN samples/batches\n",
    "        num_batches = X_map.shape[0]\n",
    "        batch_generator = BatchGenerator(X_map, y_map, batch_size, num_batches)\n",
    "\n",
    "        # create paths to save NN tuning files to\n",
    "        save_dir, save_name = self.generate_nn_save_path(corpus_name, nested_dir, X_col, y_col, batch_size,\n",
    "                                                        create_dir=True)\n",
    "        \n",
    "        # Log params in text file\n",
    "        trials_in_iter = max_epochs * (math.log(max_epochs, reduction_factor) ** 2)\n",
    "        cur_time = self.get_cur_time()\n",
    "        param_str = \"corpus_name: \" + str(corpus_name) + \"\\n\"\n",
    "        param_str += \"nested_dir: \" + str(nested_dir) + \"\\n\"\n",
    "        param_str += \"X_input: \" + str(X_col) + \"\\n\"\n",
    "        param_str += \"y_labels: \" + str(y_col) + \"\\n\"\n",
    "        param_str += \"batch_size: \" + str(batch_size) + \"\\n\"\n",
    "        param_str += \"train_topics: \" + str(train_topics) + \"\\n\"\n",
    "        param_str += \"max_epochs: \" + str(max_epochs) + \"\\n\"\n",
    "        param_str += \"tuning_iterations: \" + str(tuning_iterations) + \"\\n\"\n",
    "        param_str += \"reduction_factor: \" + str(reduction_factor) + \"\\n\"\n",
    "        param_str += \"estimated trials per iteration: \" + str(trials_in_iter) + \"\\n\"\n",
    "        param_str += \"total estimated trials: \" + str(trials_in_iter * tuning_iterations) + \"\\n\"\n",
    "        param_str += \"Started at: \" + str(cur_time) + \"\\n\"\n",
    "        input_param_text_path = os.path.join(save_dir, \"parameter_details.txt\")  # save with NN\n",
    "        if not os.path.exists(input_param_text_path) or force_reload:\n",
    "#         if not os.path.exists(input_param_text_path) or True:\n",
    "            with open(input_param_text_path, \"w\") as param_file:\n",
    "                param_file.write(param_str)\n",
    "        print(param_str)\n",
    "        \n",
    "        # add/save new nn_path_df entry\n",
    "        self.add_to_nn_path_df(corpus_name, nested_dir, X_col, y_col, save_dir,\n",
    "                               save_name, None, batch_size, None, \n",
    "                               input_param_text_path, verbose=verbose)\n",
    "        \n",
    "        # generate optimised neural network\n",
    "        tuner = NNTuner(save_dir, save_name, vector_len, tuning_iterations=tuning_iterations, \n",
    "                        max_epochs=max_epochs, reduction_factor=reduction_factor, force_reload=force_reload, \n",
    "                        batch_size=batch_size)\n",
    "        \n",
    "#         # debug get model\n",
    "#         debug_model = tuner.tuner.get_best_models(num_models=1)[0]\n",
    "#         debug_path = '/nfs/proj-repo/debug_model'\n",
    "#         if not os.path.exists(debug_path):\n",
    "#             os.makedirs(debug_path)\n",
    "#         debug_model.save(debug_path)\n",
    "#         raise Exception(\"Saved debug model\")\n",
    "\n",
    "        best_model_dir = os.path.join(save_dir, \"best_models\")\n",
    "        best_model, best_hyperparams, best_model_path = tuner.search(batch_generator, best_model_dir)\n",
    "        \n",
    "        best_hyperparams = best_hyperparams.values  # convert to dict form\n",
    "        print(\"best_hyperparams: \" + str(best_hyperparams))\n",
    "\n",
    "        # save nn_path_df entry with completed values\n",
    "        self.add_to_nn_path_df(corpus_name, nested_dir, X_col, y_col, save_dir,\n",
    "                               save_name, best_hyperparams, batch_size, best_model_path, \n",
    "                               input_param_text_path, verbose=verbose)\n",
    "        \n",
    "        # add time stamp to param file when finished\n",
    "        cur_time = self.get_cur_time()\n",
    "        end_msg = \"Ended at: \" + str(cur_time) + \"\\n\"\n",
    "        with open(input_param_text_path, \"a\") as param_file:\n",
    "            param_file.write(end_msg)\n",
    "        \n",
    "        print(\"Finished tuning neural network\")\n",
    "    \n",
    "    def get_cur_time(self):\n",
    "        cur_time = strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())\n",
    "        return cur_time\n",
    "    \n",
    "    def load_nn_path_df(self, verbose=True):\n",
    "        if os.path.exists(self.nn_path_df_path):\n",
    "            nn_path_df = read_df_file_type(self.nn_path_df_path, verbose=verbose)\n",
    "        else:\n",
    "            nn_path_df = pd.DataFrame(columns=self.nn_path_df_cols)\n",
    "            if verbose:\n",
    "                print(\"nn_path_df created from scratch\")\n",
    "        return nn_path_df\n",
    "    \n",
    "    def add_to_nn_path_df(self, corpus_name, nested_dir, X_col, y_col, tuner_dir, tuner_name,\n",
    "                          best_hyperparams, batch_size, best_model_path, input_param_text_path, verbose=True):\n",
    "        # check if exists in dataframe\n",
    "        df = self.nn_path_df\n",
    "        mask = (df['corpus_name']==corpus_name)&(df['nested_dir']==nested_dir)&(df['X_col']==X_col)&(df['y_col']==y_col)&(df['tuner_dir']==tuner_dir)&(df['tuner_name']==tuner_name)&(df['batch_size']==batch_size)\n",
    "        exist_slice = df.loc[mask]\n",
    "        if len(exist_slice) == 1:  # update existing row\n",
    "            print(\"Saving to existing row on nn_path_df\")\n",
    "            self.nn_path_df.loc[mask, self.nn_path_df_cols] = corpus_name, nested_dir, X_col, y_col, tuner_dir, tuner_name, best_hyperparams, batch_size, best_model_path, input_param_text_path\n",
    "        else:  # append new row\n",
    "            print(\"Appending new row to nn_path_df\")\n",
    "            row = {\"corpus_name\":corpus_name, \"nested_dir\":nested_dir, \"X_col\":X_col,\n",
    "                  \"y_col\":y_col, \"tuner_dir\":tuner_dir, \"tuner_name\":tuner_name,\n",
    "                  \"best_hyperparams\":best_hyperparams, \"batch_size\":batch_size, \"best_model_path\":best_model_path,\n",
    "                  \"input_param_text_path\":input_param_text_path}\n",
    "            self.nn_path_df = self.nn_path_df.append(row, ignore_index=True)\n",
    "        # save new entry\n",
    "        save_df_file_type(self.nn_path_df, self.nn_path_df_path, verbose=verbose)\n",
    "    \n",
    "    def generate_nn_save_path(self, corpus_name, nested_dir, X_col, y_col, batch_size, create_dir=True):\n",
    "        col_dir = str(X_col) + \"_\" + str(y_col) + \"_\" + str(int(batch_size))\n",
    "        dir_list = [self.nn_base_save_dir_path, corpus_name, nested_dir, col_dir]\n",
    "        # combine directories to form path of subdirectories, create dirs if necessary\n",
    "        dir_path = None\n",
    "        for cur_dir in dir_list:\n",
    "            if dir_path is None:  # first iteration\n",
    "                dir_path = dir_list[0]\n",
    "            else:\n",
    "                dir_path = os.path.join(dir_path, cur_dir)\n",
    "            if not os.path.exists(dir_path) and create_dir:\n",
    "                os.makedirs(dir_path)\n",
    "        # generate name\n",
    "        save_name = \"tuner_proj\"\n",
    "        return dir_path, save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tuning Parameters\n",
    "\n",
    "train_topics = np.arange(11, 47).tolist()  # 11 - 46\n",
    "tuning_iterations = 1\n",
    "max_epochs = 10\n",
    "batch_size = 1024\n",
    "reduction_factor = 8\n",
    "\n",
    "force_reload = False\n",
    "\n",
    "X_col = \"embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from .hdf file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_name</th>\n",
       "      <th>nested_dir</th>\n",
       "      <th>X_col</th>\n",
       "      <th>y_col</th>\n",
       "      <th>tuner_dir</th>\n",
       "      <th>tuner_name</th>\n",
       "      <th>best_hyperparams</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>best_model_path</th>\n",
       "      <th>input_param_text_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mine-trects-kba2014-filtered</td>\n",
       "      <td>stsb-roberta-base</td>\n",
       "      <td>embedding</td>\n",
       "      <td>cos_sim_nearest_nug</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "      <td>tuner_proj</td>\n",
       "      <td>None</td>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mine-trects-kba2014-filtered</td>\n",
       "      <td>stsb-roberta-base</td>\n",
       "      <td>embedding</td>\n",
       "      <td>cos_sim_nearest_nug</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "      <td>tuner_proj</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "      <td>None</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    corpus_name         nested_dir      X_col  \\\n",
       "0  mine-trects-kba2014-filtered  stsb-roberta-base  embedding   \n",
       "1  mine-trects-kba2014-filtered  stsb-roberta-base  embedding   \n",
       "\n",
       "                 y_col                                          tuner_dir  \\\n",
       "0  cos_sim_nearest_nug  /nfs/proj-repo/AAARG-dissertation/tuning_model...   \n",
       "1  cos_sim_nearest_nug  /nfs/proj-repo/AAARG-dissertation/tuning_model...   \n",
       "\n",
       "   tuner_name best_hyperparams batch_size best_model_path  \\\n",
       "0  tuner_proj             None         32            None   \n",
       "1  tuner_proj             None       1024            None   \n",
       "\n",
       "                               input_param_text_path  \n",
       "0  /nfs/proj-repo/AAARG-dissertation/tuning_model...  \n",
       "1  /nfs/proj-repo/AAARG-dissertation/tuning_model...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "loaded from .hdf file\n",
      "Already loaded ['embedding', 'cos_sim_nearest_nug']\n",
      "corpus_name: mine-trects-kba2014-filtered\n",
      "nested_dir: stsb-roberta-base\n",
      "X_input: embedding\n",
      "y_labels: cos_sim_nearest_nug\n",
      "batch_size: 1024\n",
      "train_topics: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
      "max_epochs: 10\n",
      "tuning_iterations: 1\n",
      "reduction_factor: 8\n",
      "estimated trials per iteration: 12.26134029733554\n",
      "total estimated trials: 12.26134029733554\n",
      "Started at: Tue, 02 Mar 2021 18:03:38 +0000\n",
      "\n",
      "Saving to existing row on nn_path_df\n",
      "df saved as hdf complevel 9 at: /nfs/proj-repo/AAARG-dissertation/tuning_models/nn_path_df.hdf\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reloading Oracle from existing project /nfs/proj-repo/AAARG-dissertation/tuning_models/mine-trects-kba2014-filtered/stsb-roberta-base/embedding_cos_sim_nearest_nug_1024/tuner_proj/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:2449: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['corpus_name', 'nested_dir', 'X_col', 'y_col', 'tuner_dir',\n",
      "       'tuner_name', 'best_hyperparams', 'batch_size', 'best_model_path',\n",
      "       'input_param_text_path'],\n",
      "      dtype='object')]\n",
      "\n",
      "  encoding=encoding,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "num_hidden_layers |2                 |?                 \n",
      "hidden_units_0    |1088              |?                 \n",
      "learning_rate     |0.090382          |?                 \n",
      "tuner/epochs      |2                 |?                 \n",
      "tuner/initial_e...|0                 |?                 \n",
      "tuner/bracket     |1                 |?                 \n",
      "tuner/round       |0                 |?                 \n",
      "\n",
      "Epoch 1/2\n",
      " 8215/28785 [=======>......................] - ETA: 12:09 - loss: 0.0036 - mean_squared_error: 0.0072"
     ]
    }
   ],
   "source": [
    "corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "nested_dir = \"stsb-roberta-base\"\n",
    "y_col = \"cos_sim_nearest_nug\"\n",
    "trainer = NNTrainer()\n",
    "trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, tuning_iterations=tuning_iterations,\n",
    "              max_epochs=max_epochs, batch_size=batch_size, force_reload=force_reload, verbose=True,\n",
    "              reduction_factor=reduction_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from .hdf file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_name</th>\n",
       "      <th>nested_dir</th>\n",
       "      <th>X_col</th>\n",
       "      <th>y_col</th>\n",
       "      <th>tuner_dir</th>\n",
       "      <th>tuner_name</th>\n",
       "      <th>best_hyperparams</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>best_model_path</th>\n",
       "      <th>input_param_text_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mine-trects-kba2014-filtered</td>\n",
       "      <td>stsb-roberta-base</td>\n",
       "      <td>embedding</td>\n",
       "      <td>cos_sim_nearest_nug</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "      <td>tuner_proj</td>\n",
       "      <td>None</td>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mine-trects-kba2014-filtered</td>\n",
       "      <td>stsb-roberta-base</td>\n",
       "      <td>embedding</td>\n",
       "      <td>cos_sim_nearest_nug</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "      <td>tuner_proj</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "      <td>None</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    corpus_name         nested_dir      X_col  \\\n",
       "0  mine-trects-kba2014-filtered  stsb-roberta-base  embedding   \n",
       "1  mine-trects-kba2014-filtered  stsb-roberta-base  embedding   \n",
       "\n",
       "                 y_col                                          tuner_dir  \\\n",
       "0  cos_sim_nearest_nug  /nfs/proj-repo/AAARG-dissertation/tuning_model...   \n",
       "1  cos_sim_nearest_nug  /nfs/proj-repo/AAARG-dissertation/tuning_model...   \n",
       "\n",
       "   tuner_name best_hyperparams batch_size best_model_path  \\\n",
       "0  tuner_proj             None         32            None   \n",
       "1  tuner_proj             None       1024            None   \n",
       "\n",
       "                               input_param_text_path  \n",
       "0  /nfs/proj-repo/AAARG-dissertation/tuning_model...  \n",
       "1  /nfs/proj-repo/AAARG-dissertation/tuning_model...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "loaded from .hdf file\n",
      "Creating memmaps for embedding, cosine_similarity\n",
      "with topics: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddf78da6d824683a5dc98eae4aed01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/692 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-6865ea536c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, tuning_iterations=tuning_iterations,\n\u001b[1;32m      6\u001b[0m               \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               reduction_factor=reduction_factor)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-6ea1f3f87201>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_name, nested_dir, topic_ids, X_col, y_col, tuning_iterations, max_epochs, reduction_factor, batch_size, force_reload, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmmap_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemmapGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         mmap_gen.create_maps(corpus_name, nested_dir, [X_col, y_col], topic_ids, verbose=verbose,\n\u001b[0;32m---> 41\u001b[0;31m                                          force_reload=False)  # setting to False for debug\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# get paths for inputs and total_len of samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-35ec96df3aee>\u001b[0m in \u001b[0;36mcreate_maps\u001b[0;34m(self, corpus_name, nested_dir, col_labels, topic_ids, verbose, force_reload)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;31m# get the cols that haven't been loaded for this path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# scrape data from dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mlabel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_col_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# add data to memmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcol_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-35ec96df3aee>\u001b[0m in \u001b[0;36mscrape_col_data\u001b[0;34m(self, emb_path, col_labels)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# setup return variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0memb_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcol_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memb_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/proj-repo/AAARG-dissertation/notebooks/corpus_loader.ipynb\u001b[0m in \u001b[0;36mload_embeddings\u001b[0;34m(save_path, emb_model, corpus_df, nugget_df, sents_default, only_docs_with_nugs, nlp, force_reload, save, verbose, path_handler)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;34m\"        print(\\\"There were \\\" + str(len(missed_nuggetids)) + \\\" nugget_ids found in matches.tsv but not in nuggets.tsv\\\")\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;34m\"        print(str(len(missed_sentid_streamids)) + \\\" out of \\\" + str(len(entry_list)) + \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m     \u001b[0;34m\"              \\\" streamids had out of bounds sent_ids\\\")\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m     \u001b[0;34m\"        \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;34m\"    nugget_df = pd.DataFrame(entry_list)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/proj-repo/AAARG-dissertation/notebooks/corpus_loader.ipynb\u001b[0m in \u001b[0;36mload_df_control\u001b[0;34m(save_path, load_func, save, force_reload, name, verbose, path_handler)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;34m\"def find_duplicates(df):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;34m\"    seen = set()\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m     \u001b[0;34m\"    seen_twice = set()\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m     \u001b[0;34m\"    for docid in df['docid']:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;34m\"        if docid not in seen:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/proj-repo/AAARG-dissertation/notebooks/corpus_loader.ipynb\u001b[0m in \u001b[0;36mread_df_file_type\u001b[0;34m(save_path, verbose, concat_multiple)\u001b[0m\n\u001b[1;32m    542\u001b[0m    \"source\": [\n\u001b[1;32m    543\u001b[0m     \u001b[0;34m\"def topic_id_as_int(topic_id):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0;34m\"    try:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m     \u001b[0;34m\"        topic_id = int(topic_id)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;34m\"        return topic_id\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mauto_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_close\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def select_as_coordinates(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, coordinates)\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# directly return the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1922\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(_start, _stop, _where)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;31m# function to call on iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_where\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_where\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;31m# create the iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, where, columns, start, stop)\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m             \u001b[0mblk_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"block{i}_items\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"block{i}_values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(self, key, start, stop)\u001b[0m\n\u001b[1;32m   2816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVLArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2818\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2819\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_decoded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tables/vlarray.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             start, stop, step = self._process_range(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tables/vlarray.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mlistarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0mlistarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0matom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "nested_dir = \"stsb-roberta-base\"\n",
    "y_col = \"cosine_similarity\"\n",
    "trainer = NNTrainer()\n",
    "trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, tuning_iterations=tuning_iterations,\n",
    "              max_epochs=max_epochs, batch_size=batch_size, force_reload=force_reload, verbose=True,\n",
    "              reduction_factor=reduction_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from .hdf file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_name</th>\n",
       "      <th>nested_dir</th>\n",
       "      <th>X_col</th>\n",
       "      <th>y_col</th>\n",
       "      <th>tuner_dir</th>\n",
       "      <th>tuner_name</th>\n",
       "      <th>best_hyperparams</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>best_model_path</th>\n",
       "      <th>input_param_text_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mine-trects-kba2014-filtered</td>\n",
       "      <td>stsb-roberta-base</td>\n",
       "      <td>embedding</td>\n",
       "      <td>cos_sim_nearest_nug</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "      <td>tuner_proj</td>\n",
       "      <td>None</td>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mine-trects-kba2014-filtered</td>\n",
       "      <td>stsb-roberta-base</td>\n",
       "      <td>embedding</td>\n",
       "      <td>cos_sim_nearest_nug</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "      <td>tuner_proj</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "      <td>None</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    corpus_name         nested_dir      X_col  \\\n",
       "0  mine-trects-kba2014-filtered  stsb-roberta-base  embedding   \n",
       "1  mine-trects-kba2014-filtered  stsb-roberta-base  embedding   \n",
       "\n",
       "                 y_col                                          tuner_dir  \\\n",
       "0  cos_sim_nearest_nug  /nfs/proj-repo/AAARG-dissertation/tuning_model...   \n",
       "1  cos_sim_nearest_nug  /nfs/proj-repo/AAARG-dissertation/tuning_model...   \n",
       "\n",
       "   tuner_name best_hyperparams batch_size best_model_path  \\\n",
       "0  tuner_proj             None         32            None   \n",
       "1  tuner_proj             None       1024            None   \n",
       "\n",
       "                               input_param_text_path  \n",
       "0  /nfs/proj-repo/AAARG-dissertation/tuning_model...  \n",
       "1  /nfs/proj-repo/AAARG-dissertation/tuning_model...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "loaded from .hdf file\n",
      "Creating memmaps for embedding, cos_sim_nearest_nug\n",
      "with topics: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:2449: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['corpus_name', 'nested_dir', 'col_label', 'path', 'dtype', 'vector_len',\n",
      "       'total_nums', 'offset_step', 'topic_ids', 'complete'],\n",
      "      dtype='object')]\n",
      "\n",
      "  encoding=encoding,\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dd5c9185b44e59ba0dd1771b7b0a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/692 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-5964688e1963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, tuning_iterations=tuning_iterations,\n\u001b[1;32m      6\u001b[0m               \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               reduction_factor=reduction_factor)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-97-b5697057396b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_name, nested_dir, topic_ids, X_col, y_col, tuning_iterations, max_epochs, reduction_factor, batch_size, force_reload, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmmap_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemmapGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         mmap_gen.create_maps(corpus_name, nested_dir, [X_col, y_col], topic_ids, verbose=verbose,\n\u001b[0;32m---> 41\u001b[0;31m                                          force_reload=False)  # setting to False for debug\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# get paths for inputs and total_len of samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-35ec96df3aee>\u001b[0m in \u001b[0;36mcreate_maps\u001b[0;34m(self, corpus_name, nested_dir, col_labels, topic_ids, verbose, force_reload)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;31m# get the cols that haven't been loaded for this path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# scrape data from dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mlabel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_col_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# add data to memmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcol_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-35ec96df3aee>\u001b[0m in \u001b[0;36mscrape_col_data\u001b[0;34m(self, emb_path, col_labels)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target label \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is not in file at \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;31m# collect label values from df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mlabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "nested_dir = \"distilbert-base-nli-stsb-mean-tokens\"\n",
    "y_col = \"cos_sim_nearest_nug\"\n",
    "trainer = NNTrainer()\n",
    "trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, tuning_iterations=tuning_iterations,\n",
    "              max_epochs=max_epochs, batch_size=batch_size, force_reload=force_reload, verbose=True,\n",
    "              reduction_factor=reduction_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "nested_dir = \"distilbert-base-nli-stsb-mean-tokens\"\n",
    "y_col = \"cosine_similarity\"\n",
    "trainer = NNTrainer()\n",
    "trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, tuning_iterations=tuning_iterations,\n",
    "              max_epochs=max_epochs, batch_size=batch_size, force_reload=force_reload, verbose=True,\n",
    "              reduction_factor=reduction_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Similarity Threshold for Redundant Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .defs.corpus_loader import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_predict_batches(emb_list, num_dims, batch_size):\n",
    "    # add empty rows to complete batch_size\n",
    "    num_empty_rows = batch_size - (len(emb_list) % batch_size)\n",
    "    if num_empty_rows > 0:\n",
    "        empty_rows = [np.zeros(num_dims)] * num_empty_rows\n",
    "        emb_list.extend(empty_rows)\n",
    "    # shape into batches\n",
    "    emb_arr = np.asarray(emb_list)\n",
    "    num_batches = int(len(emb_arr) / batch_size)\n",
    "    emb_arr = emb_arr.reshape(num_batches, batch_size, num_dims)\n",
    "    return emb_arr, num_empty_rows\n",
    "\n",
    "def predict_emb_list(model, emb_list, batch_size=256):\n",
    "    # create batches, where does not divide evenly, fill with empty rows\n",
    "    emb_batches, num_empty_rows = form_predict_batches(emb_list, len(emb_list[0]), batch_size)\n",
    "    preds = model.predict(emb_batches, \n",
    "                          batch_size=batch_size,\n",
    "                          verbose=1, \n",
    "                          use_multiprocessing=True, \n",
    "                          workers=num_threads)\n",
    "\n",
    "    # format predictions to add to df\n",
    "    preds = preds.flatten()  # undo batch shape\n",
    "    preds = preds[0:len(preds)-num_empty_rows]  # remove empty row predictions\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell loaded\n"
     ]
    }
   ],
   "source": [
    "class RedundancyThresholdGenerator:\n",
    "    def __init__(self, proj_repo='/nfs/proj-repo/AAARG-dissertation'):\n",
    "        self.path_ret = PathRetriever(proj_repo)\n",
    "        \n",
    "    def get_emb_paths(self, corpus_name, nested_dir, topic_ids):\n",
    "        emb_paths, emb_dir = self.path_ret.get_embedding_paths(corpus_name, nested_dir, topic_ids=topic_ids, \n",
    "                                                      return_dir_path=True)\n",
    "        emb_paths = list(emb_paths['path'])\n",
    "        return emb_paths, emb_dir\n",
    "    \n",
    "    def get_top_sentence_scores(self, emb_paths, model, k=10000, batch_size=256):        \n",
    "        # create empty df with default vals\n",
    "        df_empty_dict = {}\n",
    "        df_empty_dict[\"sentence\"] = [\"empty\"] * k\n",
    "        df_empty_dict[\"score\"] = [np.NINF] * k\n",
    "        score_df = pd.DataFrame(df_empty_dict)\n",
    "        \n",
    "        \n",
    "        # find top k scoring sentences from emb_paths\n",
    "        for emb_path in tqdm_notebook(emb_paths):\n",
    "            # load emb_df\n",
    "            emb_df = load_embeddings(emb_path, verbose=False)\n",
    "#             print(\"emb_df\")\n",
    "#             print(display(emb_df))\n",
    "            \n",
    "            # scrape embeddings\n",
    "            embs = list(emb_df['embedding'])\n",
    "            \n",
    "            # predict embeddings\n",
    "            print(\"Predicting emb_df's embeddings\")\n",
    "            preds = predict_emb_list(model, embs, batch_size=batch_size)\n",
    "            print(\"predictions 0-20\")\n",
    "            print(preds[0:20])\n",
    "            \n",
    "            # tie sentence to predicted score\n",
    "            print(\"Creating prediction dataframe\")\n",
    "            pred_dict = {\"sentence\":emb_df['sentence'], \"score\":preds}\n",
    "            pred_df = pd.DataFrame(pred_dict)\n",
    "            pred_df = pred_df.sort_values(by=\"score\", ascending=False, ignore_index=True) # descending order\n",
    "#             print(\"pred_df\")\n",
    "#             print(display(pred_df))\n",
    "#             print(\"score_df\")\n",
    "#             print(display(score_df))\n",
    "            \n",
    "            # add scores where values within top k\n",
    "            bottom_score = score_df['score'][0]\n",
    "            pred_df_highest = pred_df['score'][0]\n",
    "            print(\"bottom_score/pred_df_highest: \" + str(bottom_score) + \" / \" + str(pred_df_highest))\n",
    "            if pred_df_highest > bottom_score:\n",
    "                \n",
    "                # find how many scores to add\n",
    "                print(\"Adding new rows\")\n",
    "                num_rows_add = 0\n",
    "                for index, row in pred_df.iterrows():\n",
    "                    row_score = row['score']\n",
    "                    if row_score > bottom_score:\n",
    "                        num_rows_add += 1\n",
    "                    else:\n",
    "                        break  # found all rows that are within current top k\n",
    "                if num_rows_add > k:\n",
    "                    num_rows_add = k\n",
    "                \n",
    "                # add scores to score_df and sort\n",
    "                score_df.loc[range(num_rows_add), score_df.columns] = pred_df[0:num_rows_add]\n",
    "                score_df = score_df.sort_values(by=\"score\", ignore_index=True, ascending=True)\n",
    "                \n",
    "                if len(score_df) > k:\n",
    "                    raise Exception(\"Score df len \" + str(len(score_df)) + \" is greater than k \" + str(k))\n",
    "            else:\n",
    "                print(\"No new rows to be added\")\n",
    "#             print(\"score_df\")\n",
    "#             print(display(score_df))\n",
    "        return score_df\n",
    "    \n",
    "    def compare_sim_nuggets(self, emb_dir, sample_embs):\n",
    "        \"\"\"\n",
    "        Retrieve similarities between sample_embs where:\n",
    "            - sample_embs share the same nearest nugget\n",
    "            - sample_embs do not share the same nearest nugget\n",
    "        \"\"\"\n",
    "        # load nugget embeddings\n",
    "        nug_embs_path = os.path.join(emb_dir, \"embedding_list_all.pickle\")\n",
    "        nug_embs = None\n",
    "        with open(nug_embs_path, 'rb') as handle:\n",
    "            nug_embs = pickle.load(handle)\n",
    "            \n",
    "        # for each sample_emb, find index of nearest nug\n",
    "        nearest_nug_indexes = np.zeros(len(sample_embs))  # sample_emb_idx -> nearest_nug_idx\n",
    "        print(\"Finding nearest nug for each sample embedding\")\n",
    "        for sample_index, sample_emb in tqdm_notebook(enumerate(sample_embs)):\n",
    "            # get index of max for this sample_emb\n",
    "            max_idx = None\n",
    "            max_sim = -np.inf\n",
    "            for nug_index, nug_emb in enumerate(nug_embs):\n",
    "                cur_sim = cosine_similarity(sample_emb, nug_emb)\n",
    "                if cur_sim > max_sim:\n",
    "                    max_sim = cur_sim\n",
    "                    max_idx = nug_index\n",
    "            # store max idx\n",
    "            nearest_nug_indexes[sample_index] = max_index\n",
    "            \n",
    "        \n",
    "        # collect cosine similarities of sample embeddings that have same nearest nug, and those that don't\n",
    "        same_nug_sims = []\n",
    "        dif_nug_sims = []\n",
    "        print(\"Finding cosine similarities between sample embeddings\")\n",
    "        for x in tqdm_notebook(range(len(sample_embs - 1))):\n",
    "            for y in range(x, len(sample_embs)):\n",
    "                # get cosine_similarity between sample embs x and y\n",
    "                x_emb = sample_embs[x]\n",
    "                y_emb = sample_embs[y]\n",
    "                sim = cosine_similarity(x_emb, y_emb)\n",
    "                # check if x and y have same nearest nug\n",
    "                x_nearest = nearest_nug_indexes[x]\n",
    "                y_nearest = nearest_nug_indexes[y]\n",
    "                if x_nearest == y_nearest:  # share same nearest_nug\n",
    "                    same_nug_sims.append(sim)\n",
    "                else:\n",
    "                    dif_nug_sims.append(sim)\n",
    "        \n",
    "        # sort similarities\n",
    "        same_nug_sims = np.sort(np.asarray(same_nug_sims))\n",
    "        dif_nug_sims = np.sort(np.asarray(dif_nug_sims))   \n",
    "        return same_nug_sims, dif_nug_sims\n",
    "        \n",
    "    def plot_sim_nugget_distributions(self, same_nug_sims, dif_nug_sims):\n",
    "        # https://stackoverflow.com/a/24567715\n",
    "        fig = plt.gcf()  # might need to change so gets new figure?\n",
    "        \n",
    "        \n",
    "        # plot cumulative distribution stepwise\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.step(np.concatenate([same_nug_sims, same_nug_sims[[-1]]]), \n",
    "                 np.arange(same_nug_sims.size+1),\n",
    "                label='shared most similar nugget')\n",
    "        plt.step(np.concatenate([dif_nug_sims, dif_nug_sims[[-1]]]), \n",
    "                 np.arange(dif_nug_sims.size+1),\n",
    "                label='different most similar nugget')\n",
    "        \n",
    "        plt.title(\"Distribution of cosine similarities between sample sentences (cumulative)\")\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Cosine Similarity')\n",
    "        plt.legend(loc='upper_left')\n",
    "#         plt.xlim([min_val, max_val])\n",
    "#         plt.ylim([0, ])\n",
    "        plt.grid()\n",
    "    \n",
    "        plt.show()\n",
    "        \n",
    "print(\"cell loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from .hdf file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_name</th>\n",
       "      <th>nested_dir</th>\n",
       "      <th>X_col</th>\n",
       "      <th>y_col</th>\n",
       "      <th>tuner_dir</th>\n",
       "      <th>tuner_name</th>\n",
       "      <th>best_hyperparams</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>best_model_path</th>\n",
       "      <th>input_param_text_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mine-trects-kba2014-filtered</td>\n",
       "      <td>stsb-roberta-base</td>\n",
       "      <td>embedding</td>\n",
       "      <td>cos_sim_nearest_nug</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "      <td>tuner_proj</td>\n",
       "      <td>None</td>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mine-trects-kba2014-filtered</td>\n",
       "      <td>stsb-roberta-base</td>\n",
       "      <td>embedding</td>\n",
       "      <td>cos_sim_nearest_nug</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "      <td>tuner_proj</td>\n",
       "      <td>{'num_hidden_layers': 2, 'hidden_units_0': 897...</td>\n",
       "      <td>1024</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "      <td>/nfs/proj-repo/AAARG-dissertation/tuning_model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    corpus_name         nested_dir      X_col  \\\n",
       "0  mine-trects-kba2014-filtered  stsb-roberta-base  embedding   \n",
       "1  mine-trects-kba2014-filtered  stsb-roberta-base  embedding   \n",
       "\n",
       "                 y_col                                          tuner_dir  \\\n",
       "0  cos_sim_nearest_nug  /nfs/proj-repo/AAARG-dissertation/tuning_model...   \n",
       "1  cos_sim_nearest_nug  /nfs/proj-repo/AAARG-dissertation/tuning_model...   \n",
       "\n",
       "   tuner_name                                   best_hyperparams batch_size  \\\n",
       "0  tuner_proj                                               None         32   \n",
       "1  tuner_proj  {'num_hidden_layers': 2, 'hidden_units_0': 897...       1024   \n",
       "\n",
       "                                     best_model_path  \\\n",
       "0                                               None   \n",
       "1  /nfs/proj-repo/AAARG-dissertation/tuning_model...   \n",
       "\n",
       "                               input_param_text_path  \n",
       "0  /nfs/proj-repo/AAARG-dissertation/tuning_model...  \n",
       "1  /nfs/proj-repo/AAARG-dissertation/tuning_model...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# def get_debug_model():\n",
    "#     model_path = '/nfs/proj-repo/debug_model'\n",
    "#     model = tf.keras.models.load_model(model_path)\n",
    "#     print(\"loaded model\")\n",
    "#     return model\n",
    "\n",
    "model_path_handler = ModelPathHandler()\n",
    "print(display(model_path_handler.df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_hyperparams\n",
      "{'num_hidden_layers': 2, 'hidden_units_0': 897, 'hidden_activ_0': 'tanh', 'output_activ': 'tanh', 'learning_rate': 0.05305450445016687, 'tuner/epochs': 15, 'tuner/initial_epoch': 2, 'tuner/bracket': 1, 'tuner/round': 1, 'hidden_units_1': 1, 'hidden_activ_1': 'relu', 'tuner/trial_id': 'b182cd2325c30a3b47721e93f6de69b1'}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_layer_0 (Dense)       (256, 897)                689793    \n",
      "_________________________________________________________________\n",
      "hidden_layer_1 (Dense)       (256, 1)                  898       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (256, 1)                  2         \n",
      "=================================================================\n",
      "Total params: 690,693\n",
      "Trainable params: 690,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "nested_dir = \"stsb-roberta-base\"\n",
    "X_col = \"embedding\"\n",
    "y_col = \"cos_sim_nearest_nug\"\n",
    "batch_size = 1024\n",
    "\n",
    "model = model_path_handler.load_best_model(corpus_name, nested_dir, X_col, y_col, batch_size)\n",
    "tuner_inst_mask = model_path_handler.create_df_mask(corpus_name, nested_dir, X_col, y_col, batch_size)\n",
    "model_df = model_path_handler.df\n",
    "tuner_inst = model_df.loc[tuner_inst_mask]\n",
    "best_hyperparams = list(tuner_inst['best_hyperparams'])[0]\n",
    "print(\"best_hyperparams\")\n",
    "print(best_hyperparams)\n",
    "\n",
    "# model_path = \"/nfs/proj-repo/AAARG-dissertation/tuning_models/mine-trects-kba2014-filtered/stsb-roberta-base/embedding_cos_sim_nearest_nug_1024/best_models/0\"\n",
    "# model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_gen = RedundancyThresholdGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings paths for mine-trects-kba2014-filtered\n"
     ]
    }
   ],
   "source": [
    "test_ids = np.arange(11, 15).tolist()\n",
    "test_paths, test_emb_dir = sum_gen.get_emb_paths(corpus_name, nested_dir, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_df_test(emb_path):\n",
    "    emb_df = load_embeddings(emb_path, verbose=False)\n",
    "    print(display(emb_df[0:200]))\n",
    "    \n",
    "# show_df_test(test_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bcaba9414140e8b54853cc5fadfc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting emb_df's embeddings\n",
      "emb_batches 0-5\n",
      "[[[ 1.31808281e+00 -1.35149360e+00 -2.16791630e-01 ...  8.92709494e-01\n",
      "    7.69516230e-01 -1.19040012e+00]\n",
      "  [ 2.24484995e-01 -2.57765383e-01 -3.50602478e-01 ...  1.90732732e-01\n",
      "    3.17092270e-01 -9.16033447e-01]\n",
      "  [-2.82743037e-01  9.51191068e-01  1.54983687e+00 ... -1.56537127e+00\n",
      "   -5.88145435e-01  1.73127079e+00]\n",
      "  ...\n",
      "  [-8.03759634e-01 -9.36938167e-01 -1.48505911e-01 ...  7.28978217e-01\n",
      "   -4.84286100e-02  7.45753944e-02]\n",
      "  [ 4.77898121e-01  3.02714594e-02  5.03240060e-03 ... -1.11172009e+00\n",
      "   -1.85167104e-01 -8.78115356e-01]\n",
      "  [ 4.35933620e-01 -5.11561990e-01  8.28448772e-01 ... -2.12003872e-01\n",
      "   -1.27906501e-02  4.35475916e-01]]\n",
      "\n",
      " [[-5.42698860e-01 -3.42831537e-02 -5.71567416e-01 ... -1.72929430e+00\n",
      "    4.67046857e-01  7.81671047e-01]\n",
      "  [ 9.47753862e-02 -4.22628522e-01 -4.77715105e-01 ... -1.01307905e+00\n",
      "    5.91164291e-01 -8.80811661e-02]\n",
      "  [ 5.52666187e-01 -5.20555489e-02 -1.00115508e-01 ... -3.36523950e-01\n",
      "    7.93765113e-02 -2.06316486e-01]\n",
      "  ...\n",
      "  [-2.54102349e-01  3.62275064e-01  1.03607631e+00 ...  1.94325179e-01\n",
      "   -3.13323259e-01  5.97051322e-01]\n",
      "  [ 2.71892309e-01 -2.25213945e-01 -1.37154698e+00 ...  1.18464196e+00\n",
      "    2.22240210e-01  3.25319976e-01]\n",
      "  [ 2.82880869e-02 -1.14556921e+00 -1.07926145e-01 ...  8.55506659e-02\n",
      "    3.56364071e-01  5.53671084e-02]]\n",
      "\n",
      " [[-1.55455530e-01  2.24634483e-02 -5.70103228e-01 ... -8.27357888e-01\n",
      "    2.88802832e-01  4.32333946e-01]\n",
      "  [-8.66658613e-02  3.20538014e-01 -1.97335947e+00 ...  1.69343138e+00\n",
      "    2.61453241e-01 -2.34410316e-01]\n",
      "  [-5.34374058e-01 -4.14951503e-01 -5.22276938e-01 ... -8.54681134e-02\n",
      "   -2.64697522e-01 -2.22903676e-04]\n",
      "  ...\n",
      "  [ 1.31808305e+00 -1.35149348e+00 -2.16791824e-01 ...  8.92709851e-01\n",
      "    7.69516230e-01 -1.19039953e+00]\n",
      "  [ 2.40391433e-01 -7.41775870e-01 -3.65526676e-01 ...  1.61575139e-01\n",
      "    7.49203503e-01  3.77499573e-02]\n",
      "  [ 1.03972149e+00 -2.64803320e-01  3.25971544e-02 ... -3.69974047e-01\n",
      "    1.24501276e+00 -5.30864112e-02]]\n",
      "\n",
      " [[ 2.84640074e-01 -8.82832289e-01 -2.08279677e-03 ...  3.15619744e-02\n",
      "   -1.36774272e-01  1.70299876e+00]\n",
      "  [-5.25594234e-01 -6.65962696e-02 -1.16242077e-02 ...  1.19587541e-01\n",
      "    2.86450654e-01  7.61098554e-03]\n",
      "  [ 4.80115741e-01 -1.38671517e+00 -2.52145737e-01 ...  2.54590362e-01\n",
      "    3.34121101e-02 -1.46322340e-01]\n",
      "  ...\n",
      "  [ 5.24590850e-01 -5.14591575e-01 -6.33945346e-01 ...  6.05354965e-01\n",
      "    6.22362792e-01 -4.27287258e-02]\n",
      "  [ 5.96348345e-01 -1.12746465e+00 -4.41449434e-02 ... -7.32473254e-01\n",
      "    1.25458717e+00 -5.71087360e-01]\n",
      "  [ 5.03971219e-01 -7.17284083e-01  4.85618338e-02 ... -6.30289674e-01\n",
      "    1.46881855e+00 -8.97770405e-01]]\n",
      "\n",
      " [[ 1.39689624e-01 -1.10691607e+00 -7.25582838e-01 ...  2.41096824e-01\n",
      "    4.34835792e-01 -5.37135959e-01]\n",
      "  [ 1.86039478e-01 -6.64112687e-01 -3.30602229e-01 ...  8.51937413e-01\n",
      "    4.63675112e-02  1.92312896e-01]\n",
      "  [ 2.43181527e-01 -1.88595757e-01 -8.74712408e-01 ...  6.77059591e-01\n",
      "    4.09373522e-01 -3.78060311e-01]\n",
      "  ...\n",
      "  [-6.82840347e-01 -1.22387314e+00 -1.71277380e+00 ...  6.23031020e-01\n",
      "    3.27205896e-01  1.93740949e-01]\n",
      "  [-2.88729548e-01 -6.84150875e-01  1.12507522e-01 ...  1.97492826e+00\n",
      "   -4.16083597e-02  9.92517710e-01]\n",
      "  [ 8.97377282e-02  1.22011386e-01  2.92182446e-01 ...  3.90202582e-01\n",
      "    7.88991153e-03  3.10051501e-01]]]\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "predictions 0-20\n",
      "[-0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045]\n",
      "Creating prediction dataframe\n",
      "bottom_score/pred_df_highest: -inf / -0.0053904485\n",
      "Adding new rows\n",
      "Predicting emb_df's embeddings\n",
      "emb_batches 0-5\n",
      "[[[ 1.31808329 -1.3514936  -0.21679193 ...  0.89270926  0.76951665\n",
      "   -1.19039893]\n",
      "  [-0.38534883 -0.45985141 -0.62505019 ...  0.08370461 -0.1904918\n",
      "    0.86677206]\n",
      "  [-0.3719835  -0.5829317  -0.21712835 ...  0.5537281   0.18954036\n",
      "    0.47038987]\n",
      "  ...\n",
      "  [-0.04508932 -0.21505402 -0.48248026 ...  0.14125571  0.50564539\n",
      "    0.6283567 ]\n",
      "  [ 0.03172049 -0.64086163  0.07306741 ...  0.54979908  0.19474547\n",
      "    0.46198997]\n",
      "  [-0.32250819 -0.04373718 -0.46615371 ...  0.05661875  0.10164705\n",
      "    0.85005349]]\n",
      "\n",
      " [[ 0.7927196  -0.18684527 -0.57849097 ...  0.69613498 -0.20045762\n",
      "    0.73381603]\n",
      "  [ 0.57779676 -0.39182061 -0.65064859 ... -0.32737452 -0.13773718\n",
      "    0.11145888]\n",
      "  [-0.37089455 -0.36694902 -0.29230788 ... -0.33193448 -0.28130937\n",
      "   -0.18305099]\n",
      "  ...\n",
      "  [-0.59870714 -0.1293391   0.22858769 ... -1.4640522  -0.35571751\n",
      "    1.53969717]\n",
      "  [ 0.69533235 -1.02225816 -0.80904508 ... -1.08173048  0.05592695\n",
      "    1.02110946]\n",
      "  [-0.61979383 -0.05435692 -0.10133326 ... -0.34952068  0.64128041\n",
      "   -0.77280968]]\n",
      "\n",
      " [[ 0.81383783 -0.52677906  0.54214305 ...  0.33832547 -0.31441963\n",
      "   -0.06475075]\n",
      "  [-0.14227474 -0.94443578 -0.08576566 ...  0.21645838  0.1423267\n",
      "    1.58239079]\n",
      "  [ 0.21938787 -1.01538384  0.10367612 ... -0.8483122   0.06701492\n",
      "    0.54355711]\n",
      "  ...\n",
      "  [ 0.29770589  0.17403841  0.24685884 ... -0.25942907 -0.45603317\n",
      "   -1.45406437]\n",
      "  [ 0.14771968 -1.19446802 -0.40194482 ...  0.74565738 -0.34974924\n",
      "    0.22512674]\n",
      "  [ 0.12743933 -0.04231401  0.21436016 ... -0.14787379  0.74890304\n",
      "   -0.49513966]]\n",
      "\n",
      " [[ 0.38602185 -0.54947168  0.39112568 ... -0.23345175  0.34229073\n",
      "    1.73921466]\n",
      "  [ 0.05475456 -0.92743826  0.88810343 ... -1.33384454  0.18534957\n",
      "    0.79127759]\n",
      "  [ 0.14771968 -1.19446802 -0.40194482 ...  0.74565738 -0.34974924\n",
      "    0.22512674]\n",
      "  ...\n",
      "  [-0.09724858  0.69540626 -0.20954318 ...  0.32627368 -0.3194755\n",
      "    0.34464589]\n",
      "  [-0.37092617 -0.12059851 -0.08522095 ...  0.88742912 -0.68055993\n",
      "   -0.12035346]\n",
      "  [-0.1607946   0.60523188 -0.4648332  ... -0.3122347  -0.01043375\n",
      "    1.09413159]]\n",
      "\n",
      " [[-0.05739521 -0.28847247 -0.66291201 ...  0.74964732 -0.27511257\n",
      "    0.07531811]\n",
      "  [ 0.88406545 -1.67626035 -0.48877493 ...  0.31062421 -0.331034\n",
      "   -0.28899264]\n",
      "  [-0.28524345 -0.32343704  0.34610206 ... -0.21541566  0.39438647\n",
      "    0.05807427]\n",
      "  ...\n",
      "  [ 1.13375521 -0.44370884  0.21869762 ... -0.4129959   0.19234203\n",
      "    0.80455089]\n",
      "  [ 0.39194292 -0.48833209 -0.28242481 ... -0.93302476 -0.0342833\n",
      "   -0.31557572]\n",
      "  [ 0.10154456 -0.58854848  0.24896716 ...  0.06645588  0.35272452\n",
      "    0.07088127]]]\n",
      "1/1 [==============================] - 0s 1ms/step\n",
      "predictions 0-20\n",
      "[-0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045]\n",
      "Creating prediction dataframe\n",
      "bottom_score/pred_df_highest: -0.0053904485 / -0.0053904485\n",
      "No new rows to be added\n",
      "Predicting emb_df's embeddings\n",
      "emb_batches 0-5\n",
      "[[[ 1.31808305 -1.35149384 -0.2167922  ...  0.89271057  0.76951683\n",
      "   -1.19039989]\n",
      "  [-0.14915542  0.1778919   0.34358463 ... -0.66614383 -0.01025611\n",
      "   -0.39710239]\n",
      "  [ 0.40425837 -0.50803083  0.52733409 ... -0.33555949  0.10405703\n",
      "   -0.29296207]\n",
      "  ...\n",
      "  [ 0.79388386  0.18407449  0.15560625 ...  1.39779997  0.12014069\n",
      "   -0.25956038]\n",
      "  [-0.78075343 -0.48236322 -0.36789164 ...  0.49051613 -0.18931779\n",
      "    0.59629339]\n",
      "  [-0.45810682 -0.09817576  0.07884929 ... -0.1643032  -0.46696326\n",
      "   -0.39840204]]\n",
      "\n",
      " [[-0.78075343 -0.48236322 -0.36789164 ...  0.49051613 -0.18931779\n",
      "    0.59629339]\n",
      "  [ 0.53475434 -0.38067761 -0.64713591 ...  0.86889023 -0.15494902\n",
      "    0.8076607 ]\n",
      "  [-0.78075343 -0.48236322 -0.36789164 ...  0.49051613 -0.18931779\n",
      "    0.59629339]\n",
      "  ...\n",
      "  [ 0.74212909  0.01103216  0.85873848 ... -0.78196555 -0.31555018\n",
      "    1.12230647]\n",
      "  [ 0.09499728 -0.5729754  -0.13322471 ... -1.98034561  1.08646727\n",
      "    0.196482  ]\n",
      "  [ 1.082775   -0.58089554 -0.49350873 ... -0.64890903 -0.05351635\n",
      "   -0.38328579]]\n",
      "\n",
      " [[ 0.67310441 -1.67551184 -0.25560156 ...  0.02558167 -0.11211541\n",
      "   -0.322458  ]\n",
      "  [ 0.06065269 -1.03045535 -0.02923701 ...  0.18701522 -0.00232757\n",
      "   -0.48973095]\n",
      "  [ 0.3782497  -0.96829689  0.4820089  ... -0.0928445   0.08987875\n",
      "    0.30431452]\n",
      "  ...\n",
      "  [ 0.16549167  0.90627813  0.08656464 ...  0.32786292  0.01782251\n",
      "    1.04047465]\n",
      "  [-0.81087053  0.07804026 -0.28257868 ...  1.27384174 -0.21273346\n",
      "    1.33732426]\n",
      "  [ 0.76287735 -0.35694435 -0.00980787 ... -0.65297168  0.96893346\n",
      "   -0.04659436]]\n",
      "\n",
      " [[-0.20988055 -0.45818242 -0.22063486 ...  0.4783853   0.23813631\n",
      "   -0.15317954]\n",
      "  [ 1.19941092 -0.70550823 -0.1998679  ... -0.94940341  0.95267165\n",
      "   -0.47462469]\n",
      "  [-1.14286971  0.46591565  0.1773416  ...  0.16036843 -0.01780739\n",
      "    0.04552565]\n",
      "  ...\n",
      "  [-0.13446337  0.17131878  0.41705874 ... -0.19128594  0.38327387\n",
      "    0.6148715 ]\n",
      "  [-0.05252892 -0.40564221  0.52832907 ...  0.44475484  0.11124652\n",
      "    0.49437395]\n",
      "  [ 0.28198624 -0.68807006  0.25930786 ... -1.38512099  0.26067364\n",
      "    0.51605058]]\n",
      "\n",
      " [[-0.46895918 -0.81108117  0.26483297 ... -0.18192498 -0.20400558\n",
      "    1.3818661 ]\n",
      "  [ 0.25006914 -0.8331396   0.39133185 ... -0.33301136  0.40116063\n",
      "    0.82175225]\n",
      "  [ 0.71947116 -1.26189744 -0.59500843 ...  0.93302876  0.66325486\n",
      "   -0.44439313]\n",
      "  ...\n",
      "  [-0.31948414  0.04560049  0.4399828  ... -0.63235712 -0.63230735\n",
      "    0.73235762]\n",
      "  [-0.51383561 -0.91783518 -0.23706515 ... -0.90872365 -0.22458912\n",
      "    1.19119596]\n",
      "  [-0.11962177  0.00889336  0.71123713 ...  0.08722232 -0.00300359\n",
      "   -0.04586683]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step\n",
      "predictions 0-20\n",
      "[-0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045]\n",
      "Creating prediction dataframe\n",
      "bottom_score/pred_df_highest: -0.0053904485 / -0.0053904485\n",
      "No new rows to be added\n",
      "Predicting emb_df's embeddings\n",
      "emb_batches 0-5\n",
      "[[[ 1.31808305 -1.35149384 -0.21679243 ...  0.89271069  0.76951575\n",
      "   -1.19039869]\n",
      "  [ 0.28974602 -0.06294903  0.85951364 ... -0.71902603  1.33882415\n",
      "    0.38705596]\n",
      "  [ 1.31808305 -1.35149384 -0.21679243 ...  0.89271069  0.76951575\n",
      "   -1.19039869]\n",
      "  ...\n",
      "  [ 0.85323685 -0.57414776  0.35997775 ... -0.60646826  0.82066584\n",
      "    0.08563385]\n",
      "  [ 0.01932153 -0.7820065   0.20269561 ...  0.7233519   0.18285726\n",
      "    1.39887452]\n",
      "  [ 0.10104913 -0.51993024 -0.07946158 ...  0.15104468  0.33584976\n",
      "    0.27905038]]\n",
      "\n",
      " [[ 0.64689952 -0.72453994  0.13737181 ... -0.21017061  0.36940745\n",
      "    0.44660708]\n",
      "  [ 0.38380918 -0.66895741  0.10875688 ...  0.71498144 -0.36667073\n",
      "   -0.44161698]\n",
      "  [ 0.5176881   0.4098264   0.20523913 ...  0.39108637  0.2551488\n",
      "   -0.20448668]\n",
      "  ...\n",
      "  [ 0.19026902 -0.73238844  0.1009597  ...  0.19776818 -0.15037175\n",
      "    1.35234213]\n",
      "  [-0.65603817 -0.08962503 -0.36770436 ...  0.85037851 -0.76243418\n",
      "    0.11842967]\n",
      "  [ 0.6930775   0.27388442  0.64072824 ... -0.1222984  -0.57442868\n",
      "    0.6535852 ]]\n",
      "\n",
      " [[-0.39518258 -1.11754394 -0.30082491 ...  1.73542881 -0.24494247\n",
      "    0.68337733]\n",
      "  [ 0.01542969 -1.55224681 -0.65412951 ... -0.64408356  0.34380659\n",
      "    0.08169597]\n",
      "  [ 1.22304702 -0.48194504 -0.36405772 ...  0.90928602  0.65533304\n",
      "   -0.96596539]\n",
      "  ...\n",
      "  [ 0.37601119 -0.78223956  0.67711747 ... -1.16469431  0.13582018\n",
      "    0.56859004]\n",
      "  [ 0.47613877 -0.96707308  0.4373467  ... -0.18756194  0.72441518\n",
      "    1.43087006]\n",
      "  [-0.56667811 -0.1736404  -0.39459679 ... -0.65022665 -0.15354092\n",
      "   -0.31798983]]\n",
      "\n",
      " [[ 0.40206844 -0.25808936 -0.06733588 ... -0.56970066  0.22794096\n",
      "   -0.27787727]\n",
      "  [-0.38437927  0.33381906 -0.77817953 ... -0.96292126  0.28195253\n",
      "   -0.49121788]\n",
      "  [-0.12268506  0.77010959  0.07971422 ... -1.42639101 -0.2186445\n",
      "    0.32768911]\n",
      "  ...\n",
      "  [ 1.31808329 -1.3514936  -0.21679193 ...  0.89270926  0.76951665\n",
      "   -1.19039893]\n",
      "  [ 0.22982831 -0.37362584  0.1818682  ...  0.33268291  0.18538585\n",
      "   -0.14703788]\n",
      "  [ 0.97068191 -0.30285838  0.50149119 ... -0.04763453  0.16891804\n",
      "    0.57660675]]\n",
      "\n",
      " [[-0.39608598 -0.3493754  -0.28494519 ... -0.6133126   0.1771266\n",
      "    0.4079918 ]\n",
      "  [ 0.08404128  0.04229954 -0.18539146 ... -0.38302892  0.03142486\n",
      "   -0.149073  ]\n",
      "  [-0.06174296 -0.02467783  0.07674412 ... -2.29462838  0.13211995\n",
      "   -0.29543093]\n",
      "  ...\n",
      "  [-0.04687089 -1.21821964 -0.08986941 ... -0.58845484  0.06900687\n",
      "    0.12460177]\n",
      "  [ 1.10108638 -0.13333501 -0.12117437 ...  0.23591538  0.09752043\n",
      "    0.5247913 ]\n",
      "  [ 0.32209903 -0.95349109  0.34785873 ... -1.59521234  0.31867272\n",
      "    0.85961252]]]\n",
      "1/1 [==============================] - 0s 1ms/step\n",
      "predictions 0-20\n",
      "[-0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045]\n",
      "Creating prediction dataframe\n",
      "bottom_score/pred_df_highest: -0.0053904485 / -0.0053904485\n",
      "No new rows to be added\n",
      "Predicting emb_df's embeddings\n",
      "emb_batches 0-5\n",
      "[[[ 1.31808329 -1.35149348 -0.21679161 ...  0.89270961  0.76951599\n",
      "   -1.19039941]\n",
      "  [ 0.99391687 -0.12992591 -0.15554927 ...  0.69652426 -0.2883063\n",
      "    0.19299667]\n",
      "  [ 0.19139606 -0.87172735 -0.29519716 ... -1.50056815  0.29945481\n",
      "    0.96630114]\n",
      "  ...\n",
      "  [-0.40252224  0.34706101  0.63862896 ...  0.89169157 -0.28875107\n",
      "   -0.46282879]\n",
      "  [ 0.14382499 -0.14487407  0.14924301 ... -1.19576657 -0.65466261\n",
      "    0.01159699]\n",
      "  [-0.20075701 -1.12399971 -0.55664271 ... -0.62398249  0.82191938\n",
      "    1.40547609]]\n",
      "\n",
      " [[ 0.59357768 -1.241449   -0.44994721 ... -0.90076321  0.5477615\n",
      "    0.18445094]\n",
      "  [ 0.04401345 -1.30347383 -0.41674143 ... -0.49210149  0.03214364\n",
      "    0.45112866]\n",
      "  [ 0.24953954 -0.62603825  0.03604725 ... -0.0282643   0.11891413\n",
      "   -0.21419507]\n",
      "  ...\n",
      "  [ 0.91823077  0.34663871  0.79049307 ...  0.13295938 -0.30558667\n",
      "    0.47130215]\n",
      "  [ 0.53586793 -0.04916694 -0.22630914 ... -0.10277164  0.0995546\n",
      "    0.32214233]\n",
      "  [ 0.00788076 -0.82508534 -0.5559994  ... -0.66912758 -0.20955795\n",
      "    0.92009103]]\n",
      "\n",
      " [[ 0.76345783 -0.09915631 -0.03718329 ... -0.04759699  0.04988931\n",
      "    0.42736194]\n",
      "  [ 0.07807226  0.44417837  0.30157393 ... -0.87272978 -0.120696\n",
      "    0.28037876]\n",
      "  [ 0.68177861 -0.29272738  0.74121892 ...  1.02786016  0.05404229\n",
      "   -0.88990724]\n",
      "  ...\n",
      "  [ 0.3046343  -0.16775472 -0.17737393 ... -1.09730875  0.21321559\n",
      "    1.06630397]\n",
      "  [ 0.5732559   0.08781427  0.29740673 ... -0.23889789  0.51625937\n",
      "   -0.32617056]\n",
      "  [-0.18846479 -0.70592475  0.00989208 ...  0.06468283  0.66455102\n",
      "    0.4943884 ]]\n",
      "\n",
      " [[-0.94839185  0.44141623  0.05443477 ... -0.09313846 -0.32142022\n",
      "    0.37585953]\n",
      "  [ 0.33734071  0.34561691  0.2290034  ... -0.29875943 -0.07355417\n",
      "    0.17914657]\n",
      "  [-0.62423402  0.30173981  0.27867383 ... -1.14309978 -1.21996975\n",
      "    0.51214623]\n",
      "  ...\n",
      "  [ 0.83328515  0.1804284   0.68296784 ... -1.37788999 -0.20284899\n",
      "    1.05683172]\n",
      "  [ 0.1289576  -1.44737697 -0.44215479 ... -0.21699202 -0.21994893\n",
      "    0.38369799]\n",
      "  [ 0.95280224 -0.20722786  0.36976895 ... -0.84851837  0.33313704\n",
      "    0.85192013]]\n",
      "\n",
      " [[ 0.52140123 -0.46278313  0.68718851 ... -0.20867182 -0.86547834\n",
      "    0.93574405]\n",
      "  [ 1.17044616 -0.705998    0.27630475 ... -1.60878325 -0.11428799\n",
      "    0.11817271]\n",
      "  [ 0.55828249 -0.11251254  1.74975049 ... -0.5965516  -0.09686513\n",
      "    0.86427689]\n",
      "  ...\n",
      "  [-0.06201661 -0.64924181  0.33143094 ... -0.80442411  0.23050252\n",
      "   -0.7483061 ]\n",
      "  [-0.45580605  0.09602937  0.78328204 ... -1.70672631 -0.06505618\n",
      "    0.42853445]\n",
      "  [ 0.48246053 -0.2204954   0.1618378  ... -0.52560872 -0.18846795\n",
      "    0.3795172 ]]]\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "predictions 0-20\n",
      "[-0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045]\n",
      "Creating prediction dataframe\n",
      "bottom_score/pred_df_highest: -0.0053904485 / -0.0053904485\n",
      "No new rows to be added\n",
      "Predicting emb_df's embeddings\n",
      "emb_batches 0-5\n",
      "[[[ 1.31808305 -1.35149348 -0.21679182 ...  0.89270985  0.76951623\n",
      "   -1.19039953]\n",
      "  [ 0.28420544 -0.55543298 -0.23018    ... -0.8717286   0.06574668\n",
      "   -0.14937095]\n",
      "  [ 0.36088744 -0.48471692  0.68921489 ... -0.0492482   0.58796751\n",
      "    0.85517043]\n",
      "  ...\n",
      "  [ 0.55426466 -0.54954338  0.31771317 ... -0.44657984 -0.15339261\n",
      "    0.1464449 ]\n",
      "  [ 0.1808998  -0.1968732   0.12437605 ... -0.9677996   0.34198949\n",
      "    0.35786507]\n",
      "  [ 1.27824104 -0.57804745  0.34114793 ...  0.62819785  0.1554801\n",
      "   -0.50048679]]\n",
      "\n",
      " [[ 0.53927052 -0.71331793 -0.01348676 ...  0.11404542  0.29021496\n",
      "   -0.28181994]\n",
      "  [-0.68594688 -0.55736345 -0.35489276 ... -0.72912592 -0.22776532\n",
      "    0.100887  ]\n",
      "  [-0.1868864  -0.57727987  0.1417395  ... -0.08405228 -0.10096917\n",
      "   -0.50662816]\n",
      "  ...\n",
      "  [-0.54650187 -0.69144714  1.17221117 ... -1.05844903  0.11220492\n",
      "    0.39444205]\n",
      "  [-0.00520939 -0.52719927 -0.5118553  ...  0.39271441  0.31936839\n",
      "   -0.60892993]\n",
      "  [ 0.48011583 -1.38671494 -0.25214529 ...  0.25459057  0.03341193\n",
      "   -0.14632182]]\n",
      "\n",
      " [[ 0.21754374 -0.54150921 -0.54131746 ... -0.80264491  0.47959045\n",
      "   -0.22369023]\n",
      "  [ 0.48011583 -1.38671494 -0.25214529 ...  0.25459057  0.03341193\n",
      "   -0.14632182]\n",
      "  [ 0.27616349 -0.98067635 -0.03022822 ...  0.31864026  0.2233298\n",
      "   -0.19382524]\n",
      "  ...\n",
      "  [-0.18688709 -0.57728034  0.14173934 ... -0.08405207 -0.10096893\n",
      "   -0.50662851]\n",
      "  [-0.06618523 -0.12550013  0.25075004 ...  0.24008672 -0.12667997\n",
      "   -0.41110981]\n",
      "  [ 0.38336605 -1.10943854  0.50462812 ...  0.20184873  0.07080089\n",
      "    1.10793138]]\n",
      "\n",
      " [[-0.9428528   0.3825306  -0.30032948 ... -0.44264612  0.17271325\n",
      "    1.17494237]\n",
      "  [ 0.55599833 -0.37818432 -0.33366549 ... -0.45180282 -0.1826659\n",
      "    0.8641752 ]\n",
      "  [ 0.05089449  0.18500562  0.1874537  ...  0.69907576 -0.20634861\n",
      "    0.37527999]\n",
      "  ...\n",
      "  [ 0.59357768 -1.241449   -0.44994721 ... -0.90076321  0.5477615\n",
      "    0.18445094]\n",
      "  [ 0.04401345 -1.30347383 -0.41674143 ... -0.49210149  0.03214364\n",
      "    0.45112866]\n",
      "  [ 0.24953954 -0.62603825  0.03604725 ... -0.0282643   0.11891413\n",
      "   -0.21419507]]\n",
      "\n",
      " [[ 0.3378112  -0.55029112 -0.211199   ... -1.9302932  -0.22899878\n",
      "    0.77652103]\n",
      "  [ 0.84388971 -0.40060309  0.01174372 ... -0.32005614 -0.30824038\n",
      "    0.04349968]\n",
      "  [ 0.36417243  0.05715813  0.28844634 ... -0.39011353  0.59930015\n",
      "    0.36066023]\n",
      "  ...\n",
      "  [-1.06962502  0.78682476  0.72436786 ... -0.53613001 -0.58270341\n",
      "    0.14255838]\n",
      "  [-0.67944336  0.07855192  0.42040804 ... -0.78313297 -0.49249905\n",
      "    0.41619471]\n",
      "  [-0.39671212 -0.18269059 -0.41171789 ...  0.90836906  0.44902676\n",
      "   -0.24145763]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step\n",
      "predictions 0-20\n",
      "[-0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045]\n",
      "Creating prediction dataframe\n",
      "bottom_score/pred_df_highest: -0.0053904485 / -0.0053904485\n",
      "No new rows to be added\n",
      "Predicting emb_df's embeddings\n",
      "emb_batches 0-5\n",
      "[[[ 1.31808281 -1.3514936  -0.21679163 ...  0.89270949  0.76951623\n",
      "   -1.19040012]\n",
      "  [ 0.33735931 -0.1040054   1.00903106 ... -1.33279955  0.87681776\n",
      "    0.31744558]\n",
      "  [ 0.01791837 -0.97980791  0.46298274 ... -1.93654048  0.3613238\n",
      "    0.36071238]\n",
      "  ...\n",
      "  [ 0.11713862  0.42606425 -0.15007584 ... -0.41017073  0.2193383\n",
      "    0.51430464]\n",
      "  [-0.10173391 -0.70218891 -0.83921206 ...  1.12638533 -0.23936138\n",
      "   -0.39608955]\n",
      "  [-0.14516385  0.58596158  0.58762747 ... -1.25435221  0.40228564\n",
      "    0.42066166]]\n",
      "\n",
      " [[ 0.23356158 -0.15426749  0.40086606 ... -0.67714918  0.78808224\n",
      "    0.16744353]\n",
      "  [ 0.00444681  0.14944266 -0.2683509  ...  0.3181223   0.45420167\n",
      "   -0.24558577]\n",
      "  [ 0.65879792 -0.59899187 -0.10502103 ...  0.73461318 -0.10690756\n",
      "   -0.48007953]\n",
      "  ...\n",
      "  [ 0.5992561  -1.13726938  0.57434452 ... -0.89185619 -0.21876633\n",
      "   -0.16157556]\n",
      "  [ 0.32668194 -1.17876136 -0.0023062  ... -1.05100274  0.24919936\n",
      "    0.7801882 ]\n",
      "  [ 0.37885395 -0.5047586  -0.25516713 ...  0.71193844 -0.18489811\n",
      "    0.07655898]]\n",
      "\n",
      " [[ 0.40219629 -0.40118083  1.17797506 ... -0.45830467  0.11473206\n",
      "   -0.37986788]\n",
      "  [ 0.57143235 -0.04252774  0.48274794 ... -0.24402282 -0.09674586\n",
      "    0.71201503]\n",
      "  [ 0.66398919  0.20572887 -1.43780887 ... -0.15141886 -0.19020487\n",
      "    0.44754916]\n",
      "  ...\n",
      "  [-0.27056015  0.02912338  0.97249627 ...  0.09854445 -0.05543882\n",
      "   -0.64701706]\n",
      "  [-0.18297735 -0.26472804  0.58379215 ...  1.13978815  0.26678652\n",
      "    0.41932821]\n",
      "  [-0.03800634  0.27114022  0.37854928 ...  0.54282355  0.05095331\n",
      "   -0.4714461 ]]\n",
      "\n",
      " [[-0.69361871  0.47491413  0.48743385 ... -0.23909773  0.82134992\n",
      "   -0.53033566]\n",
      "  [ 0.97063297 -0.92182773  0.14427795 ...  1.9064821   0.36595622\n",
      "    0.68177843]\n",
      "  [-0.59386814 -0.66887385  0.3356213  ...  0.08797702 -0.27289549\n",
      "    0.04259611]\n",
      "  ...\n",
      "  [ 0.5464837  -0.29269776 -0.31674343 ... -0.29023319 -0.61354369\n",
      "    1.0753932 ]\n",
      "  [ 0.01792429 -0.86150604 -0.43439397 ...  0.83796936 -0.05401474\n",
      "    0.53235233]\n",
      "  [-0.25648314 -0.16552624 -0.26222962 ...  1.35606873  0.41039115\n",
      "   -0.33543825]]\n",
      "\n",
      " [[-0.38734555 -0.67451459 -0.47089419 ... -0.18563515  0.27062866\n",
      "   -0.49975112]\n",
      "  [ 0.77775067 -0.80555272 -0.59699816 ...  0.22375385  0.24090694\n",
      "   -0.32062927]\n",
      "  [-0.20365916 -0.84334338 -0.23313153 ... -1.28081715  0.95481551\n",
      "    0.12630847]\n",
      "  ...\n",
      "  [ 0.37540871 -1.03967547 -0.39729795 ... -0.69450551  0.715473\n",
      "   -0.73699719]\n",
      "  [-0.2218024  -0.20450254  0.6021831  ... -0.01097273 -0.41217744\n",
      "    0.75795168]\n",
      "  [-0.10201009 -0.56228507  0.83541965 ... -1.57260334 -0.25533253\n",
      "    1.38315868]]]\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "predictions 0-20\n",
      "[-0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045 -0.00539045\n",
      " -0.00539045 -0.00539045]\n",
      "Creating prediction dataframe\n",
      "bottom_score/pred_df_highest: -0.0053904485 / -0.0053904485\n",
      "No new rows to be added\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-a6b280ad18ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_top_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_top_sentence_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-a9011bea47d2>\u001b[0m in \u001b[0;36mget_top_sentence_scores\u001b[0;34m(self, emb_paths, model, k, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0memb_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# load emb_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0memb_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#             print(\"emb_df\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#             print(display(emb_df))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/proj-repo/AAARG-dissertation/notebooks/corpus_loader.ipynb\u001b[0m in \u001b[0;36mload_embeddings\u001b[0;34m(save_path, emb_model, corpus_df, nugget_df, sents_default, only_docs_with_nugs, nlp, force_reload, save, verbose, path_handler)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;34m\"        print(\\\"There were \\\" + str(len(missed_nuggetids)) + \\\" nugget_ids found in matches.tsv but not in nuggets.tsv\\\")\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;34m\"        print(str(len(missed_sentid_streamids)) + \\\" out of \\\" + str(len(entry_list)) + \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m     \u001b[0;34m\"              \\\" streamids had out of bounds sent_ids\\\")\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m     \u001b[0;34m\"        \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;34m\"    nugget_df = pd.DataFrame(entry_list)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/proj-repo/AAARG-dissertation/notebooks/corpus_loader.ipynb\u001b[0m in \u001b[0;36mload_df_control\u001b[0;34m(save_path, load_func, save, force_reload, name, verbose, path_handler)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;34m\"def find_duplicates(df):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;34m\"    seen = set()\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m     \u001b[0;34m\"    seen_twice = set()\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m     \u001b[0;34m\"    for docid in df['docid']:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;34m\"        if docid not in seen:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/proj-repo/AAARG-dissertation/notebooks/corpus_loader.ipynb\u001b[0m in \u001b[0;36mread_df_file_type\u001b[0;34m(save_path, verbose, concat_multiple)\u001b[0m\n\u001b[1;32m    542\u001b[0m    \"source\": [\n\u001b[1;32m    543\u001b[0m     \u001b[0;34m\"def topic_id_as_int(topic_id):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0;34m\"    try:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m     \u001b[0;34m\"        topic_id = int(topic_id)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;34m\"        return topic_id\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mauto_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_close\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def select_as_coordinates(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, coordinates)\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# directly return the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1922\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(_start, _stop, _where)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;31m# function to call on iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_where\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_where\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;31m# create the iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, where, columns, start, stop)\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m             \u001b[0mblk_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"block{i}_items\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"block{i}_values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(self, key, start, stop)\u001b[0m\n\u001b[1;32m   2816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVLArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2818\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2819\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_decoded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tables/vlarray.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             start, stop, step = self._process_range(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tables/vlarray.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mlistarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0mlistarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0matom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 10000\n",
    "batch_size = 256\n",
    "test_top_scores = sum_gen.get_top_sentence_scores(test_paths, model, k=k, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_score_df_path(tuner_dir, topic_id, X_col, corpus_name):\n",
    "    fn = str(topic_id) + \"_\" + str(X_col) + \"_\" + str(corpus_name)\n",
    "    fn += \".hdf\"\n",
    "    path = os.path.join(tuner_dir, fn)\n",
    "    return path\n",
    "\n",
    "def gen_ranked_df_path(tuner_dir, rank_method, topic_id, X_col, corpus_name):\n",
    "    fn = str(rank_method) + \"_\" + str(topic_id) + \"_\" + str(X_col) + \"_\" + str(corpus_name)\n",
    "    fn += \".hdf\"\n",
    "    path = os.path.join(tuner_dir, fn)\n",
    "    return path\n",
    "\n",
    "def gen_varname_path(tuner_dir, varname, rank_method, topic_id, X_col, corpus_name):\n",
    "    fn = \"ranked_idxs\" + \"_\" + str(rank_method) + \"_\" + str(topic_id) + \"_\" + str(X_col) + str(corpus_name)\n",
    "    fn += \".pickle\"\n",
    "    path = os.path.join(tuner_dir, fn)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryGenerator:\n",
    "    def __init__(self, model_path_handler, proj_repo='/nfs/proj-repo/AAARG-dissertation'):\n",
    "        self.model_path_handler = self.model_path_handler\n",
    "        self.path_ret = PathRetriever(proj_repo)\n",
    "        self.proj_repo = proj_repo\n",
    "        self.rank_methods = ['k', 'k_non_redund']\n",
    "        \n",
    "    def generate_ranked_df(self, rank_method, model, tuner_dir, corpus_name, nested_dir, topic_id, k=10000,\n",
    "                         X_col='embedding', batch_size=256, redundancy_score=None, verbose=verbose,\n",
    "                          force_reload=False):\n",
    "        \n",
    "        # create/load predicted scores\n",
    "        score_df_path = gen_score_df_path(tuner_dir, topic_id, X_col, corpus_name)\n",
    "        score_df = None\n",
    "        if os.path.exists(score_df_path):  # load\n",
    "            score_df = read_df_file_type(score_df_path, verbose=verbose)\n",
    "        else:  \n",
    "            # create df of predicted scores\n",
    "            emb_paths = self.path_ret.get_embedding_paths(corpus_name, nested_dir, topic_ids=topic_id, \n",
    "                                                      return_dir_path=False)\n",
    "            emb_paths = list(emb_paths['path'])\n",
    "            score_df = self.get_prediction_scores(model, emb_paths, X_col=X_col, batch_size=batch_size,\n",
    "                                                 sort_scores=True)\n",
    "            # save df\n",
    "            score_df = save_df_file_type(score_df_path, verbose=verbose)\n",
    "        \n",
    "        # get ranked df\n",
    "        ranked_df = None\n",
    "        ranked_df_path = gen_ranked_df_path(tuner_dir, rank_method, topic_id, X_col, corpus_name)\n",
    "        if not os.path.exists(ranked_df_path) or force_reload:\n",
    "            # load new ranked_df with selected method\n",
    "            ranked_idxs = None\n",
    "            if rank_method == \"k\":\n",
    "                ranked_df = self.retrieve_top_k_sentences(score_df, k, is_sorted=True)\n",
    "            elif rank_method == \"k_non_redund\":\n",
    "                ranked_df, ranked_idxs = self.ret_top_k_non_redundant(score_df, \n",
    "                                                                    k, \n",
    "                                                                    redundancy_threshold, \n",
    "                                                                    X_col='embedding', \n",
    "                                                                    is_sorted=True,\n",
    "                                                                    return_indexes=True)\n",
    "            else:\n",
    "                raise Exception(str(rank_method) + \" is not a valid rank_method: \" + str(self.rank_methods))\n",
    "            # save ranked_idxs\n",
    "            if ranked_idxs is not None:\n",
    "                ranked_idxs_path = gen_varname_path(tuner_dir, \"ranked_idxs\", rank_method, topic_id, X_col,\n",
    "                                                   corpus_name)\n",
    "                with open(ranked_idxs_path, 'wb') as handle:\n",
    "                    pickle.dump(ranked_idxs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        else:\n",
    "            ranked_df = read_df_file_type(ranked_df_path, verbose=verbose)\n",
    "            \n",
    "        return ranked_df\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_prediction_scores(self, model, emb_paths, X_col='embedding',\n",
    "                             batch_size=256, sort_scores=True, add_X_col_to_df=True):\n",
    "        \"\"\"\n",
    "        Return dataframe with columns:\n",
    "            sentence: string of sentence\n",
    "            score: predicted score from model\n",
    "            relative_pos: relative_position of sentence in its article (sent_id)\n",
    "            embedding (optional): embedding of sentence (determined by X_col parameter)\n",
    "        \"\"\"\n",
    "        \n",
    "        # predict scores for target embeddings to place into df\n",
    "        score_df_dict = defaultdict(list)\n",
    "        print(\"Predicting scores...\")\n",
    "        for emb_path in tqdm_notebook(emb_paths):\n",
    "            emb_df = load_embeddings(emb_path, verbose=False)\n",
    "            embs = list(embs[X_col])\n",
    "            \n",
    "            scores = predict_emb_list(model, emb_list, batch_size=batch_size)\n",
    "            \n",
    "            score_df_dict['sentence'].extend(list(emb_df['sentence']))\n",
    "            score_df_dict['score'].extend(scores.tolist())\n",
    "            score_df_dict['relative_pos'].extend(list(emb_df['sent_id']))  # relative position in article\n",
    "            if add_X_col_to_df:\n",
    "                score_df_dict['embedding'].extend(embs)\n",
    "        \n",
    "        # tie sentence <-> score together and access DataFrame functionality\n",
    "        score_df = pd.DataFrame(score_df_dict)\n",
    "        if sort_scores:\n",
    "            score_df = score_df.sort_values(by=\"score\", ascending=False)  # descending order\n",
    "        return score_df\n",
    "        \n",
    "    def retrieve_top_k_sentences(self, score_df, k, is_sorted=True):\n",
    "        \"\"\"\n",
    "        Retrieve top k scoring sentences from score_df\n",
    "        \"\"\"\n",
    "        if not is_sorted:\n",
    "            score_df = score_df.sort_values(by=\"score\", ascending=False)\n",
    "        \n",
    "        top_k_df = score_df[0:k]\n",
    "        \n",
    "        return top_k_df\n",
    "    \n",
    "    def ret_top_k_non_redundant(self, score_df, k, redundancy_threshold, X_col='embedding', is_sorted=True,\n",
    "                               return_indexes=True):\n",
    "        \"\"\"\n",
    "        Retrieve top k sentences from df, not repeating redundant sentences\n",
    "        \"\"\"\n",
    "        if not is_sorted:\n",
    "            score_df = score_df.sort_values(by=\"score\", ascending=False)\n",
    "        \n",
    "        top_k_rows = []\n",
    "        top_k_row_idxs = None\n",
    "        if return_indexes:\n",
    "            top_k_row_idxs = []\n",
    "        \n",
    "        # iteratively find top k sentences where cosine similarity is under redundancy_threshold\n",
    "        for index, row in score_df.iterrows():\n",
    "            under_threshold = True\n",
    "            row_emb = row['embedding']\n",
    "            for k_row in top_k_rows:  # compare cosine similarity with each sentence so far\n",
    "                k_emb = k_row['embedding']\n",
    "                cos_sim = cosine_similarity(row_emb, k_emb)\n",
    "                if cos_sim >= redundancy_threshold:\n",
    "                    under_threshold = False\n",
    "                    break  # break from nested loop\n",
    "            if under_threshold:  # add when dissimilar enough\n",
    "                top_k_rows.append(row)\n",
    "                if return_indexes:\n",
    "                    top_k_row_idxs.append(index)\n",
    "            if len(top_k_rows) == k:\n",
    "                break\n",
    "        \n",
    "        top_k_df = pd.DataFrame(top_k_rows)\n",
    "#         top_k_df = self.order_by_article_position(top_k_df)\n",
    "        return top_k_df\n",
    "    \n",
    "    def order_by_article_position(self, top_k_df):\n",
    "        \"\"\"Order sentences by relative article position\"\"\"\n",
    "        top_k_df = top_k_df.sort_values(by=\"relative_pos\", ascending=True)\n",
    "        return top_k_df\n",
    "    \n",
    "    def "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

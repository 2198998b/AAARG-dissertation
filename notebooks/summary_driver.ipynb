{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Generating Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp while cluster is full\n",
    "# !pip install keras-tuner\n",
    "# !pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_threads = 32\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = str(num_threads)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ipynb.fs\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import kerastuner as kt\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from kerastuner.tuners import Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_col_labels = ['cosine_similarity', 'cos_sim_nearest_nug']\n",
    "default_input_col = \"embedding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTuner:\n",
    "    def __init__(self, save_dir, save_name, input_shape, tuning_iterations=50, batch_size=32, force_reload=False):\n",
    "        \"\"\"Can save using project_name param, if overwrite false then will reload where it started\n",
    "        In Tuner Class documentation\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.models = []\n",
    "        self.tuner = Hyperband(self.build_model, \n",
    "                          objective='mean_squared_error', \n",
    "                          max_epochs=25,\n",
    "                          hyperband_iterations=tuning_iterations,\n",
    "                          directory=save_dir,\n",
    "                          project_name=save_name,\n",
    "                          overwrite=force_reload)\n",
    "        \n",
    "    def build_model(self, hp):\n",
    "        model = Sequential()\n",
    "        ilayer = InputLayer(input_shape=self.input_shape, batch_size=self.batch_size)\n",
    "        model.add(ilayer)\n",
    "        for i in range(hp.Int('num_layers', min_value=1, max_value=4)):\n",
    "            model.add(Dense(units=hp.Int('units_' + str(i),\n",
    "                                        min_value=1, max_value=1024, step=32),\n",
    "                            activation=hp.Choice('activ_' + str(i),\n",
    "                                                ['relu', 'tanh', 'sigmoid'])))\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=0.00001, max_value=0.1))           \n",
    "        losses = hp.Choice('loss_func', ['MSE', 'huber', 'binary_crossentropy', 'categorical_crossentropy'])\n",
    "        model.compile(optimizer=opt, loss=losses, metrics=['mean_squared_error'])  # add metrics here\n",
    "        self.models.append(model)\n",
    "        return model\n",
    "    \n",
    "    def search(self, batch_generator, save_path=None, return_hyperparams=False):\n",
    "        \"\"\"Find optimal model given dataset\n",
    "        \"\"\"\n",
    "        self.tuner.search(x=batch_generator, verbose=1, use_multiprocessing=False, workers=num_threads)\n",
    "        best_model = self.tuner.get_best_models(num_models=1)\n",
    "        if save_path is not None:\n",
    "            tf.keras.save(save_path)\n",
    "        if return_hyperparams:\n",
    "            hyperparams = self.tuner.get_best_hyperparameters(num_trials=1)\n",
    "            return best_model, hyperparams\n",
    "        return best_model\n",
    "    \n",
    "\n",
    "# from collections import OrderedDict\n",
    "from collections import deque\n",
    "\n",
    "class BatchGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Class to load in dataset that is too large to load into memory at once\n",
    "    \n",
    "    Do check in class before to make sure all X lists and y lists are same length\n",
    "    \n",
    "    https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self, X_paths, y_paths, batch_size, file_type):\n",
    "        if batch_size is None:\n",
    "            self.batch_size = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        \n",
    "        self.X_paths = X_paths\n",
    "        self.y_paths = y_paths\n",
    "        self.file_type = file_type\n",
    "#         self.shuffle = False  # make sure linear progression through dataset for sake of memory efficiency\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.X_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one batch of data\"\"\"\n",
    "        inputs = self.load_samples(X_paths, idx)\n",
    "        labels = self.load_samples(y_paths, idx)\n",
    "        return inputs, labels\n",
    "    \n",
    "    \n",
    "    def load_samples(self, path, index):\n",
    "        samples = None\n",
    "        if self.file_type == '.pickle':\n",
    "            target_path = path[index]\n",
    "            with open(target_path, 'rb') as handle:\n",
    "                samples = pickle.load(handle)\n",
    "        elif self.file_type == '.memmap':\n",
    "            path[index] = samples\n",
    "        else:\n",
    "            raise Exception(\"Invalid file type: \" + str(self.file_type))\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .defs.corpus_loader import PathRetriever, load_embeddings, load_topics, read_df_file_type, save_df_file_type\n",
    "from .defs.corpus_loader import convert_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_input_params(path_ret, corpus_names, nested_dirs, col_labels, input_col=None):\n",
    "    \"\"\"Helper function to resolve the selection of input params that determine what data to load/generate\"\"\"\n",
    "    # resolve corpus_names\n",
    "    if corpus_names is None:\n",
    "        corpus_names = path_ret.get_corpus_names()\n",
    "        if len(corpus_names) == 0:\n",
    "            raise Exception(\"There are no corpuses to load from\")\n",
    "    # resolve col_labels\n",
    "    if col_labels is None:  # our columns to generate files for\n",
    "        col_labels = default_col_labels.copy()\n",
    "        if input_col is not None:\n",
    "            col_labels.append(input_col)\n",
    "    # resolve nested_dirs\n",
    "    if type(nested_dirs) != dict:  # if output gets passed through again\n",
    "        nested_dict = {}\n",
    "        for corpus_name in corpus_names:  # get the nested dir for each corpus name\n",
    "            nested_dict[corpus_name] = path_ret.get_nested_dirs(corpus_name, \"embeddings\")\n",
    "            if nested_dirs is not None:\n",
    "                # add only selected nested_dirs for this corpus_name\n",
    "                nested_dict[corpus_name] = [x for x in nested_dict[corpus_name] if x in nested_dirs]\n",
    "        nested_dirs = nested_dict\n",
    "    # make sure there is at least one entry in nested_dict\n",
    "    empty_dirs = [len(x) == 0 for x in nested_dirs.values()]  # get if empty for each item\n",
    "    if all(empty_dirs):\n",
    "        raise Exception(\"There are no nested_dirs matching the selection\")\n",
    "    return corpus_names, nested_dirs, col_labels\n",
    "\n",
    "def corpus_name_topic_ids(path_retriever, corpus_name):\n",
    "    topic_path = path_retriever.get_topic_path(corpus_name, verbose=False)\n",
    "    topic_df = load_topics(topic_path, verbose=False)\n",
    "    topic_ids = list(topic_df['id'].unique())\n",
    "    return topic_ids\n",
    "\n",
    "def find_combinations(path_df, corpus_names, nested_dirs, col_labels, add_topics=False, col_labels_as_list=False,\n",
    "                      as_tuples=True, force_reload=False, path_retriever=None, batch_size=None, file_type=None,\n",
    "                     exists_only=False):\n",
    "    \"\"\"Find the combinations that have not been generated/trained already in path_df\n",
    "    \n",
    "    Tuple ordering: (corpus_name, nested_dir, col_label/[col_labels], **topic_id**)\n",
    "    \"\"\"\n",
    "    if exists_only:\n",
    "        path_df = path_df[path_df['exists'] == True]  # checking of path_df is only concerned with existing files\n",
    "    if batch_size is not None:\n",
    "        path_df = path_df[path_df['batch_size'] == batch_size]\n",
    "    if file_type is not None:\n",
    "        path_df = path_df[path_df['file_type'] == file_type]\n",
    "    topic_ids = {}\n",
    "    if add_topics:  # find topic_ids for each corpus\n",
    "        for corpus_name in corpus_names:\n",
    "            if path_retriever is not None:\n",
    "                topic_ids[corpus_name] = corpus_name_topic_ids(path_retriever, corpus_name)\n",
    "            else:\n",
    "                raise Exception(\"If add_topics is True then path_retriever must be set to an instance of PathRetriever\")\n",
    "    # get possible combinations\n",
    "    combinations = []\n",
    "    for corpus_name in corpus_names:\n",
    "        for nested_dir in nested_dirs[corpus_name]:\n",
    "            combo_path = path_df[(path_df['corpus_name'] == corpus_name)\n",
    "                                    & (path_df['nested_dir'] == nested_dir)]\n",
    "            combo = [corpus_name, nested_dir]\n",
    "            if add_topics:  # create permutations with topic_ids\n",
    "                topic_combo_dict = defaultdict(list)\n",
    "                for label in col_labels:\n",
    "                    for topic_id in topic_ids[corpus_name]:  # check if label exists for topic_id\n",
    "                        topic_path = combo_path[(combo_path['col_label'] == label)\n",
    "                                               & (combo_path['topic_id'] == topic_id)]\n",
    "                        if len(topic_path) == 0 or force_reload:\n",
    "                            topic_combo_dict[topic_id].append(label)\n",
    "                topic_combos = []\n",
    "                for topic_id, labels in topic_combo_dict.items():\n",
    "                    topic_combos = []\n",
    "                    if col_labels_as_list:  # add single tuple with all missing col_labels for topic_id\n",
    "                        topic_combo = copy.deepcopy(combo)\n",
    "                        topic_combo.append(labels)\n",
    "                        topic_combo.append(topic_id)\n",
    "                        topic_combos.append(topic_combo)\n",
    "                    else:\n",
    "                        for label in labels:  # add a tuple for each missing col_label for topic_id\n",
    "                            topic_combo = copy.deepcopy(combo)\n",
    "                            topic_combo.append(topic_id)\n",
    "                            topic_combos.append(topic_combo)\n",
    "                    combinations.extend(topic_combos)\n",
    "            else:  # create permutations with col_labels only\n",
    "                label_combos = []\n",
    "                add_labels = None\n",
    "                if not force_reload:  # find which col_labels don't exist already\n",
    "                    exist_labels = list(combo_path['col_label'].unique())\n",
    "                    add_labels = [x for x in col_labels if x not in exist_labels]\n",
    "                else:\n",
    "                    add_labels = copy.deepcopy(col_labels)  # force_reload add all labels\n",
    "                if col_labels_as_list:  # add single tuple\n",
    "                    label_combo = copy.deepcopy(combo)\n",
    "                    label_combo.append(add_labels)\n",
    "                    label_combos.append(label_combo)\n",
    "                else:\n",
    "                    for add_label in add_labels:  # add tuple for each col_label\n",
    "                        label_combo = copy.deepcopy(combo)\n",
    "                        label_combo.append(add_label)\n",
    "                        label_combos.append(label_combo)\n",
    "                combinations.extend(label_combos)\n",
    "                \n",
    "    if as_tuples:\n",
    "        combinations = [tuple(x) for x in combinations]\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                             row = {'corpus_name':corpus_name, 'nested_dir':nested_dir, 'col_label':col_label, \n",
    "#                                    'path':path, 'dtype':dtype, 'vector_len':vector_len, 'total_nums':total_nums, \n",
    "#                                    'offset_step':offset_step, 'topic_ids':self.topic_ids_str(topic_ids), \n",
    "#                                    'history_path':history_path, 'complete':False}\n",
    "#                             self.path_df = self.path_df.append(row, ignore_index=True)\n",
    "\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "class MemmapGenerator:\n",
    "    def __init__(self, proj_dir):\n",
    "        if proj_dir is None:\n",
    "            self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "        else:\n",
    "            self.proj_dir = proj_dir\n",
    "        self.default_file_type = \".hdf\"\n",
    "        self.path_ret = PathRetriever(proj_dir)\n",
    "        self.path_df_cols = ['corpus_name', 'nested_dir', 'col_label', 'path', 'dtype', 'vector_len', \n",
    "                             'total_nums', 'offset_step', 'topic_ids', 'complete']\n",
    "        self.dataset_dir = self.path_ret.path_handler.dataset_dir\n",
    "        self.path_df_path = os.path.join(self.dataset_dir, \"memmap_paths.hdf\")\n",
    "        self.path_df = self.load_path_df()\n",
    "        self.order = 'C'\n",
    "        \n",
    "        \n",
    "    def create_maps(self, corpus_name, nested_dir, col_labels, topic_ids, verbose=True, force_reload=False):\n",
    "        # check if already completed\n",
    "        path_slice = self.slice_path_df(corpus_name, nested_dir, topic_ids)\n",
    "        emb_paths, nested_dir_path = self.path_ret.get_embedding_paths(corpus_name, nested_dir, \n",
    "                                                                file_type=self.default_file_type, verbose=False, \n",
    "                                                                return_dir_path=True, topic_ids=topic_ids)\n",
    "        emb_paths = list(emb_paths['path'])\n",
    "        # load partial information on maps that need completed\n",
    "        meta_dict = self.create_meta_dict(path_slice, corpus_name, nested_dir, col_labels, \n",
    "                                          self.topic_ids_str(topic_ids), nested_dir_path, force_reload=force_reload)\n",
    "        \n",
    "        if len(meta_dict) > 0:\n",
    "            if verbose:\n",
    "                print(\"Creating memmaps for \" + str(\", \".join(col_labels)))\n",
    "            # debug vars\n",
    "            count = 0\n",
    "            count_lim = 3\n",
    "            nums_to_add = defaultdict(list)\n",
    "            total_nums_d = defaultdict(list)\n",
    "            \n",
    "#             mmap_dict = {}\n",
    "            for emb_path in tqdm_notebook(emb_paths):\n",
    "                print(\"loop count: \" + str(count))\n",
    "                if count >= count_lim:\n",
    "                    break\n",
    "                # get the cols that haven't been loaded for this path\n",
    "                # scrape data from dataframe\n",
    "                label_data = self.scrape_col_data(emb_path, meta_dict.keys())\n",
    "                # add data to memmap\n",
    "                for col_label, data in label_data.items():\n",
    "                    print(\"processing \" + str(col_label))\n",
    "                    \n",
    "                    col_dict = meta_dict[col_label]\n",
    "                    if not col_dict['initialised']:\n",
    "                        col_dict['dtype'] = data.dtype\n",
    "                        ndim = data.ndim\n",
    "                        if ndim == 1:  # 1d\n",
    "                            col_dict['vector_len'] = 1\n",
    "                        elif ndim == 2:  # 2d\n",
    "                            col_dict['vector_len'] = data.shape[1]\n",
    "                        else:\n",
    "                            raise Exception(\"Too many dimensions: \" + str(data.shape))\n",
    "                        col_dict['offset_step'] = data.dtype.itemsize\n",
    "                        col_dict['initialised'] = True\n",
    "                        \n",
    "#                         mmap_dict[col_label] = np.memmap(col_dict['path'], dtype=col_dict['dtype'], mode='w+',\n",
    "#                                                         order=self.order)\n",
    "                        \n",
    "                    # load meta_dict vars, save hashing time\n",
    "                    total_nums = col_dict['total_nums']\n",
    "                    offset_step = col_dict['offset_step']\n",
    "                    path = col_dict['path']\n",
    "                    dtype = col_dict['dtype']\n",
    "\n",
    "                    # add data to map\n",
    "                    flat = data.ravel()\n",
    "                    offset = 0\n",
    "                    num_to_add = len(flat)\n",
    "                    nums_to_add[col_label].append(num_to_add)  # debug\n",
    "                    \n",
    "                    memmap = None\n",
    "                    if total_nums != 0:\n",
    "#                         offset = offset_step * (total_nums + 1)  # +1 to get space after last add\n",
    "                        offset = offset_step * (total_nums + 1)  # +1 to get space after last add\n",
    "                        memmap = np.memmap(path, dtype=dtype, mode='r+', offset=0, \n",
    "                                       order=self.order, shape=(total_nums + num_to_add,))\n",
    "                    else:\n",
    "                        print(\"total_nums is zero : \" + str(total_nums))\n",
    "                        memmap = np.memmap(path, dtype=dtype, mode='w+', offset=0, \n",
    "                                       order=self.order, shape=(num_to_add,))\n",
    "                    \n",
    "                    print(\"main loop memmap shape: \" + str(memmap.shape))\n",
    "                    print(\"main loop flat shape: \" + str(flat.shape))\n",
    "                    memmap[total_nums:total_nums+num_to_add] = flat[:]\n",
    "                    print(\"memmap after flat assignment\")\n",
    "                    print(str(memmap))\n",
    "                    if not np.array_equal(memmap[total_nums:total_nums+num_to_add], flat):\n",
    "                        print(\"memmap and flat not equal\")\n",
    "                        print(\"memmap: \" + str(memmap[total_nums:total_nums+num_to_add]))\n",
    "                        print(\"flat: \" + str(flat))\n",
    "                    \n",
    "                    memmap.flush()\n",
    "#                     del memmap\n",
    "                    \n",
    "#                     if total_nums == 0:\n",
    "#                         mmap = np.memmap(path, dtype=dtype, mode='r', shape=(num_to_add,), order=self.order,\n",
    "#                                         offset=0)\n",
    "#                         print(\"load check mmap main loop\")\n",
    "#                         for i in range(10):\n",
    "#                             print(str(mmap[i]))\n",
    "\n",
    "                    # update fields\n",
    "                    col_dict['total_nums'] += num_to_add\n",
    "                    total_nums_d[col_label].append(col_dict['total_nums'] / col_dict['vector_len'])\n",
    "                count += 1\n",
    "                    \n",
    "#                     pprint.pprint(col_dict)\n",
    "#                     print(\"\")\n",
    "                    \n",
    "            for col_label, meta in meta_dict.items():\n",
    "                self.update_path_df_entry(meta['path'], col_label, meta['dtype'], meta['vector_len'],\n",
    "                         meta['offset_step'], meta['total_nums'])\n",
    "\n",
    "            batch_size = 32\n",
    "            # test correctness\n",
    "            for col_label in meta_dict.keys():\n",
    "                # load couple emb_paths\n",
    "                emb_data = []\n",
    "                for emb_path in emb_paths[0:count_lim]:\n",
    "                    print(\"emb_path in check loop: \" + str(emb_path))\n",
    "                    emb_data.extend(self.scrape_col_data(emb_path, [col_label])[col_label])\n",
    "                emb_data = np.asarray(emb_data)\n",
    "                # load memmap\n",
    "                mmap = self.load_memmap(corpus_name, nested_dir, topic_ids, col_label, batch_size=None)\n",
    "                print(\"checking equality for \" + str(col_label))\n",
    "                print(\"mmap shape: \" + str(mmap.shape))\n",
    "                print(\"emb_data shape: \" + str(emb_data.shape))\n",
    "                total_added = sum(nums_to_add[col_label]) / meta_dict[col_label]['vector_len']\n",
    "                total_minus_first = int(mmap.shape[0]) - (nums_to_add[col_label][-1] / meta_dict[col_label]['vector_len'])\n",
    "                print(\"total_added / emb_data shape: \" + str(total_added) + \" / \" + str(mmap.shape))\n",
    "                print(\"minus first: \" + str(total_minus_first))\n",
    "                print(\"total_nums_d: \" + str(total_nums_d[col_label]))\n",
    "                \n",
    "                print(\"looping checking 0 equality of memmap\")\n",
    "                zero_arr = np.zeros(meta_dict[col_label]['vector_len'])\n",
    "                on_zeros = True\n",
    "                for i in tqdm_notebook(range(int(meta_dict[col_label]['total_nums'] / meta_dict[col_label]['vector_len']))):\n",
    "                    if not np.array_equal(mmap[i], zero_arr):  # array not zeros\n",
    "                        if on_zeros:\n",
    "                            print(\"not zeroes, count==\" + str(i))\n",
    "#                             print(str(mmap[i]))\n",
    "                            on_zeros = False\n",
    "#                             raise Exception(\"stop zero check\")\n",
    "                    else:  # array is zeros\n",
    "                        if not on_zeros:\n",
    "                            print(\"is zeroes, count==\" + str(i))\n",
    "#                             print(str(mmap[i]))\n",
    "                            on_zeros = True\n",
    "    \n",
    "                for i in tqdm_notebook(range(emb_data.shape[0])):\n",
    "                    m_row = mmap[i]\n",
    "                    emb_row = emb_data[i]\n",
    "                    if np.isscalar(emb_row):\n",
    "                        emb_row = np.asarray([emb_row])\n",
    "                    if not np.array_equal(emb_row, m_row):\n",
    "                        raise Exception(\"Not equal\\nemb_row: \" + str(emb_row) + \"\\nm_row: \" + str(m_row))\n",
    "                print(\"All equal for \" + str(col_label))\n",
    "            \n",
    "            raise Exception(\"stop check\")\n",
    "\n",
    "                    \n",
    "            if verbose:\n",
    "                print(display(path_slice))\n",
    "            print(\"Completed creating memmaps\")\n",
    "        else:\n",
    "            print(\"Already loaded \" + str(col_labels))\n",
    "            \n",
    "    def update_path_df_entry(self, path, col_label, dtype, vector_len, offset_step, total_nums):\n",
    "        mask = (self.path_df['path'] == path) & (self.path_df['col_label'] == col_label)\n",
    "        change_cols = ['dtype', 'vector_len', 'offset_step', 'total_nums', 'complete']\n",
    "        self.path_df.loc[mask, change_cols] = dtype, vector_len, offset_step, total_nums, True\n",
    "        self.save_path_df()\n",
    "        \n",
    "            \n",
    "    def add_path_df_entry(self, corpus_name, nested_dir, col_label, path, topic_ids, return_row_dict=False):\n",
    "        row = {'corpus_name':corpus_name, 'nested_dir':nested_dir, 'col_label':col_label, \n",
    "               'path':path, 'dtype':None, 'vector_len':np.nan, 'total_nums':0, \n",
    "               'offset_step':0, 'topic_ids':topic_ids, 'complete':False}\n",
    "        self.path_df = self.path_df.append(row, ignore_index=True)\n",
    "        self.save_path_df()\n",
    "        if return_row_dict:\n",
    "            return row\n",
    "        \n",
    "    def create_meta_dict(self, path_slice, corpus_name, nested_dir, col_labels, topic_ids, nested_dir_path,\n",
    "                        force_reload=False):\n",
    "        meta_dict = {}\n",
    "        for col_label in col_labels:\n",
    "            col_slice = path_slice[path_slice['col_label'] == col_label]\n",
    "            if len(col_slice) > 0:\n",
    "                if len(col_slice) == 1:\n",
    "                    complete = list(col_slice['complete'])[0]\n",
    "                    if not complete or force_reload:\n",
    "                        # add previous values\n",
    "                        col_slice = col_slice.to_dict(orient='list')\n",
    "                        col_slice['path'][0]\n",
    "                        row_dict = {\"dtype\":col_slice['dtype'][0], \"path\":col_slice['path'][0], \n",
    "                                    \"vector_len\":col_slice['vector_len'][0], \n",
    "                                    \"offset_step\":col_slice['offset_step'][0], \"total_nums\":0, # set to 0 to restart\n",
    "                                    \"initialised\":False, \"completed\":False}  \n",
    "                        meta_dict[col_label] = row_dict\n",
    "                else:\n",
    "                    print(display(col_slice))\n",
    "                    raise Exception(\"Multiple entries in path_df\")\n",
    "            else:\n",
    "                # add to path df\n",
    "                row_dict = self.add_path_df_entry(corpus_name, nested_dir, col_label,\n",
    "                                                 self.generate_new_map_path(nested_dir_path, col_label),\n",
    "                                                 topic_ids, return_row_dict=True)\n",
    "                row_dict['initialised'] = False\n",
    "                meta_dict[col_label] = row_dict\n",
    "        return meta_dict\n",
    "        \n",
    "    def load_memmap(self, corpus_name, nested_dir, topic_ids, col_label, batch_size=None,\n",
    "                   return_input_shape=False):\n",
    "        path_slice = self.slice_path_df(corpus_name, nested_dir, topic_ids)\n",
    "        col_slice = path_slice[path_slice['col_label'] == col_label]\n",
    "        if len(col_slice) == 1:\n",
    "            col_dict = col_slice.to_dict(orient='list')\n",
    "            dtype = col_dict['dtype'][0]\n",
    "            vector_len = int(col_dict['vector_len'][0])\n",
    "            total_nums = int(col_dict['total_nums'][0])\n",
    "            path = col_dict['path'][0]\n",
    "            \n",
    "            shape = None\n",
    "            num_items = int(total_nums / vector_len)\n",
    "            print(\"total_items load_memap: \" + str(num_items))\n",
    "#             print(\"num_items/batch_size: \" + str(num_items/batch_size))\n",
    "#             print(\"num_items % batch_size: \" + str(num_items%batch_size))\n",
    "            if batch_size is not None:\n",
    "                shape = (num_items, batch_size, vector_len)\n",
    "            else:\n",
    "                shape = (num_items, vector_len)\n",
    "            print(\"shape load_memmap: \" + str(shape))\n",
    "            memmap = np.memmap(path, dtype=dtype, mode='r', shape=shape, order=self.order)\n",
    "            print(\"loaded memmap from: \" + str(path))\n",
    "            if return_input_shape:\n",
    "                input_shape = None\n",
    "                if batch_size is None:\n",
    "                    input_shape = (vector_len,)\n",
    "                else:\n",
    "                    input_shape = (batch_size, vector_len)\n",
    "                return memmap, input_shape\n",
    "            return memmap\n",
    "        else:\n",
    "            print(display(path_slice))\n",
    "            raise Exception(str(len(path_slice)) + \" entries for \")\n",
    "    \n",
    "    def slice_path_df(self, corpus_name, nested_dir, topic_ids):\n",
    "        topic_id_str = topic_ids\n",
    "        if type(topic_id_str) != str:\n",
    "            topic_id_str = self.topic_ids_str(topic_ids)\n",
    "            \n",
    "        mask = (self.path_df['corpus_name'] == corpus_name) & (self.path_df['nested_dir'] == nested_dir) & (self.path_df['topic_ids'] == topic_id_str)\n",
    "        path_slice = self.path_df.loc[mask]\n",
    "        return path_slice\n",
    "        \n",
    "    def topic_ids_str(self, topic_ids):\n",
    "        if type(topic_ids) != str:\n",
    "            sort = sorted(topic_ids)\n",
    "            sort = [str(x) for x in sort]\n",
    "            string = \",\".join(sort)\n",
    "            return string\n",
    "        else:\n",
    "            raise Exception(str(topic_ids) + \" is already type str\")\n",
    "        \n",
    "    def save_path_df(self):\n",
    "        save_df_file_type(self.path_df, self.path_df_path, verbose=False)\n",
    "                \n",
    "    def load_path_df(self):\n",
    "        if os.path.exists(self.path_df_path):\n",
    "            path_df = read_df_file_type(self.path_df_path, verbose=True)\n",
    "        else:\n",
    "            path_df = pd.DataFrame(columns=self.path_df_cols)\n",
    "            print(\"memmap path df created from scratch\")\n",
    "        return path_df\n",
    "        \n",
    "    def incompleted_col_labels(self, path_slice, col_labels):\n",
    "        incompleted = []\n",
    "        for col_label in col_labels:\n",
    "            col_slice = path_slice[path_slice['col_label'] == col_label]\n",
    "            if len(col_slice) > 0:\n",
    "                if len(col_slice) == 1:\n",
    "                    complete = list(col_slice['complete'])[0]\n",
    "                    if not complete:\n",
    "                        incompleted.append(col_label)\n",
    "                else:\n",
    "                    print(display(col_slice))\n",
    "                    raise Exception(\"Multiple entries in path_df\")\n",
    "            else:\n",
    "                incompleted.append(col_label)\n",
    "        return incompleted\n",
    "            \n",
    "    def generate_new_map_path(self, nested_dir_path, col_label):\n",
    "        # putting topic_ids in filename too long, use count instead\n",
    "        count = len(self.path_df)\n",
    "        base = str(count) + \"_\" + str(col_label)\n",
    "        mappath = os.path.join(nested_dir_path, base + \".memmap\")\n",
    "        return mappath          \n",
    "            \n",
    "    def scrape_col_data(self, emb_path, col_labels):\n",
    "        # setup return variables\n",
    "        labels = {}\n",
    "        emb_df = load_embeddings(emb_path, verbose=False)\n",
    "        for col_label in col_labels:\n",
    "            if col_label not in emb_df.columns:\n",
    "                raise ValueError(\"Target label \" + str(col_label) + \" is not in file at \" + str(emb_path))\n",
    "            # collect label values from df\n",
    "            labs = np.array(list(emb_df[col_label]))\n",
    "            labels[col_label] = labs\n",
    "        return labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTrainer:\n",
    "    def __init__(self, proj_dir=None, nn_base_save_dir_name=None):\n",
    "        if proj_dir is None:\n",
    "            self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "        else:\n",
    "            self.proj_dir = proj_dir\n",
    "        self.memmap_generator = MemmapGenerator(self.proj_dir)\n",
    "        self.nn_base_save_dir_name = nn_base_save_dir_name\n",
    "        if self.nn_base_save_dir_name is None:\n",
    "            self.nn_base_save_dir_name = \"summarization_models\"\n",
    "        self.nn_base_save_dir_path = os.path.join(self.proj_dir, self.nn_base_save_dir_name)\n",
    "        self.nn_path_df_name = \"nn_path_df.hdf\"\n",
    "        self.nn_path_df_path = os.path.join(self.nn_base_save_dir_path, self.nn_path_df_name)\n",
    "        self.nn_path_df_cols = ['corpus_name', 'nested_dir', 'col_label', 'dir_path']\n",
    "        self.default_test_topics = [1,2,3,4,5,6,8,9,10]\n",
    "        self.default_train_ratio = 0.8\n",
    "        self.min_train_ratio = 0.5\n",
    "    \n",
    "    def train(self, corpus_name, nested_dir, topic_ids, X_col, y_col, tuning_iterations=100, batch_size=32,\n",
    "                force_reload=False, verbose=True):\n",
    "        \"\"\"\n",
    "        1. Generate Data if needed\n",
    "        2. Determine combinations to try\n",
    "        3. Load combination\n",
    "        4. Train network on it\n",
    "        5. Generate summary on test topics\n",
    "        5. Save tuned network, metrics, database entries\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "        self.nn_path_df = self.load_nn_path_df(verbose=verbose)\n",
    "        \n",
    "        # generate data\n",
    "        self.memmap_generator.create_maps(corpus_name, nested_dir, [X_col, y_col], topic_ids, verbose=verbose,\n",
    "                                         force_reload=force_reload)\n",
    "        \n",
    "\n",
    "        if verbose:\n",
    "            print(\"corpus_name: \" + str(corpus_name) + \"\\n\"\n",
    "                 + \"nested_dir: \" + str(nested_dir) + \"\\n\"\n",
    "                 + \"X_input: \" + str(X_col) + \"\\n\"\n",
    "                 + \"y_labels: \" + str(y_col) + \"\\n\"\n",
    "                 + \"train_topics: \" + str(train_topics))\n",
    "\n",
    "#                 # get paths for inputs and total_len of samples\n",
    "        X_map, input_shape = load_memmap(self, corpus_name, nested_dir, topic_ids, X_col, batch_size=batch_size,\n",
    "                                           return_input_shape=True)\n",
    "        # get paths for labels\n",
    "        y_map = load_memmap(self, corpus_name, nested_dir, topic_ids, y_col, batch_size=batch_size,\n",
    "                                           return_input_shape=False)\n",
    "\n",
    "        # create a generator to feed NN samples/batches\n",
    "        batch_generator = BatchGenerator(X_paths, y_paths, batch_size, '.memmap')\n",
    "\n",
    "        save_dir, save_name = self.generate_nn_save_path(corpus_name, nested_dir, col_label,\n",
    "                                                        create_dir=True)\n",
    "        # generate optimised neural network\n",
    "        tuner = NNTuner(save_dir, save_name, input_shape, tuning_iterations=tuning_iterations, \n",
    "                        force_reload=force_reload, batch_size=batch_size)\n",
    "\n",
    "        best_model_path = os.path.join(save_dir, \"best_model\")\n",
    "        best_model, best_hyperparams = tuner.search(batch_generator, save_path=best_model_path, \n",
    "                     return_hyperparams=True)\n",
    "\n",
    "        self.add_path_to_nn_path_df(corpus_name, nested_dir, X_col, y_col, save_dir,\n",
    "                                   save_name, best_hyperparams, batch_size, best_model_path, \n",
    "                                    verbose=verbose)\n",
    "        print(\"Finished tuning neural network\")\n",
    "                \n",
    "    \n",
    "    def load_nn_path_df(self, verbose=True):\n",
    "        if os.path.exists(self.nn_path_df_path):\n",
    "            nn_path_df = read_df_file_type(self.nn_path_df, verbose=verbose)\n",
    "        else:\n",
    "            nn_path_df = pd.DataFrame(columns=self.nn_path_df_cols)\n",
    "            if verbose:\n",
    "                print(\"nn_path_df created from scratch\")\n",
    "        return nn_path_df\n",
    "    \n",
    "    def add_to_nn_path_df(self, corpus_name, nested_dir, input_col_name, label_col_name, tuner_dir, tuner_name,\n",
    "                          best_hyperparams, batch_size, best_model_path, verbose=True):\n",
    "        row = {\"corpus_name\":corpus_name, \"nested_dir\":nested_dir, \"input_col_name\":input_col_name,\n",
    "              \"label_col_names\":label_col_name, \"tuner_dir\":tuner_dir, \"tuner_name\":tuner_name,\n",
    "              \"best_hyperparams\":best_hyperparams, \"batch_size\":batch_size, \"best_model_path\":best_model_path}\n",
    "        \n",
    "        self.nn_path_df = self.nn_path_df.append(row, ignore_index=True)\n",
    "        save_df_file_type(self.nn_path_df, self.nn_path_df_path, verbose=verbose)\n",
    "    \n",
    "    def generate_nn_save_path(self, corpus_name, nested_dir, col_labels, create_dir=True):\n",
    "        col_dir = \"_\".join(convert_to_list(col_labels))\n",
    "        dir_list = [self.nn_base_save_dir_path, corpus_name, nested_dir, col_dir]\n",
    "        # combine directories to form path of subdirectories, create dirs if necessary\n",
    "        dir_path = None\n",
    "        for cur_dir in dir_list:\n",
    "            if dir_path is None:  # first iteration\n",
    "                dir_path = dir_list[0]\n",
    "            else:\n",
    "                dir_path = os.path.join(dir_path, cur_dir)\n",
    "            if not os.path.exists(dir_path) and create_dir:\n",
    "                os.makedirs(dir_path)\n",
    "        # generate name\n",
    "        save_name = \"tuner_proj\"\n",
    "        return dir_path, save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from .hdf file\n",
      "nn_path_df created from scratch\n",
      "Creating memmaps for embedding, cos_sim_nearest_nug\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e056564644d14c1aaaf3b24b77506436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=692), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop count: 0\n",
      "processing embedding\n",
      "total_nums is zero : 0\n",
      "main loop memmap shape: (23933952,)\n",
      "main loop flat shape: (23933952,)\n",
      "memmap after flat assignment\n",
      "[ 1.3180828  -1.3514936  -0.21679163 ...  0.89270985  0.7695162\n",
      " -1.1903995 ]\n",
      "processing cos_sim_nearest_nug\n",
      "total_nums is zero : 0\n",
      "main loop memmap shape: (31164,)\n",
      "main loop flat shape: (31164,)\n",
      "memmap after flat assignment\n",
      "[-0.00916366 -0.00437354 -0.00419822 ... -0.00509296 -0.00916364\n",
      " -0.00916364]\n",
      "loop count: 1\n",
      "processing embedding\n",
      "main loop memmap shape: (48629760,)\n",
      "main loop flat shape: (24695808,)\n",
      "memmap after flat assignment\n",
      "[ 1.3180828  -1.3514936  -0.21679163 ... -0.40571207 -0.22572818\n",
      " -0.28645784]\n",
      "processing cos_sim_nearest_nug\n",
      "main loop memmap shape: (63320,)\n",
      "main loop flat shape: (32156,)\n",
      "memmap after flat assignment\n",
      "[-0.00916366 -0.00437354 -0.00419822 ... -0.00850286 -0.00414493\n",
      " -0.00387387]\n",
      "loop count: 2\n",
      "processing embedding\n",
      "main loop memmap shape: (75329280,)\n",
      "main loop flat shape: (26699520,)\n",
      "memmap after flat assignment\n",
      "[ 1.3180828  -1.3514936  -0.21679163 ... -0.90562016 -0.0545146\n",
      "  0.40072322]\n",
      "processing cos_sim_nearest_nug\n",
      "main loop memmap shape: (98085,)\n",
      "main loop flat shape: (34765,)\n",
      "memmap after flat assignment\n",
      "[-0.00916366 -0.00437354 -0.00419822 ... -0.00367968 -0.00497859\n",
      " -0.00440423]\n",
      "loop count: 3\n",
      "emb_path in check loop: /nfs/proj-repo/AAARG-dissertation/dataset/mine-trects-kba2014-filtered/embeddings/stsb-roberta-base/11_0.hdf\n",
      "emb_path in check loop: /nfs/proj-repo/AAARG-dissertation/dataset/mine-trects-kba2014-filtered/embeddings/stsb-roberta-base/11_200.hdf\n",
      "emb_path in check loop: /nfs/proj-repo/AAARG-dissertation/dataset/mine-trects-kba2014-filtered/embeddings/stsb-roberta-base/11_400.hdf\n",
      "total_items load_memap: 98085\n",
      "shape load_memmap: (98085, 768)\n",
      "loaded memmap from: /nfs/proj-repo/AAARG-dissertation/dataset/mine-trects-kba2014-filtered/embeddings/stsb-roberta-base/0_embedding.memmap\n",
      "checking equality for embedding\n",
      "mmap shape: (98085, 768)\n",
      "emb_data shape: (98085, 768)\n",
      "total_added / emb_data shape: 98085.0 / (98085, 768)\n",
      "minus first: 63320.0\n",
      "total_nums_d: [31164.0, 63320.0, 98085.0]\n",
      "looping checking 0 equality of memmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f595871ebf4a0b9d87d3ea18f4e381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98085), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not zeroes, count==0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f982bcfd75644568205b2eb9f29bbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98085), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All equal for embedding\n",
      "emb_path in check loop: /nfs/proj-repo/AAARG-dissertation/dataset/mine-trects-kba2014-filtered/embeddings/stsb-roberta-base/11_0.hdf\n",
      "emb_path in check loop: /nfs/proj-repo/AAARG-dissertation/dataset/mine-trects-kba2014-filtered/embeddings/stsb-roberta-base/11_200.hdf\n",
      "emb_path in check loop: /nfs/proj-repo/AAARG-dissertation/dataset/mine-trects-kba2014-filtered/embeddings/stsb-roberta-base/11_400.hdf\n",
      "total_items load_memap: 98085\n",
      "shape load_memmap: (98085, 1)\n",
      "loaded memmap from: /nfs/proj-repo/AAARG-dissertation/dataset/mine-trects-kba2014-filtered/embeddings/stsb-roberta-base/1_cos_sim_nearest_nug.memmap\n",
      "checking equality for cos_sim_nearest_nug\n",
      "mmap shape: (98085, 1)\n",
      "emb_data shape: (98085,)\n",
      "total_added / emb_data shape: 98085.0 / (98085, 1)\n",
      "minus first: 63320.0\n",
      "total_nums_d: [31164.0, 63320.0, 98085.0]\n",
      "looping checking 0 equality of memmap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9980e90b645645549c1de3645fb4cedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98085), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not zeroes, count==0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d855e205ba4728a0b66c0cef9bd5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=98085), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All equal for cos_sim_nearest_nug\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "stop check",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-107412fc77ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, tuning_iterations=100, batch_size=32,\n\u001b[0;32m---> 10\u001b[0;31m                 force_reload=force_reload, verbose=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b4d16d297a18>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_name, nested_dir, topic_ids, X_col, y_col, tuning_iterations, batch_size, force_reload, verbose)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# generate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         self.memmap_generator.create_maps(corpus_name, nested_dir, [X_col, y_col], topic_ids, verbose=verbose,\n\u001b[0;32m---> 36\u001b[0;31m                                          force_reload=force_reload)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-e1ca4f461b92>\u001b[0m in \u001b[0;36mcreate_maps\u001b[0;34m(self, corpus_name, nested_dir, col_labels, topic_ids, verbose, force_reload)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All equal for \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop check\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: stop check"
     ]
    }
   ],
   "source": [
    "train_topics = np.arange(11, 47).tolist()  # 11 - 46\n",
    "corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "nested_dir = \"stsb-roberta-base\"\n",
    "X_col = \"embedding\"\n",
    "y_col = \"cos_sim_nearest_nug\"\n",
    "force_reload=True\n",
    "\n",
    "trainer = NNTrainer()\n",
    "trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, tuning_iterations=100, batch_size=32,\n",
    "                force_reload=force_reload, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLabelHandler:\n",
    "    \"\"\"Class that will load and store an instance of the dataset to be fed to a model\n",
    "    \n",
    "    will save in a dir entitled 'samples' in nested_dir\n",
    "    \"\"\"\n",
    "    def __init__(self, proj_dir=None, input_col_name=\"embedding\"):\n",
    "        if proj_dir is None:\n",
    "            self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "        else:\n",
    "            self.proj_dir = proj_dir\n",
    "        self.default_file_type = \".hdf\"\n",
    "        self.path_ret = PathRetriever(proj_dir)\n",
    "        self.label_options = ['cosine_similarity', 'cos_sim_nearest_nug']\n",
    "        self.input_col_name = input_col_name\n",
    "#         self.default_test_topics = [1,2,3,4,5,6,8,9,10]\n",
    "        # label_path_df variables\n",
    "        self.label_path_df_dir = self.path_ret.path_handler.dataset_dir\n",
    "        self.sample_dir_name = \"samples\"\n",
    "        self.label_path_df_path = os.path.join(self.label_path_df_dir, \"label_path_df.hdf\")\n",
    "        self.label_path_df_cols = ['corpus_name', 'nested_dir', 'topic_id', 'col_label', 'batch_instance',\n",
    "                                   'batches_in_topic', 'batch_size', 'shape', 'file_type', 'path', 'exists']\n",
    "        self.possible_file_types = ['.pickle']\n",
    "        \n",
    "        \n",
    "    def generate(self, corpus_names=None, nested_dirs=None, col_labels=None, emb_file_type=None, batch_size=32,\n",
    "                    file_type='.pickle', force_reload=False, verbose=True):\n",
    "        \"\"\"Generate easily loadable inputs/labels files to be fed to NN when needed\"\"\"\n",
    "        \n",
    "        self.label_path_df = self.load_label_path_df(verbose=verbose)\n",
    "        \n",
    "        corpus_names, nested_dirs, col_labels = resolve_input_params(self.path_ret, corpus_names,\n",
    "                                                                    nested_dirs, col_labels, input_col=\"embedding\")\n",
    "        \n",
    "        \n",
    "        if file_type not in self.possible_file_types:\n",
    "            raise Exception(str(file_type) + \" is not a valid file type\")\n",
    "        if emb_file_type is None:  # target file type to load from\n",
    "            emb_file_type = self.default_file_type\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Retrieving the following with batch_size(\" + str(batch_size) +\"): \" \n",
    "                  + str(\", \".join(col_labels)))\n",
    "            \n",
    "        combinations = find_combinations(self.label_path_df, corpus_names, nested_dirs, col_labels,\n",
    "                                         add_topics=True, col_labels_as_list=True, as_tuples=True, \n",
    "                                         force_reload=force_reload, path_retriever=self.path_ret,\n",
    "                                        batch_size=batch_size, file_type=file_type, exists_only=True)\n",
    "\n",
    "        if len(combinations) > 0:\n",
    "            for corpus_name, nested_dir, col_labels, topic_id in tqdm_notebook(combinations):\n",
    "                if verbose:\n",
    "                    print(\"corpus_name: \" + str(corpus_name) + \"\\n\"\n",
    "                         + \"nested_dir: \" + str(nested_dir) + \"\\n\"\n",
    "                         + \"col_labels: \" + str(col_labels) + \"\\n\"\n",
    "                         + \"topic_id: \" + str(topic_id))\n",
    "\n",
    "                emb_paths, nested_dir_path = self.path_ret.get_embedding_paths(corpus_name, nested_dir, \n",
    "                                                            file_type=emb_file_type, verbose=False, \n",
    "                                                            return_dir_path=True, topic_ids=[topic_id])\n",
    "                if len(emb_paths) == 0:\n",
    "                    raise Exception(\"No paths for \" + str(corpus_name) + \", \" + str(nested_dir) + \", \"\n",
    "                                   + str(emb_file_type) + \", topic_id: \" + str(topic_id))\n",
    "                    \n",
    "                # load the selected labels\n",
    "                loaded_labels = self.retrieve_col_data(emb_paths, col_labels, verbose=verbose)\n",
    "                \n",
    "                # create and save batches\n",
    "                for label, label_data in loaded_labels.items():\n",
    "                    batches = self.create_batches(label_data, batch_size)\n",
    "                    shape = batches[0].get_shape()\n",
    "                    topic_id_path = str(topic_id)  # identifier used in filename\n",
    "                    update_paths = []\n",
    "                    pbar = None\n",
    "                    if verbose:\n",
    "                        print(\"Saving batches for \" + str(label))\n",
    "                        pbar = tqdm_notebook(total=len(batches))\n",
    "                    for index, batch in enumerate(batches):\n",
    "                        # create file name for batch\n",
    "                        path = self.generate_path(nested_dir_path, topic_id, index, label, file_type=file_type)\n",
    "                        update_paths.append(path)\n",
    "                        # add to path_df\n",
    "                        if not os.path.exists(path) or force_reload:  # saves resaving files\n",
    "                            self.add_path_to_df(corpus_name, nested_dir, topic_id, label, index, \n",
    "                                                len(batches), batch_size, shape, file_type, path, False)\n",
    "                            # save file\n",
    "                            self.save_object(batch, path, file_type)\n",
    "                        if verbose:\n",
    "                            pbar.update()\n",
    "                    self.update_path_exists(update_paths) \n",
    "                    self.save_label_path_df()\n",
    "                    if verbose:\n",
    "                        print(str(len(batches)) + \" files saved for \" + str(label))\n",
    "            print(\"Completed generating inputs/labels\")\n",
    "        else:\n",
    "            print(\"Input/label combinations fully loaded\")\n",
    "    \n",
    "    def get_paths(self, corpus_name, nested_dir, col_label, topic_ids=None, file_type='.pickle',\n",
    "                  batch_size=32, return_shape=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            return_indices: add start and end index for topic into dict, if topics were to be loaded as a\n",
    "                            continuous list\n",
    "        Return:\n",
    "            Dict where:\n",
    "                    key: a topic_id or 'input_dim'\n",
    "                    value: nested_dict  : keys = \"path\", \"length\", (\"start_idx\", \"end_idx\")\n",
    "        \"\"\"\n",
    "        paths = self.label_path_df\n",
    "        paths = paths[(paths['corpus_name'] == corpus_name) \n",
    "                      & (paths['nested_dir'] == nested_dir)\n",
    "                      & (paths['col_label'] == col_label)\n",
    "                      & (paths['batch_size'] == batch_size)\n",
    "                      & (paths['file_type'] == file_type)]\n",
    "        if topic_ids is not None:\n",
    "            paths = paths[paths['topic_id'].isin(topic_ids)]\n",
    "            \n",
    "        # sort column so consistent ordering\n",
    "        paths = paths.sort_values(by=['topic_id', 'batch_instance'], ascending=True)\n",
    "        path_list = list(paths['path'])\n",
    "        \n",
    "        shape = list(paths['shape'].unique())\n",
    "        if len(input_dim) > 1:\n",
    "            raise Exception(\"Dimensions of list objects varies: \" + str(shape))\n",
    "        else:\n",
    "            shape = shape[0]\n",
    "            \n",
    "        if return_shape:\n",
    "            return path_list, shape\n",
    "        return path_list\n",
    "        \n",
    "    \n",
    "    def corpus_topic_ids(self, corpus_name):\n",
    "        \"\"\"Num topics for given corpus_name\"\"\"\n",
    "        topic_ids = list(self.label_path_df[self.label_path_df['corpus_name'] == corpus_name]['topic_id'].unique())\n",
    "        return topic_ids\n",
    "                        \n",
    "    def load_label_path_df(self, verbose=True):\n",
    "        label_path_df = None\n",
    "        if verbose:\n",
    "            print(\"Loading label_path_df\")\n",
    "        if os.path.exists(self.label_path_df_path):\n",
    "            label_path_df = read_df_file_type(self.label_path_df_path, verbose=True)\n",
    "        else:\n",
    "            label_path_df = pd.DataFrame(columns=self.label_path_df_cols)\n",
    "            if verbose:\n",
    "                print(\"label_path_df created from scratch\")\n",
    "        return label_path_df\n",
    "    \n",
    "    def add_path_to_df(self, corpus_name, nested_dir, topic_id, col_label, batch_instance, batches_in_topic,\n",
    "                      batch_size, shape, file_type, path, exists):\n",
    "        row = {\"corpus_name\":corpus_name, \"nested_dir\":nested_dir, \"topic_id\":topic_id, \"col_label\":col_label,\n",
    "               \"batch_instance\":batch_instance, \"batches_in_topic\":batches_in_topic, \"batch_size\":batch_size,\n",
    "               \"shape\":shape, \"file_type\":file_type, \"path\":path, \"exists\":exists}\n",
    "        self.label_path_df = self.label_path_df.append(row, ignore_index=True)\n",
    "        \n",
    "    def save_label_path_df(self):\n",
    "        save_df_file_type(self.label_path_df, self.label_path_df_path, verbose=False)\n",
    "        \n",
    "    def update_path_exists(self, path):\n",
    "        path = convert_to_list(path)\n",
    "        self.label_path_df.loc[self.label_path_df['path'].isin(path), 'exists'] = True\n",
    "        \n",
    "    def create_batches(self, samples, batch_size):\n",
    "        batch_list = []\n",
    "        for i in range(0, len(samples), batch_size):\n",
    "            slice_end_idx = i + batch_size\n",
    "            if slice_end_idx > len(samples):  # leave last potential batch if doesn't divide evenly\n",
    "                break\n",
    "            batch_slice = samples[i:slice_end_idx]  # end step is exclusive\n",
    "            test_dims = batch_slice[0]  # debug\n",
    "            is_scalar = np.isscalar(batch_slice[0])\n",
    "            batch_slice = tf.convert_to_tensor(batch_slice)\n",
    "            input_dim = None  # debug\n",
    "            if is_scalar:\n",
    "                input_dim = 1  # debug\n",
    "                batch_slice = labels = tf.expand_dims(batch_slice, 1)  # add dimension to get appropriate shape\n",
    "            else:\n",
    "                input_dim = len(test_dims)\n",
    "            if batch_slice.shape != (batch_size, input_dim):\n",
    "                raise Exception(\"Wrong shape for batch_slice: \" + str(batch_slice.shape)\n",
    "                               + \"\\nExpected shape: \" + str((batch_size, input_dim)))\n",
    "            batch_list.append(batch_slice)\n",
    "        return batch_list\n",
    "                \n",
    "    def retrieve_col_data(self, emb_paths, col_labels, verbose=True):\n",
    "        # setup return variables\n",
    "        labels = {}\n",
    "        for col_label in col_labels:\n",
    "            labels[col_label] = []\n",
    "        # search through paths for labels\n",
    "        pbar = None\n",
    "        if verbose:\n",
    "            print(\"Retrieving samples from dataframes\")\n",
    "            pbar = tqdm_notebook(total=len(emb_paths))\n",
    "        for emb_path in emb_paths['path']:\n",
    "            emb_df = load_embeddings(emb_path, verbose=False)\n",
    "            for col_label in col_labels:\n",
    "                if col_label not in emb_df.columns:\n",
    "                    raise ValueError(\"Target label \" + str(col_label) + \" is not in file at \" + str(emb_path))\n",
    "                # collect label values from df\n",
    "                labs = list(emb_df[col_label])\n",
    "                labels[col_label].extend(labs)\n",
    "            if verbose:\n",
    "                pbar.update()\n",
    "        return labels\n",
    "    \n",
    "    def save_object(self, obj, path, file_type, offset=None):\n",
    "        if file_type == '.pickle':\n",
    "            with open(path, 'wb') as handle:\n",
    "                pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def generate_path(self, nested_dir_path, topic_id, instance_num, col_label, file_type='.pickle'):\n",
    "        filename = str(col_label) + \"_\" + str(topic_id) + \"_\" + str(instance_num) + str(file_type)\n",
    "        dir_path = os.path.join(nested_dir_path, self.sample_dir_name)\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        return file_path\n",
    "    \n",
    "    def delete_files(self):\n",
    "        paths = self.label_path_df['path']\n",
    "        deleted_paths = []\n",
    "        print(\"Deleting \" + str(len(paths)) + \" paths\")\n",
    "        for path in tqdm_notebook(paths):\n",
    "            if os.path.exists(path):\n",
    "                os.remove(path)\n",
    "            deleted_paths.append(path)\n",
    "        self.label_path_df[~self.label_path_df['path'].isin(deleted_paths)]\n",
    "        self.save_label_path_df()\n",
    "        print(\"deleted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Tuning Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTrainer:\n",
    "    def __init__(self, proj_dir=None, nn_base_save_dir_name=None):\n",
    "        if proj_dir is None:\n",
    "            self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "        else:\n",
    "            self.proj_dir = proj_dir\n",
    "        self.input_handler = InputLabelHandler(self.proj_dir)\n",
    "        self.nn_base_save_dir_name = nn_base_save_dir_name\n",
    "        if self.nn_base_save_dir_name is None:\n",
    "            self.nn_base_save_dir_name = \"summarization_models\"\n",
    "        self.nn_base_save_dir_path = os.path.join(self.proj_dir, self.nn_base_save_dir_name)\n",
    "        self.nn_path_df_name = \"nn_path_df.hdf\"\n",
    "        self.nn_path_df_path = os.path.join(self.nn_base_save_dir_path, self.nn_path_df_name)\n",
    "        self.nn_path_df_cols = ['corpus_name', 'nested_dir', 'col_label', 'dir_path']\n",
    "        self.default_test_topics = [1,2,3,4,5,6,8,9,10]\n",
    "        self.default_train_ratio = 0.8\n",
    "        self.min_train_ratio = 0.5\n",
    "    \n",
    "    def train(self, corpus_names=None, nested_dirs=None, col_labels=None, tuning_iterations=100,\n",
    "              train_topics = None, test_topics=None, input_col_name=\"embedding\", batch_size=32,\n",
    "              train_ratio=None, sample_file_type='.pickle', force_reload=False, verbose=True):\n",
    "        \"\"\"\n",
    "        1. Generate Data if needed\n",
    "        2. Determine combinations to try\n",
    "        3. Load combination\n",
    "        4. Train network on it\n",
    "        5. Generate summary on test topics\n",
    "        5. Save tuned network, metrics, database entries\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "        self.nn_path_df = self.load_nn_path_df(verbose=verbose)\n",
    "        \n",
    "        # generate data\n",
    "        self.input_handler.generate(corpus_names=corpus_names, nested_dirs=nested_dirs, col_labels=col_labels,\n",
    "                                   force_reload=force_reload, verbose=verbose, file_type=sample_file_type)\n",
    "        \n",
    "        # get our dataset identifiers, used to load correct inputs/labels\n",
    "        corpus_names, nested_dirs, col_labels = resolve_input_params(self.input_handler.path_ret,\n",
    "                                                                     corpus_names, nested_dirs, col_labels)\n",
    "        \n",
    "        # resolve train/test split\n",
    "        if train_ratio is None:\n",
    "            train_ratio = self.default_train_ratio\n",
    "        else:\n",
    "            if train_ratio < self.min_train_ratio:\n",
    "                raise Exception(\"Train ratio must be at least 0.5\")\n",
    "        corpus_topics = self.resolve_topics_per_corpus(corpus_names, train_topics, test_topics, train_ratio)\n",
    "        \n",
    "        combinations = find_combinations(self.nn_path_df, corpus_names, nested_dirs, col_labels, add_topics=False,\n",
    "                                        col_labels_as_list=False, as_tuples=True, force_reload=force_reload)\n",
    "        if len(combinations) > 0:\n",
    "            for corpus_name, nested_dir, col_label in tqdm_notebook(combinations):  # will this loop through input_col? \n",
    "                train_topics = corpus_topics[corpus_name]['train']\n",
    "                if verbose:\n",
    "                    print(\"corpus_name: \" + str(corpus_name) + \"\\n\"\n",
    "                         + \"nested_dir: \" + str(nested_dir) + \"\\n\"\n",
    "                         + \"X_input: \" + str(input_col_name) + \"\\n\"\n",
    "                         + \"y_labels: \" + str(col_label) + \"\\n\"\n",
    "                         + \"train_topics: \" + str(train_topics))\n",
    "\n",
    "#                 # get paths for inputs and total_len of samples\n",
    "                X_paths, input_shape = self.input_handler.get_paths(corpus_name, nested_dir, \n",
    "                                            input_col_name, topic_ids=train_topics, file_type=sample_file_type,\n",
    "                                            return_shape=True)\n",
    "                # get paths for labels\n",
    "                y_paths = self.input_handler.get_paths(corpus_name, nested_dir, col_label,\n",
    "                                            topic_ids=train_topics, return_shape=False, file_type=sample_file_type)\n",
    "                \n",
    "                # ensure matching path dicts\n",
    "                if len(X_paths) != len(y_paths):\n",
    "                    print(\"Length of X and y paths do not match: \" + str(len(X_paths)) + \" / \" + str(len(y_paths)))\n",
    "                \n",
    "                # create a generator to feed NN samples/batches\n",
    "                batch_generator = BatchGenerator(X_paths, y_paths, batch_size, sample_file_type)\n",
    "                \n",
    "                save_dir, save_name = self.generate_nn_save_path(corpus_name, nested_dir, col_label,\n",
    "                                                                create_dir=True)\n",
    "                # generate optimised neural network\n",
    "                tuner = NNTuner(save_dir, save_name, input_shape, tuning_iterations=tuning_iterations, \n",
    "                                force_reload=force_reload, batch_size=batch_size)\n",
    "\n",
    "                best_model_path = os.path.join(save_dir, \"best_model\")\n",
    "                best_model, best_hyperparams = tuner.search(batch_generator, save_path=best_model_path, \n",
    "                             return_hyperparams=True)\n",
    "\n",
    "                self.add_path_to_nn_path_df(corpus_name, nested_dir, input_col_name, col_label, save_dir,\n",
    "                                           save_name, best_hyperparams, batch_size, best_model_path, \n",
    "                                            verbose=verbose)\n",
    "            print(\"Finished tuning neural networks\")\n",
    "        else:\n",
    "            print(\"All neural networks have previously been tuned\")\n",
    "            \n",
    "            \n",
    "    def resolve_topics_per_corpus(self, corpus_names, train_topics, test_topics, train_ratio):\n",
    "        \"\"\"Resolve the train/test corpus for each corpus\n",
    "        This is a bit off in logic\n",
    "        \"\"\"\n",
    "        corpus_topics_dict = defaultdict(dict)\n",
    "        for corpus_name in corpus_names:\n",
    "            corpus_topics = self.input_handler.corpus_topic_ids(corpus_name)\n",
    "            corp_test, corp_train = test_topics, train_topics\n",
    "            # resolve test_topics for corpus\n",
    "            if corp_test is None:\n",
    "                corp_test = self.default_test_topics\n",
    "            if corp_train is None:\n",
    "                corp_train = [x for x in corpus_topics if x not in corp_test]\n",
    "            \n",
    "            # get rid of repeats\n",
    "            corp_test, corp_train = set(corp_test), set(corp_train)\n",
    "            \n",
    "            cur_train_ratio = len(corpus_topics) / len(corp_train)\n",
    "            if cur_train_ratio < self.min_train_ratio:\n",
    "                # set to train_ratio instead\n",
    "                num_train = math.floor(len(corpus_topics) * train_ratio)\n",
    "                num_test = len(corpus_topics) - num_train\n",
    "                corp_test = corpus_topics[0:num_test]\n",
    "                corp_train = corpus_topics[num_test:]\n",
    "                \n",
    "            # check for overlap in train/test topics\n",
    "            if not corp_test.isdisjoint(corp_train):  # overlap between topics\n",
    "                raise Exception(\"Train and test sets contain overlapping topic_ids\\nTrain: \" + str(corp_train)\n",
    "                               +\"\\nTest: \" + str(corp_test))\n",
    "            \n",
    "            corp_train, corp_test = list(corp_train), list(corp_test)\n",
    "            corpus_topics_dict[corpus_name]['train'] = corp_train\n",
    "            corpus_topics_dict[corpus_name]['test'] = corp_test\n",
    "        return corpus_topics_dict\n",
    "                \n",
    "    \n",
    "    def load_nn_path_df(self, verbose=True):\n",
    "        if os.path.exists(self.nn_path_df_path):\n",
    "            nn_path_df = read_df_file_type(self.nn_path_df, verbose=verbose)\n",
    "        else:\n",
    "            nn_path_df = pd.DataFrame(columns=self.nn_path_df_cols)\n",
    "            if verbose:\n",
    "                print(\"nn_path_df created from scratch\")\n",
    "        return nn_path_df\n",
    "    \n",
    "    def add_to_nn_path_df(self, corpus_name, nested_dir, input_col_name, label_col_name, tuner_dir, tuner_name,\n",
    "                          best_hyperparams, batch_size, best_model_path, verbose=True):\n",
    "        row = {\"corpus_name\":corpus_name, \"nested_dir\":nested_dir, \"input_col_name\":input_col_name,\n",
    "              \"label_col_names\":label_col_name, \"tuner_dir\":tuner_dir, \"tuner_name\":tuner_name,\n",
    "              \"best_hyperparams\":best_hyperparams, \"batch_size\":batch_size, \"best_model_path\":best_model_path}\n",
    "        \n",
    "        self.nn_path_df = self.nn_path_df.append(row, ignore_index=True)\n",
    "        save_df_file_type(self.nn_path_df, self.nn_path_df_path, verbose=verbose)\n",
    "    \n",
    "    def generate_nn_save_path(self, corpus_name, nested_dir, col_labels, create_dir=True):\n",
    "        col_dir = \"_\".join(convert_to_list(col_labels))\n",
    "        dir_list = [self.nn_base_save_dir_path, corpus_name, nested_dir, col_dir]\n",
    "        # combine directories to form path of subdirectories, create dirs if necessary\n",
    "        dir_path = None\n",
    "        for cur_dir in dir_list:\n",
    "            if dir_path is None:  # first iteration\n",
    "                dir_path = dir_list[0]\n",
    "            else:\n",
    "                dir_path = os.path.join(dir_path, cur_dir)\n",
    "            if not os.path.exists(dir_path) and create_dir:\n",
    "                os.makedirs(dir_path)\n",
    "        # generate name\n",
    "        save_name = \"tuner_proj\"\n",
    "        return dir_path, save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.memmap.html#numpy.memmap\n",
    "memmap might provide way to access segments of an array from a binary file\n",
    "\n",
    "numpy also provides functions to read/save individual arrays to text files (could be slow)\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html#numpy.savetxt\n",
    "\n",
    "can save individual ndarrays as binary files  - might have an issue with precision, need to check\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.ndarray.tofile.html#numpy.ndarray.tofile\n",
    "\n",
    "might have issues with it being ndarray and not np.array, difference?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# corpus_names = [\"mine-trects-kba2014-filtered\"]\n",
    "# sample_file_type = '.pickle'\n",
    "\n",
    "# trainer = NNTrainer()\n",
    "\n",
    "# trainer.train(corpus_names=corpus_names, sample_file_type=sample_file_type, verbose=True, force_reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Generating Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_threads = 32\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = str(num_threads)\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)  # rid tqdm depreciation warning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ipynb.fs\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import kerastuner as kt\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "import spacy\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"gpus:\")\n",
    "print(gpus)\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)  # should stop error with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTuner:\n",
    "    def __init__(self, save_dir, save_name, input_shape, tuning_iterations=2, max_epochs=15, reduction_factor=3,\n",
    "                 batch_size=32, force_reload=False, output_dims=1, layer_type=\"LSTM\"):\n",
    "        \"\"\"Can save using project_name param, if overwrite false then will reload where it started\n",
    "        In Tuner Class documentation\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_dims = output_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_type = layer_type.upper()\n",
    "        self.models = []\n",
    "        self.distribution_strategy=tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "        self.tuner = Hyperband(self.build_model, \n",
    "                          objective='mean_squared_error', \n",
    "                          max_epochs=max_epochs,\n",
    "                          hyperband_iterations=tuning_iterations,\n",
    "                          factor=reduction_factor,  # keras-tuner default is 3\n",
    "                          directory=save_dir,\n",
    "                          project_name=save_name,\n",
    "                          overwrite=force_reload,\n",
    "                          tune_new_entries = True,\n",
    "                          allow_new_entries = True,\n",
    "                          distribution_strategy=self.distribution_strategy)\n",
    "\n",
    "        \n",
    "    def build_model(self, hp):\n",
    "        model = Sequential()\n",
    "        # specify input layer to ensure correct input shape\n",
    "        \n",
    "        ilayer = None\n",
    "        if self.layer_type == \"LSTM\":\n",
    "            batch_shape = list(self.input_shape)\n",
    "            batch_shape.insert(0, self.batch_size)\n",
    "            batch_shape = tuple(batch_shape)\n",
    "            print(\"batch_shape : \" + str(batch_shape))\n",
    "            ilayer = tf.keras.Input(batch_shape=batch_shape,\n",
    "                                name='input_layer')\n",
    "        elif self.layer_type == \"DENSE\":\n",
    "            ilayer = tf.keras.Input(shape=self.input_shape[-1], batch_size=self.batch_size)\n",
    "        model.add(ilayer)\n",
    "        \n",
    "        # add hidden layers\n",
    "        num_hidden_layers = hp.Int('num_hidden_layers', min_value=1, max_value=6)\n",
    "        for i in range(num_hidden_layers):\n",
    "            \n",
    "            units_name = 'hidden_units_' + str(i)\n",
    "            units = hp.Int(units_name, min_value=32, max_value=2048, step=32)\n",
    "            print(units_name + \": \" + str(units))\n",
    "            name = 'hidden_layer_' + str(i)\n",
    "            \n",
    "            if self.layer_type == \"DENSE\":\n",
    "                model.add(Dense(units=units,\n",
    "                                activation='tanh',\n",
    "                                name=name))\n",
    "                \n",
    "            elif self.layer_type == \"LSTM\":\n",
    "                return_sequences = True\n",
    "                if i == num_hidden_layers - 1:\n",
    "                    return_sequences = False  # don't return sequences to Dense layer\n",
    "                    \n",
    "                model.add(LSTM(units=units,\n",
    "                              activation='tanh',\n",
    "                              recurrent_activation='sigmoid',\n",
    "                              unroll=False,\n",
    "                              use_bias=True,\n",
    "                              return_sequences=return_sequences,\n",
    "                              batch_size=self.batch_size,\n",
    "                              stateful=False,\n",
    "                              name=name))\n",
    "            else:\n",
    "                raise Exception(str(self.layer_type) + \" is not a valid layer type\")\n",
    "            \n",
    "        # add output layer\n",
    "        model.add(Dense(units=self.output_dims, \n",
    "                        activation='tanh'))\n",
    "        \n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=0.000001, max_value=0.001))      \n",
    "        \n",
    "        model.compile(optimizer=opt, loss='huber', metrics=['mean_squared_error'])  # add metrics here\n",
    "        \n",
    "        self.models.append(model)\n",
    "        return model\n",
    "    \n",
    "    def search(self, batch_generator, best_model_dir):\n",
    "        \"\"\"Find optimal model given dataset\n",
    "        \"\"\"\n",
    "        self.tuner.search(x=batch_generator, verbose=1, use_multiprocessing=False, workers=num_threads)\n",
    "        best_models = self.tuner.get_best_models(num_models=5)\n",
    "        if best_model_dir is not None:\n",
    "            for i in range(len(best_models)):\n",
    "                print(\"Saving best model number: \" + str(i))\n",
    "                save_path = os.path.join(best_model_dir, str(i))\n",
    "                best_models[i].save(save_path)\n",
    "            hyperparams = self.tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        best_model_path = os.path.join(best_model_dir, str(0))\n",
    "        return best_models[0], hyperparams, best_model_path\n",
    "    \n",
    "\n",
    "# from collections import OrderedDict\n",
    "# from collections import deque\n",
    "\n",
    "class BatchGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Class to load in dataset that is too large to load into memory at once\n",
    "    \n",
    "    Do check in class before to make sure all X lists and y lists are same length\n",
    "    \n",
    "    https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size, num_batches):\n",
    "        if batch_size is None:\n",
    "            self.batch_size = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "#         self.shuffle = False  # make sure linear progression through dataset for sake of memory efficiency\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return self.num_batches\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one batch of data\"\"\"\n",
    "        inputs = self.load_samples(self.X, idx)\n",
    "        labels = self.load_samples(self.y, idx)\n",
    "        return inputs, labels\n",
    "    \n",
    "    \n",
    "    def load_samples(self, path, idx):\n",
    "        samples = path[idx]\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .defs.corpus_loader import PathRetriever, load_embeddings, load_topics, read_df_file_type, save_df_file_type\n",
    "from .defs.corpus_loader import convert_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_input_params(path_ret, corpus_names, nested_dirs, col_labels, input_col=None):\n",
    "    \"\"\"Helper function to resolve the selection of input params that determine what data to load/generate\"\"\"\n",
    "    # resolve corpus_names\n",
    "    if corpus_names is None:\n",
    "        corpus_names = path_ret.get_corpus_names()\n",
    "        if len(corpus_names) == 0:\n",
    "            raise Exception(\"There are no corpuses to load from\")\n",
    "    # resolve col_labels\n",
    "    if col_labels is None:  # our columns to generate files for\n",
    "        col_labels = default_col_labels.copy()\n",
    "        if input_col is not None:\n",
    "            col_labels.append(input_col)\n",
    "    # resolve nested_dirs\n",
    "    if type(nested_dirs) != dict:  # if output gets passed through again\n",
    "        nested_dict = {}\n",
    "        for corpus_name in corpus_names:  # get the nested dir for each corpus name\n",
    "            nested_dict[corpus_name] = path_ret.get_nested_dirs(corpus_name, \"embeddings\")\n",
    "            if nested_dirs is not None:\n",
    "                # add only selected nested_dirs for this corpus_name\n",
    "                nested_dict[corpus_name] = [x for x in nested_dict[corpus_name] if x in nested_dirs]\n",
    "        nested_dirs = nested_dict\n",
    "    # make sure there is at least one entry in nested_dict\n",
    "    empty_dirs = [len(x) == 0 for x in nested_dirs.values()]  # get if empty for each item\n",
    "    if all(empty_dirs):\n",
    "        raise Exception(\"There are no nested_dirs matching the selection\")\n",
    "    return corpus_names, nested_dirs, col_labels\n",
    "\n",
    "def corpus_name_topic_ids(path_retriever, corpus_name):\n",
    "    topic_path = path_retriever.get_topic_path(corpus_name, verbose=False)\n",
    "    topic_df = load_topics(topic_path, verbose=False)\n",
    "    topic_ids = list(topic_df['id'].unique())\n",
    "    return topic_ids\n",
    "\n",
    "def find_combinations(path_df, corpus_names, nested_dirs, col_labels, add_topics=False, col_labels_as_list=False,\n",
    "                      as_tuples=True, force_reload=False, path_retriever=None, batch_size=None, file_type=None,\n",
    "                     exists_only=False):\n",
    "    \"\"\"Find the combinations that have not been generated/trained already in path_df\n",
    "    \n",
    "    Tuple ordering: (corpus_name, nested_dir, col_label/[col_labels], **topic_id**)\n",
    "    \"\"\"\n",
    "    if exists_only:\n",
    "        path_df = path_df[path_df['exists'] == True]  # checking of path_df is only concerned with existing files\n",
    "    if batch_size is not None:\n",
    "        path_df = path_df[path_df['batch_size'] == batch_size]\n",
    "    if file_type is not None:\n",
    "        path_df = path_df[path_df['file_type'] == file_type]\n",
    "    topic_ids = {}\n",
    "    if add_topics:  # find topic_ids for each corpus\n",
    "        for corpus_name in corpus_names:\n",
    "            if path_retriever is not None:\n",
    "                topic_ids[corpus_name] = corpus_name_topic_ids(path_retriever, corpus_name)\n",
    "            else:\n",
    "                raise Exception(\"If add_topics is True then path_retriever must be set to an instance of PathRetriever\")\n",
    "    # get possible combinations\n",
    "    combinations = []\n",
    "    for corpus_name in corpus_names:\n",
    "        for nested_dir in nested_dirs[corpus_name]:\n",
    "            combo_path = path_df[(path_df['corpus_name'] == corpus_name)\n",
    "                                    & (path_df['nested_dir'] == nested_dir)]\n",
    "            combo = [corpus_name, nested_dir]\n",
    "            if add_topics:  # create permutations with topic_ids\n",
    "                topic_combo_dict = defaultdict(list)\n",
    "                for label in col_labels:\n",
    "                    for topic_id in topic_ids[corpus_name]:  # check if label exists for topic_id\n",
    "                        topic_path = combo_path[(combo_path['col_label'] == label)\n",
    "                                               & (combo_path['topic_id'] == topic_id)]\n",
    "                        if len(topic_path) == 0 or force_reload:\n",
    "                            topic_combo_dict[topic_id].append(label)\n",
    "                topic_combos = []\n",
    "                for topic_id, labels in topic_combo_dict.items():\n",
    "                    topic_combos = []\n",
    "                    if col_labels_as_list:  # add single tuple with all missing col_labels for topic_id\n",
    "                        topic_combo = copy.deepcopy(combo)\n",
    "                        topic_combo.append(labels)\n",
    "                        topic_combo.append(topic_id)\n",
    "                        topic_combos.append(topic_combo)\n",
    "                    else:\n",
    "                        for label in labels:  # add a tuple for each missing col_label for topic_id\n",
    "                            topic_combo = copy.deepcopy(combo)\n",
    "                            topic_combo.append(topic_id)\n",
    "                            topic_combos.append(topic_combo)\n",
    "                    combinations.extend(topic_combos)\n",
    "            else:  # create permutations with col_labels only\n",
    "                label_combos = []\n",
    "                add_labels = None\n",
    "                if not force_reload:  # find which col_labels don't exist already\n",
    "                    exist_labels = list(combo_path['col_label'].unique())\n",
    "                    add_labels = [x for x in col_labels if x not in exist_labels]\n",
    "                else:\n",
    "                    add_labels = copy.deepcopy(col_labels)  # force_reload add all labels\n",
    "                if col_labels_as_list:  # add single tuple\n",
    "                    label_combo = copy.deepcopy(combo)\n",
    "                    label_combo.append(add_labels)\n",
    "                    label_combos.append(label_combo)\n",
    "                else:\n",
    "                    for add_label in add_labels:  # add tuple for each col_label\n",
    "                        label_combo = copy.deepcopy(combo)\n",
    "                        label_combo.append(add_label)\n",
    "                        label_combos.append(label_combo)\n",
    "                combinations.extend(label_combos)\n",
    "                \n",
    "    if as_tuples:\n",
    "        combinations = [tuple(x) for x in combinations]\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemmapGenerator:\n",
    "    def __init__(self, proj_dir='/nfs/proj-repo/AAARG-dissertation'):\n",
    "        self.proj_dir = proj_dir\n",
    "        self.default_file_type = \".hdf\"\n",
    "        self.path_ret = PathRetriever(proj_dir)\n",
    "        self.path_df_cols = ['corpus_name', 'nested_dir', 'col_label', 'path', 'dtype', 'vector_len', \n",
    "                             'total_nums', 'offset_step', 'topic_ids', 'complete']\n",
    "        self.dataset_dir = self.path_ret.path_handler.dataset_dir\n",
    "        self.sample_dir = os.path.join(self.dataset_dir, \"samples\")\n",
    "        self.path_df_path = os.path.join(self.dataset_dir, \"memmap_paths.hdf\")\n",
    "        self.path_df = self.load_path_df()\n",
    "        self.order = 'C'\n",
    "        \n",
    "        \n",
    "    def create_maps(self, corpus_name, nested_dir, col_labels, topic_ids, verbose=True, force_reload=False):\n",
    "        # check if already completed\n",
    "        path_slice = self.slice_path_df(corpus_name, nested_dir, topic_ids)\n",
    "        emb_paths = self.path_ret.get_embedding_paths(corpus_name, nested_dir, \n",
    "                                    file_type=self.default_file_type, verbose=False, \n",
    "                                    return_dir_path=False, topic_ids=topic_ids)\n",
    "        emb_paths = list(emb_paths['path'])\n",
    "        # load partial information on maps that need completed\n",
    "        meta_dict = self.create_meta_dict(path_slice, corpus_name, nested_dir, col_labels, \n",
    "                                          self.topic_ids_str(topic_ids), force_reload=force_reload)\n",
    "        \n",
    "        if len(meta_dict) > 0:\n",
    "            if verbose:\n",
    "                print(\"Creating memmaps for \" + str(\", \".join(col_labels)) + \"\\nwith topics: \" + str(topic_ids))\n",
    "                if force_reload:\n",
    "                    print(\"force_reload set to True\")\n",
    "            for emb_path in tqdm_notebook(emb_paths):\n",
    "                # get the cols that haven't been loaded for this path\n",
    "                # scrape data from dataframe\n",
    "                label_data = self.scrape_col_data(emb_path, meta_dict.keys())\n",
    "                # add data to memmap\n",
    "                for col_label, data in label_data.items():\n",
    "                    col_dict = meta_dict[col_label]\n",
    "                    if not col_dict['initialised']:\n",
    "                        col_dict['dtype'] = data.dtype\n",
    "                        ndim = data.ndim\n",
    "                        if ndim == 1:  # 1d\n",
    "                            col_dict['vector_len'] = 1\n",
    "                        elif ndim == 2:  # 2d\n",
    "                            col_dict['vector_len'] = data.shape[1]\n",
    "                        else:\n",
    "                            raise Exception(\"Too many dimensions: \" + str(data.shape))\n",
    "                        col_dict['offset_step'] = data.dtype.itemsize\n",
    "                        col_dict['initialised'] = True\n",
    "                        \n",
    "                        \n",
    "                    # load meta_dict vars, save hashing time\n",
    "                    total_nums = col_dict['total_nums']\n",
    "                    offset_step = col_dict['offset_step']\n",
    "                    path = col_dict['path']\n",
    "                    dtype = col_dict['dtype']\n",
    "\n",
    "                    # add data to map\n",
    "                    flat = data.ravel()\n",
    "                    num_to_add = len(flat)\n",
    "                    \n",
    "                    memmap = None\n",
    "                    if total_nums != 0:  # write to existing file \n",
    "                        memmap = np.memmap(path, dtype=dtype, mode='r+', offset=0, \n",
    "                                       order=self.order, shape=(total_nums + num_to_add,))\n",
    "                    else:  # create new file on first iteration\n",
    "                        memmap = np.memmap(path, dtype=dtype, mode='w+', offset=0, \n",
    "                                       order=self.order, shape=(num_to_add,))\n",
    "                    \n",
    "                    memmap[total_nums:total_nums+num_to_add] = flat[:]\n",
    "                    if not np.array_equal(memmap[total_nums:total_nums+num_to_add], flat):\n",
    "                        print(\"memmap: \" + str(memmap[total_nums:total_nums+num_to_add]))\n",
    "                        print(\"flat: \" + str(flat))\n",
    "                        raise Exception(\"Memmap and flat not equal\")\n",
    "                    \n",
    "                    memmap.flush()\n",
    "\n",
    "                    # update fields\n",
    "                    col_dict['total_nums'] += num_to_add\n",
    "                    \n",
    "            for col_label, meta in meta_dict.items():\n",
    "                self.update_path_df_entry(meta['path'], col_label, meta['dtype'], meta['vector_len'],\n",
    "                         meta['offset_step'], meta['total_nums'])\n",
    "\n",
    "            if verbose:\n",
    "                print(display(path_slice))\n",
    "            print(\"Completed creating memmaps\")\n",
    "        else:\n",
    "            print(\"Already loaded \" + str(col_labels))\n",
    "            \n",
    "    def update_path_df_entry(self, path, col_label, dtype, vector_len, offset_step, total_nums):\n",
    "        mask = (self.path_df['path'] == path) & (self.path_df['col_label'] == col_label)\n",
    "        change_cols = ['dtype', 'vector_len', 'offset_step', 'total_nums', 'complete']\n",
    "        self.path_df.loc[mask, change_cols] = dtype, vector_len, offset_step, total_nums, True\n",
    "        self.save_path_df()\n",
    "        \n",
    "            \n",
    "    def add_path_df_entry(self, corpus_name, nested_dir, col_label, path, topic_ids, return_row_dict=False):\n",
    "        row = {'corpus_name':corpus_name, 'nested_dir':nested_dir, 'col_label':col_label, \n",
    "               'path':path, 'dtype':None, 'vector_len':np.nan, 'total_nums':0, \n",
    "               'offset_step':0, 'topic_ids':topic_ids, 'complete':False}\n",
    "        self.path_df = self.path_df.append(row, ignore_index=True)\n",
    "        self.save_path_df()\n",
    "        if return_row_dict:\n",
    "            return row\n",
    "        \n",
    "    def create_meta_dict(self, path_slice, corpus_name, nested_dir, col_labels, topic_ids,\n",
    "                        force_reload=False):\n",
    "        meta_dict = {}\n",
    "        for col_label in col_labels:\n",
    "            col_slice = path_slice[path_slice['col_label'] == col_label]\n",
    "            if len(col_slice) > 0:\n",
    "                if len(col_slice) == 1:\n",
    "                    complete = list(col_slice['complete'])[0]\n",
    "                    if not complete or force_reload:\n",
    "                        # add previous values\n",
    "                        col_slice = col_slice.to_dict(orient='list')\n",
    "                        col_slice['path'][0]\n",
    "                        row_dict = {\"dtype\":col_slice['dtype'][0], \"path\":col_slice['path'][0], \n",
    "                                    \"vector_len\":col_slice['vector_len'][0], \n",
    "                                    \"offset_step\":col_slice['offset_step'][0], \"total_nums\":0, # set to 0 to restart\n",
    "                                    \"initialised\":False, \"completed\":False}  \n",
    "                        meta_dict[col_label] = row_dict\n",
    "                else:\n",
    "                    print(display(col_slice))\n",
    "                    raise Exception(\"Multiple entries in path_df\")\n",
    "            else:\n",
    "                # add to path df\n",
    "                row_dict = self.add_path_df_entry(corpus_name, nested_dir, col_label,\n",
    "                                                 self.generate_new_map_path(col_label),\n",
    "                                                 topic_ids, return_row_dict=True)\n",
    "                row_dict['initialised'] = False\n",
    "                meta_dict[col_label] = row_dict\n",
    "        return meta_dict\n",
    "        \n",
    "    def load_memmap(self, corpus_name, nested_dir, topic_ids, col_label, batch_size=None, timesteps=None,\n",
    "                   return_input_shape=False):\n",
    "        path_slice = self.slice_path_df(corpus_name, nested_dir, topic_ids)\n",
    "        col_slice = path_slice[path_slice['col_label'] == col_label]\n",
    "        if len(col_slice) == 1:\n",
    "            col_dict = col_slice.to_dict(orient='list')\n",
    "            dtype = col_dict['dtype'][0]\n",
    "            vector_len = int(col_dict['vector_len'][0])\n",
    "            total_nums = int(col_dict['total_nums'][0])\n",
    "            path = col_dict['path'][0]\n",
    "            \n",
    "            # get shape of data\n",
    "            shape = [vector_len]  # base shape\n",
    "            input_shape = [vector_len]  # exclude batching info / num_items\n",
    "            num_items = int(total_nums / vector_len)  # num vectors\n",
    "            if batch_size is not None:\n",
    "                if timesteps is not None:  # timesteps only available when batches selected\n",
    "                    timesteps_dims = [timesteps]\n",
    "                    shape.extend(timesteps_dims)\n",
    "                    input_shape.extend(timesteps_dims)\n",
    "                    num_items = math.floor(num_items / timesteps)  # num timesteps\n",
    "                \n",
    "                shape.append(batch_size)\n",
    "                input_shape.append(batch_size)\n",
    "                num_items = math.floor(num_items / batch_size)  # num batches\n",
    "                \n",
    "            shape.append(num_items)  # append last (either num batches or num_vectors)\n",
    "            total_nums_from_shape = np.product(shape)\n",
    "            print(\"total_nums / from shape: \" + str(total_nums) + \" / \" + str(total_nums_from_shape))\n",
    "            shape.reverse()  # correct order for shape\n",
    "            input_shape.reverse()\n",
    "            input_shape = tuple(input_shape)\n",
    "            shape = tuple(shape)\n",
    "            \n",
    "            memmap = np.memmap(path, dtype=dtype, mode='r', shape=shape, order=self.order)\n",
    "            \n",
    "            if return_input_shape:\n",
    "                return memmap, input_shape\n",
    "            return memmap\n",
    "        else:\n",
    "            print(display(path_slice))\n",
    "            raise Exception(str(len(path_slice)) + \" entries for \")\n",
    "    \n",
    "    def slice_path_df(self, corpus_name, nested_dir, topic_ids):\n",
    "        topic_id_str = topic_ids\n",
    "        if type(topic_id_str) != str:\n",
    "            topic_id_str = self.topic_ids_str(topic_ids)\n",
    "            \n",
    "        mask = (self.path_df['corpus_name'] == corpus_name) & (self.path_df['nested_dir'] == nested_dir) & (self.path_df['topic_ids'] == topic_id_str)\n",
    "        path_slice = self.path_df.loc[mask]\n",
    "        return path_slice\n",
    "        \n",
    "    def topic_ids_str(self, topic_ids):\n",
    "        if type(topic_ids) != str:\n",
    "            sort = sorted(topic_ids)\n",
    "            sort = [str(x) for x in sort]\n",
    "            string = \",\".join(sort)\n",
    "            return string\n",
    "        else:\n",
    "            raise Exception(str(topic_ids) + \" is already type str\")\n",
    "        \n",
    "    def save_path_df(self):\n",
    "        save_df_file_type(self.path_df, self.path_df_path, verbose=False)\n",
    "                \n",
    "    def load_path_df(self):\n",
    "        if os.path.exists(self.path_df_path):\n",
    "            path_df = read_df_file_type(self.path_df_path, verbose=True)\n",
    "        else:\n",
    "            path_df = pd.DataFrame(columns=self.path_df_cols)\n",
    "            print(\"memmap path df created from scratch\")\n",
    "        return path_df\n",
    "        \n",
    "    def incompleted_col_labels(self, path_slice, col_labels):\n",
    "        incompleted = []\n",
    "        for col_label in col_labels:\n",
    "            col_slice = path_slice[path_slice['col_label'] == col_label]\n",
    "            if len(col_slice) > 0:\n",
    "                if len(col_slice) == 1:\n",
    "                    complete = list(col_slice['complete'])[0]\n",
    "                    if not complete:\n",
    "                        incompleted.append(col_label)\n",
    "                else:\n",
    "                    print(display(col_slice))\n",
    "                    raise Exception(\"Multiple entries in path_df\")\n",
    "            else:\n",
    "                incompleted.append(col_label)\n",
    "        return incompleted\n",
    "            \n",
    "    def generate_new_map_path(self, col_label):\n",
    "        # putting topic_ids in filename too long, use count instead\n",
    "        count = len(self.path_df)\n",
    "        base = str(count) + \"_\" + str(col_label)\n",
    "        if not os.path.exists(self.sample_dir):\n",
    "            os.makedirs(self.sample_dir)\n",
    "        mappath = os.path.join(self.sample_dir, base + \".memmap\")\n",
    "        return mappath          \n",
    "            \n",
    "    def scrape_col_data(self, emb_path, col_labels):\n",
    "        # setup return variables\n",
    "        labels = {}\n",
    "        emb_df = load_embeddings(emb_path, verbose=False)\n",
    "        for col_label in col_labels:\n",
    "            if col_label not in emb_df.columns:\n",
    "                raise ValueError(\"Target label \" + str(col_label) + \" is not in file at \" + str(emb_path))\n",
    "            # collect label values from df\n",
    "            labs = np.array(list(emb_df[col_label]))\n",
    "            labels[col_label] = labs\n",
    "        return labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPathHandler:\n",
    "    def __init__(self, proj_dir='/nfs/proj-repo/AAARG-dissertation', base_dir_name=\"tuning_models\",\n",
    "                verbose=True):\n",
    "        self.proj_dir = proj_dir\n",
    "        self.base_dir_name = base_dir_name\n",
    "        self.model_dir_path = os.path.join(self.proj_dir, self.base_dir_name)\n",
    "        self.df_name = \"nn_path_df.hdf\"\n",
    "        self.df_path = os.path.join(self.model_dir_path, self.df_name)\n",
    "        self.df_cols = [\"corpus_name\", \"nested_dir\", \"X_col\", \"y_col\", \"tuner_dir\", \n",
    "                                \"tuner_name\", \"best_hyperparams\", \"batch_size\", \"best_model_path\",\n",
    "                               \"input_param_text_path\", \"redundancy_threshold\", \"layer_type\"]\n",
    "        self.verbose = verbose\n",
    "        self.df = self.load_df(verbose=verbose)\n",
    "        \n",
    "    def load_df(self, verbose=True):\n",
    "        if os.path.exists(self.df_path):\n",
    "            df = read_df_file_type(self.df_path, verbose=verbose)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=self.df_cols)\n",
    "            if verbose:\n",
    "                print(\"model path df created from scratch\")\n",
    "        return df\n",
    "    \n",
    "    def save_df(self, verbose=False):\n",
    "        save_df_file_type(self.df, self.df_path, verbose=verbose)\n",
    "    \n",
    "    def add_path(self, corpus_name, nested_dir, X_col, y_col, tuner_dir, tuner_name, batch_size, \n",
    "                 input_param_text_path, layer_type, verbose=True):\n",
    "        # check if exists in dataframe\n",
    "        df = self.df\n",
    "        mask = self.create_df_mask(corpus_name, nested_dir, X_col, y_col, batch_size, layer_type)\n",
    "        exist_slice = df.loc[mask]\n",
    "        if len(exist_slice) == 0:\n",
    "            print(\"Appending new row to model path df\")\n",
    "            row = {\"corpus_name\":corpus_name, \"nested_dir\":nested_dir, \"X_col\":X_col,\n",
    "                  \"y_col\":y_col, \"tuner_dir\":tuner_dir, \"tuner_name\":tuner_name,\n",
    "                  \"batch_size\":batch_size,\n",
    "                  \"input_param_text_path\":input_param_text_path, \n",
    "                  \"layer_type\":layer_type.upper()}\n",
    "            self.df = self.df.append(row, ignore_index=True)\n",
    "        # save new entry\n",
    "        self.save_df(verbose=verbose)\n",
    "        \n",
    "    def generate_nn_save_path(self, corpus_name, nested_dir, X_col, y_col, batch_size, layer_type, \n",
    "                              create_dir=True):\n",
    "        col_dir = str(layer_type) + \"_\" + str(X_col) + \"_\" + str(y_col) + \"_\" + str(int(batch_size))\n",
    "        dir_list = [self.model_dir_path, corpus_name, nested_dir, col_dir]\n",
    "        # combine directories to form path of subdirectories, create dirs if necessary\n",
    "        dir_path = None\n",
    "        for cur_dir in dir_list:\n",
    "            if dir_path is None:  # first iteration\n",
    "                dir_path = dir_list[0]\n",
    "            else:\n",
    "                dir_path = os.path.join(dir_path, cur_dir)\n",
    "            if not os.path.exists(dir_path) and create_dir:\n",
    "                os.makedirs(dir_path)\n",
    "        # generate name\n",
    "        save_name = \"tuner_proj\"\n",
    "        return dir_path, save_name\n",
    "        \n",
    "    def update_tuner_instance_col(self, update_col, update_val, corpus_name, nested_dir, X_col, y_col, \n",
    "                                  batch_size, layer_type, verbose=True):\n",
    "        \"\"\"Update a column value on a row/tuner instance in the path df\"\"\"\n",
    "        mask = self.create_df_mask(corpus_name, nested_dir, X_col, y_col, batch_size, layer_type)\n",
    "#         target_row = self.df.loc[mask]\n",
    "#         target_row[update_col] = update_val\n",
    "        target_row_idx = self.df.index[mask].tolist()\n",
    "        if len(target_row_idx) == 1:\n",
    "            self.df.iat[int(target_row_idx[0]), self.df.columns.get_loc(update_col)] = update_val\n",
    "        else:\n",
    "            raise Exception(\"There are \" + str(len(target_row_idx)) + \" entries in model path df with given values\")\n",
    "        self.save_df(verbose=verbose)\n",
    "        \n",
    "    def create_df_mask(self, corpus_name, nested_dir, X_col, y_col, batch_size, layer_type):\n",
    "        layer_type = layer_type.upper()\n",
    "        mask = (self.df['corpus_name']==corpus_name)&(self.df['nested_dir']==nested_dir)&(self.df['X_col']==X_col)&(self.df['y_col']==y_col)&(self.df['batch_size']==batch_size)&(self.df['layer_type']==layer_type)\n",
    "        return mask\n",
    "    \n",
    "    def get_tuner_instance(self, corpus_name, nested_dir, X_col, y_col, batch_size, layer_type):\n",
    "        mask = self.create_df_mask(corpus_name, nested_dir, X_col, y_col, batch_size, layer_type)\n",
    "        target_row_idx = self.df.index[mask].tolist()\n",
    "        if len(target_row_idx) == 1:\n",
    "            tuner_instance = self.df.iloc[int(target_row_idx[0])]\n",
    "            return tuner_instance\n",
    "        else:\n",
    "            raise Exception(\"There are \" + str(len(target_row_idx)) + \" entries in model path df with given values\")\n",
    "        \n",
    "    def load_best_model(self, corpus_name, nested_dir, X_col, y_col, batch_size, layer_type):\n",
    "        mask = self.create_df_mask(corpus_name, nested_dir, X_col, y_col, batch_size)\n",
    "        tuner_instance = self.df.loc[mask]\n",
    "        best_model_path = list(tuner_instance['best_model_path'])[0]\n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "class NNTrainer:\n",
    "    def __init__(self, proj_dir='/nfs/proj-repo/AAARG-dissertation', nn_base_save_dir_name=None):\n",
    "        self.proj_dir = proj_dir\n",
    "        self.model_path_handler = ModelPathHandler(proj_dir=proj_dir)\n",
    "    \n",
    "    def train(self, corpus_name, nested_dir, topic_ids, X_col, y_col, layer_type, tuning_iterations=5, max_epochs=15,\n",
    "              reduction_factor=3, batch_size=32, force_reload=False, verbose=True):\n",
    "        \n",
    "        if verbose:\n",
    "            if force_reload:\n",
    "                print(\"FORCE RELOAD IS SET TO TRUE\")\n",
    "            print(display(self.model_path_handler.df))\n",
    "            print(\"\")\n",
    "        \n",
    "        # generate data\n",
    "        mmap_gen = MemmapGenerator(self.proj_dir)\n",
    "        mmap_gen.create_maps(corpus_name, nested_dir, [X_col, y_col], topic_ids, verbose=verbose,\n",
    "                                         force_reload=False)  # setting to False for debug\n",
    "        \n",
    "        \n",
    "        timesteps = None\n",
    "        if layer_type == \"LSTM\":\n",
    "            timesteps = 64\n",
    "\n",
    "        # generate readable X input that can be loaded from disk\n",
    "        X_map, input_shape = mmap_gen.load_memmap(corpus_name, nested_dir, topic_ids, X_col, \n",
    "                                                  batch_size=batch_size, timesteps=timesteps,\n",
    "                                                  return_input_shape=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # generate readable y output that can be loaded from disk\n",
    "        y_map = mmap_gen.load_memmap(corpus_name, nested_dir, topic_ids, y_col, batch_size=batch_size,\n",
    "                                           return_input_shape=False)\n",
    "        \n",
    "\n",
    "        # create a generator to feed NN samples/batches\n",
    "        num_batches = X_map.shape[0]\n",
    "        print(\"X_map shape:\" + str(X_map.shape))\n",
    "        print(\"input_shape: \" + str(input_shape))\n",
    "        batch_generator = BatchGenerator(X_map, y_map, batch_size, num_batches)\n",
    "\n",
    "        # create paths to save NN tuning files to\n",
    "        tuner_dir, tuner_name = self.model_path_handler.generate_nn_save_path(corpus_name, nested_dir, X_col, \n",
    "                                    y_col, batch_size, layer_type, create_dir=True)\n",
    "        \n",
    "        \n",
    "        # Log params in text file\n",
    "        trials_in_iter = max_epochs * (math.log(max_epochs, reduction_factor) ** 2)\n",
    "        cur_time = self.get_cur_time()\n",
    "        param_str = \"corpus_name: \" + str(corpus_name) + \"\\n\"\n",
    "        param_str += \"nested_dir: \" + str(nested_dir) + \"\\n\"\n",
    "        param_str += \"X_input: \" + str(X_col) + \"\\n\"\n",
    "        param_str += \"y_labels: \" + str(y_col) + \"\\n\"\n",
    "        param_str += \"batch_size: \" + str(batch_size) + \"\\n\"\n",
    "        param_str += \"train_topics: \" + str(train_topics) + \"\\n\"\n",
    "        param_str += \"max_epochs: \" + str(max_epochs) + \"\\n\"\n",
    "        param_str += \"tuning_iterations: \" + str(tuning_iterations) + \"\\n\"\n",
    "        param_str += \"reduction_factor: \" + str(reduction_factor) + \"\\n\"\n",
    "        param_str += \"estimated trials per iteration: \" + str(trials_in_iter) + \"\\n\"\n",
    "        param_str += \"total estimated trials: \" + str(trials_in_iter * tuning_iterations) + \"\\n\"\n",
    "        param_str += \"Started at: \" + str(cur_time) + \"\\n\"\n",
    "        input_param_text_path = os.path.join(tuner_dir, \"parameter_details.txt\")  # save with NN\n",
    "        if not os.path.exists(input_param_text_path) or force_reload:\n",
    "#         if not os.path.exists(input_param_text_path) or True:\n",
    "            with open(input_param_text_path, \"w\") as param_file:\n",
    "                param_file.write(param_str)\n",
    "        print(param_str)\n",
    "        \n",
    "        # add new nn_path_df entry\n",
    "        self.model_path_handler.add_path(corpus_name, nested_dir, X_col, y_col, tuner_dir, \n",
    "                                         tuner_name, batch_size, input_param_text_path, \n",
    "                                         layer_type, verbose=verbose)\n",
    "        \n",
    "        # generate optimised neural network\n",
    "        tuner = NNTuner(tuner_dir, tuner_name, input_shape, tuning_iterations=tuning_iterations, \n",
    "                        max_epochs=max_epochs, reduction_factor=reduction_factor, force_reload=force_reload, \n",
    "                        batch_size=batch_size, layer_type=layer_type)\n",
    "        \n",
    "#         # debug get model\n",
    "#         debug_model = tuner.tuner.get_best_models(num_models=1)[0]\n",
    "#         debug_path = '/nfs/proj-repo/debug_model'\n",
    "#         if not os.path.exists(debug_path):\n",
    "#             os.makedirs(debug_path)\n",
    "#         debug_model.save(debug_path)\n",
    "#         raise Exception(\"Saved debug model\")\n",
    "\n",
    "        best_model_dir = os.path.join(tuner_dir, \"best_models\")\n",
    "        best_model, best_hyperparams, best_model_path = tuner.search(batch_generator, best_model_dir)\n",
    "        \n",
    "        best_hyperparams = best_hyperparams.values  # convert to dict form\n",
    "        print(\"best_hyperparams: \" + str(best_hyperparams))\n",
    "        \n",
    "        # update best_hyperparams and best_model_path results in path df\n",
    "        self.model_path_handler.update_tuner_instance_col(\"best_hyperparams\", best_hyperparams, corpus_name, nested_dir, X_col, \n",
    "                                       y_col, batch_size, layer_type, verbose=True)\n",
    "        \n",
    "        self.model_path_handler.update_tuner_instance_col(\"best_model_path\", best_model_path, corpus_name, nested_dir, X_col, \n",
    "                                       y_col, batch_size, layer_type, verbose=True)\n",
    "        \n",
    "        \n",
    "        # add time stamp to param file when finished\n",
    "        cur_time = self.get_cur_time()\n",
    "        end_msg = \"Ended at: \" + str(cur_time) + \"\\n\"\n",
    "        with open(input_param_text_path, \"a\") as param_file:\n",
    "            param_file.write(end_msg)\n",
    "            \n",
    "        if verbose:\n",
    "            print(display(self.model_path_handler.df))\n",
    "        \n",
    "        print(\"Finished tuning neural network\")\n",
    "    \n",
    "    def get_cur_time(self):\n",
    "        cur_time = strftime(\"%a, %d %b %Y %H:%M:%S +0000\", gmtime())\n",
    "        return cur_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "# train_topics = np.arange(11, 47).tolist()  # 11 - 46\n",
    "# # nested_dirs = [\"stsb-roberta-base\", \"distilbert-base-nli-stsb-mean-tokens\"]\n",
    "# nested_dirs = [\"distilbert-base-nli-stsb-mean-tokens\"]\n",
    "# X_col = \"embedding\"\n",
    "# y_cols = [\"cos_sim_nearest_nug\", \"cosine_similarity\"]\n",
    "\n",
    "# # create arg list of tuples for multiprocessing\n",
    "# arg_list = []\n",
    "# for nested_dir in nested_dirs:\n",
    "#     base = [corpus_name, nested_dir]\n",
    "#     for y_col in y_cols:\n",
    "#         add_list = [[X_col, y_col], train_topics, True, True]\n",
    "#         args = base + add_list\n",
    "#         args = tuple(args)\n",
    "#         arg_list.append(args)\n",
    "    \n",
    "# print(\"arg_list: \" + str(arg_list))\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# mmap_gen = MemmapGenerator()\n",
    "\n",
    "# # corpus_name, nested_dir, col_labels, topic_ids, verbose=True, force_reload=Fals\n",
    "\n",
    "# pool = Pool(len(arg_list))\n",
    "# pool.starmap(mmap_gen.create_maps, arg_list)\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "# print(\"memmap creation complete\")\n",
    "\n",
    "\n",
    "# mmap_gen.create_maps(corpus_name, nested_dir, [X_col, y_col], topic_ids, verbose=verbose,\n",
    "#                                  force_reload=False)  # setting to False for debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tuning Parameters\n",
    "\n",
    "train_topics = np.arange(11, 47).tolist()  # 11 - 46\n",
    "tuning_iterations = 1\n",
    "max_epochs = 10\n",
    "batch_size = 1024\n",
    "# batch_size = 32\n",
    "reduction_factor = 8\n",
    "layer_type = \"DENSE\"\n",
    "\n",
    "force_reload = False\n",
    "\n",
    "X_col = \"embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "nested_dir = \"stsb-roberta-base\"\n",
    "y_col = \"cos_sim_nearest_nug\"\n",
    "# trainer = NNTrainer()\n",
    "# trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, layer_type, tuning_iterations=tuning_iterations,\n",
    "#               max_epochs=max_epochs, batch_size=batch_size, force_reload=force_reload, verbose=True,\n",
    "#               reduction_factor=reduction_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "# nested_dir = \"stsb-roberta-base\"\n",
    "# y_col = \"cosine_similarity\"\n",
    "# layer_type = \"DENSE\"\n",
    "# trainer = NNTrainer()\n",
    "# trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, layer_type, tuning_iterations=tuning_iterations,\n",
    "#               max_epochs=max_epochs, batch_size=batch_size, force_reload=force_reload, verbose=True,\n",
    "#               reduction_factor=reduction_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "# nested_dir = \"distilbert-base-nli-stsb-mean-tokens\"\n",
    "# y_col = \"cos_sim_nearest_nug\"\n",
    "# trainer = NNTrainer()\n",
    "# trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, layer_type, tuning_iterations=tuning_iterations,\n",
    "#               max_epochs=max_epochs, batch_size=batch_size, force_reload=force_reload, verbose=True,\n",
    "#               reduction_factor=reduction_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "# nested_dir = \"distilbert-base-nli-stsb-mean-tokens\"\n",
    "# y_col = \"cosine_similarity\"\n",
    "# trainer = NNTrainer()\n",
    "# trainer.train(corpus_name, nested_dir, train_topics, X_col, y_col, layer_type, tuning_iterations=tuning_iterations,\n",
    "#               max_epochs=max_epochs, batch_size=batch_size, force_reload=force_reload, verbose=True,\n",
    "#               reduction_factor=reduction_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Similarity Threshold for Redundant Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .defs.corpus_loader import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_predict_batches(emb_list, num_dims, batch_size):\n",
    "    \"\"\"Creates batching of target embedding list, with empty values to ensure all batches full\"\"\"\n",
    "    # add empty rows to complete batch_size\n",
    "    num_empty_rows = batch_size - (len(emb_list) % batch_size)\n",
    "    if num_empty_rows > 0:\n",
    "        empty_rows = [np.zeros(num_dims)] * num_empty_rows\n",
    "        emb_list.extend(empty_rows)\n",
    "    # shape into batches\n",
    "    emb_arr = np.asarray(emb_list)\n",
    "#     num_batches = int(len(emb_arr) / batch_size)\n",
    "#     print(\"num batches: \" + str(num_batches))\n",
    "#     emb_arr = emb_arr.reshape(num_batches, batch_size, num_dims)\n",
    "#     print(\"emb_arr.shape: \" + str(emb_arr.shape))\n",
    "    return emb_arr, num_empty_rows\n",
    "\n",
    "def predict_emb_list(model, emb_list, batch_size=256):\n",
    "    \"\"\"Predict scores of embedding list on trained model\"\"\"\n",
    "    if batch_size == 1024:\n",
    "        batch_size = 256  # special case, model reduces batch size to 256 in passes\n",
    "    \n",
    "    # create batches, where does not divide evenly, fill with empty rows\n",
    "    emb_batches, num_empty_rows = form_predict_batches(emb_list, len(emb_list[0]), batch_size)\n",
    "    preds = model.predict(emb_batches, \n",
    "                          batch_size=batch_size,\n",
    "                          verbose=1, \n",
    "                          use_multiprocessing=True, \n",
    "                          workers=num_threads)\n",
    "\n",
    "    # format predictions to add to df\n",
    "    preds = preds.flatten()  # undo batch shape\n",
    "    preds = preds[0:len(preds)-num_empty_rows]  # remove empty row predictions\n",
    "    return preds\n",
    "\n",
    "def gen_train_sent_scores_df_path(tuner_dir, k, top=True):\n",
    "    \"\"\"Generate a path for the top/bottom k predicted sentence scores of the training set\"\"\"\n",
    "    top_bottom_str = None\n",
    "    if top:\n",
    "        top_bottom_str = \"top\"\n",
    "    else:\n",
    "        top_bottom_str = \"bottom\"\n",
    "    fn = \"train_set_pred_\" + top_bottom_str + \"_\" + str(k) + \".hdf\"\n",
    "    path = os.path.join(tuner_dir, fn)\n",
    "    return path\n",
    "\n",
    "def gen_shared_dif_nug_paths(tuner_dir):\n",
    "    shared_fn = \"share_nug_sims.pickle\"\n",
    "#     dif_fn = \"dif_nug_sims.pickle\"\n",
    "    shared_path = os.path.join(tuner_dir, shared_fn)\n",
    "#     dif_path = os.path.join(tuner_dir, dif_fn)\n",
    "#     return shared_path, dif_path\n",
    "    return shared_path\n",
    "\n",
    "def gen_plot_path(tuner_dir, plot_name, file_type=\".png\"):\n",
    "    fn = str(plot_name) + \".png\"\n",
    "    path = os.path.join(tuner_dir, fn)\n",
    "    return path\n",
    "\n",
    "def find_nearest_nug_emb_idx(emb, nug_embs):\n",
    "     # for each sample_emb, find index of nearest nug\n",
    "    max_idx = None\n",
    "    max_sim = float('-inf')\n",
    "    for nug_index, nug_emb in enumerate(nug_embs):\n",
    "        cur_sim = cosine_similarity(emb, nug_emb)\n",
    "        if cur_sim > max_sim:\n",
    "            max_sim = cur_sim\n",
    "            max_idx = nug_index\n",
    "    return max_idx\n",
    "\n",
    "def streamid_to_docid(streamid):\n",
    "    docid = streamid.split(\"-\")\n",
    "    docid = docid[-1]\n",
    "    return docid\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "def enforce_emb_df_constraints(emb_df, existing_sents=None, existing_docids=None, min_tokens=None, nlp=None,\n",
    "                              remove_duplicate_docid=True, remove_duplicate_sents=True):\n",
    "    # remove duplicate docids\n",
    "    if remove_duplicate_docid:\n",
    "        print(\"init emb_df len: \" + str(len(emb_df)))\n",
    "        docid_groups = emb_df.groupby(by=\"docid\")\n",
    "        keep_streamids = []\n",
    "        for docid, group in docid_groups:\n",
    "            streamids = group['streamid'].unique()\n",
    "            keep_streamids.append(streamids[-1])\n",
    "        emb_df = emb_df[emb_df['streamid'].isin(keep_streamids)]\n",
    "        \n",
    "    # remove already scored docids\n",
    "    if existing_docids is not None:\n",
    "        emb_df = emb_df[~emb_df['docid'].isin(existing_docids)]\n",
    "\n",
    "    # remove duplicate sentences in emb_df\n",
    "    if remove_duplicate_sents:\n",
    "        print(\"len before drop duplicate sents: \" + str(len(emb_df)))\n",
    "        emb_df = emb_df.drop_duplicates(subset='sentence')\n",
    "        print(\"len after drop duplicate sents: \" + str(len(emb_df)))\n",
    "\n",
    "    # remove already scored sentences in emb_df\n",
    "    if existing_sents is not None:\n",
    "        print(\"len before drop scored sents: \" + str(len(emb_df)))\n",
    "        emb_df = emb_df[~emb_df['sentence'].isin(existing_sents)]\n",
    "        print(\"len after drop scored sents: \" + str(len(emb_df)))\n",
    "    \n",
    "    emb_df = emb_df.reset_index()  # reset index prior to min_tokens\n",
    "    \n",
    "    # remove sentences which are smaller than min_tokens\n",
    "    if min_tokens is not None:\n",
    "        if nlp is None:  # load nlp if not already\n",
    "            spacy_model = \"en_core_web_sm\"\n",
    "            nlp = spacy.load(spacy_model)\n",
    "            print(\"loaded \" + str(spacy_model))\n",
    "            \n",
    "        \n",
    "        # get indexes of sentences that do not meet minimum tokens\n",
    "        remove_idxs = []\n",
    "        for index, row in emb_df.iterrows():\n",
    "            sent = row['sentence']\n",
    "            spacy_tokens = nlp(sent)\n",
    "            if len(spacy_tokens) < min_tokens:\n",
    "                remove_idxs.append(index)\n",
    "        \n",
    "        # drop gathered indexes\n",
    "        emb_df = emb_df.drop(emb_df.index[remove_idxs])\n",
    "        emb_df = emb_df.reset_index()  # reset again new indexes\n",
    "    return emb_df\n",
    "\n",
    "def cosine_similarity_from_idxs(x_idx, y_idx, sample_embs):\n",
    "    \"\"\"Wrapper to get cosine_similarity given two idxs in sample_embs\n",
    "    \n",
    "    Must be at top-level (outside class) to be pickled for multiprocessing\n",
    "    \"\"\"\n",
    "    x_emb = sample_embs[x_idx]\n",
    "    y_emb = sample_embs[y_idx]\n",
    "    cos_sim = cosine_similarity(x_emb, y_emb)\n",
    "    return cos_sim\n",
    "\n",
    "def run_starmap(func, arg_list):\n",
    "    \"\"\"This method was created to fix a bug re-instantiating Pool() object in same method\"\"\"\n",
    "     # create multiprocessing object\n",
    "    pool = Pool()\n",
    "    results = pool.starmap(func, arg_list)\n",
    "    # ensure multiprocessing objects are closed\n",
    "    pool.close()  \n",
    "    pool.join()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell loaded\n"
     ]
    }
   ],
   "source": [
    "class RedundancyThresholdGenerator:\n",
    "    def __init__(self, proj_repo='/nfs/proj-repo/AAARG-dissertation'):\n",
    "        self.path_ret = PathRetriever(proj_repo)\n",
    "        self.model_path_handler = ModelPathHandler(proj_repo)\n",
    "        \n",
    "    def generate_threshold(self, corpus_name, nested_dir, X_col, y_col, batch_size, layer_type,\n",
    "                          train_topic_ids, k=100000, verbose=True, force_reload=False, save=True,\n",
    "                          min_tokens=None, tolerable_ratio=0.05):\n",
    "        # get target emb_paths and emb_dir\n",
    "        if verbose:\n",
    "            print(\"Gathering embedding paths\")\n",
    "        emb_paths, emb_dir = self.get_emb_paths(corpus_name, nested_dir, train_topic_ids, verbose=False)\n",
    "        \n",
    "        # get model entry on path df\n",
    "        model_entry = self.model_path_handler.get_tuner_instance(corpus_name, nested_dir, X_col, y_col, \n",
    "                                                                 batch_size, layer_type)\n",
    "        # get tuner dir to save files\n",
    "        tuner_dir = str(model_entry['tuner_dir'])\n",
    "        \n",
    "        # model to make predictions\n",
    "        model_path = str(model_entry['best_model_path'])\n",
    "        if verbose:\n",
    "            print(\"Loading prediction model from \" + str(model_path))\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        # get top scores on train set\n",
    "        if verbose:\n",
    "            print(\"Gathering top sentence scores\")\n",
    "        score_df = self.get_top_train_set_sentence_scores(emb_paths, tuner_dir, model, k=k, batch_size=batch_size, \n",
    "                                                 save=save, verbose=verbose, force_reload=force_reload,\n",
    "                                                         min_tokens=min_tokens)\n",
    "        \n",
    "        # extract embeddings from top scores\n",
    "        sample_embs = list(score_df['embedding'])\n",
    "        sample_embs = np.asarray(sample_embs)\n",
    "        \n",
    "        \n",
    "        # debug free up memory\n",
    "        del score_df\n",
    "        print(\"score_df removed from mem\")\n",
    "        \n",
    "        # compare similarity of nuggets (shared nearest nug, not same nearest nug)\n",
    "        if verbose:\n",
    "            print(\"Gathering similarities between embeddings\")\n",
    "        share_nug_sims = self.compare_sim_same_near_nug(sample_embs, emb_dir, tuner_dir=tuner_dir, \n",
    "                                                                verbose=verbose, force_reload=force_reload,\n",
    "                                                                save=save)\n",
    "        # plot onto graph\n",
    "        if verbose:\n",
    "            print(\"Forming a plot\")\n",
    "        self.plot_sim_nugget_distributions(share_nug_sims, save=save, tuner_dir=tuner_dir,\n",
    "                                          verbose=verbose)\n",
    "        \n",
    "        # get threshold\n",
    "        redundancy_threshold = self.get_redundancy_threshold(share_nug_sims, tolerable_ratio=tolerable_ratio)\n",
    "        \n",
    "        # save threshold\n",
    "        self.model_path_handler.update_tuner_instance_col(\"redundancy_threshold\", redundancy_threshold, \n",
    "                                        corpus_name, nested_dir, X_col, y_col, batch_size, layer_type, \n",
    "                                        verbose=verbose)\n",
    "        if verbose:\n",
    "            print(\"Redundancy threshold saved as \" + str(redundancy_threshold))\n",
    "        \n",
    "    def get_redundancy_threshold(self, share_nug_sims, tolerable_ratio=0.05):\n",
    "        # get value where allow tolerable_ratio of values in same_nug_sims\n",
    "        tolerable_idx = math.floor(len(share_nug_sims) * tolerable_ratio)\n",
    "        redundancy_threshold = share_nug_sims[tolerable_idx]\n",
    "        return redundancy_threshold\n",
    "        \n",
    "        \n",
    "    def get_emb_paths(self, corpus_name, nested_dir, topic_ids, verbose=False):\n",
    "        emb_paths, emb_dir = self.path_ret.get_embedding_paths(corpus_name, nested_dir, topic_ids=topic_ids, \n",
    "                                                      return_dir_path=True, verbose=verbose)\n",
    "        \n",
    "        emb_paths = list(emb_paths['path'])\n",
    "        return emb_paths, emb_dir\n",
    "    \n",
    "    def get_top_train_set_sentence_scores(self, train_emb_paths, tuner_dir, model, k=100000, batch_size=256, \n",
    "                                          save=True, verbose=True, force_reload=False, min_tokens=None):\n",
    "        \n",
    "        # generate save path to find and/or save file\n",
    "        save_path = gen_train_sent_scores_df_path(tuner_dir, k)\n",
    "        \n",
    "        if not force_reload:\n",
    "            if os.path.exists(save_path):  # return already generated file\n",
    "                score_df = read_df_file_type(save_path, verbose=verbose)\n",
    "                if verbose:\n",
    "                    print(\"Loaded top train scores from file\")\n",
    "                return score_df\n",
    "        \n",
    "        # create empty df with default vals\n",
    "        df_empty_dict = {}\n",
    "        df_empty_dict[\"sentence\"] = [\"empty\"] * k\n",
    "        df_empty_dict[\"score\"] = [np.NINF] * k\n",
    "        df_empty_dict[\"embedding\"] = [np.nan] * k\n",
    "        df_empty_dict[\"docid\"] = [\"empty\"] * k\n",
    "        score_df = pd.DataFrame(df_empty_dict)\n",
    "        \n",
    "        nlp = None\n",
    "        \n",
    "        # find top k scoring sentences from emb_paths\n",
    "        for emb_path in tqdm_notebook(train_emb_paths):  # reverse to get article with latest docid\n",
    "            # load emb_df\n",
    "            emb_df = load_embeddings(emb_path, verbose=False)\n",
    "            \n",
    "            # add docid column\n",
    "            emb_df['docid'] = [streamid_to_docid(x) for x in emb_df['streamid']]\n",
    "            \n",
    "            # enforce constraints\n",
    "            emb_df = enforce_emb_df_constraints(emb_df, existing_sents=score_df['sentence'], \n",
    "                                                existing_docids=score_df['docid'],\n",
    "                                                min_tokens=min_tokens, nlp=nlp)\n",
    "            \n",
    "            # scrape embeddings\n",
    "            embs = list(emb_df['embedding'])\n",
    "            \n",
    "            # predict embeddings\n",
    "            if verbose:\n",
    "                print(\"Predicting emb_df's embeddings\")\n",
    "            preds = predict_emb_list(model, embs, batch_size=batch_size)\n",
    "#             print(\"predictions 0-20\")\n",
    "#             print(preds[0:20])\n",
    "            \n",
    "            # tie sentence to predicted score\n",
    "            if verbose:\n",
    "                print(\"Creating prediction dataframe\")\n",
    "            pred_dict = {\"sentence\":emb_df['sentence'], \"score\":preds, \"embedding\":emb_df['embedding'],\n",
    "                        \"docid\":emb_df[\"docid\"]}\n",
    "            pred_df = pd.DataFrame(pred_dict)\n",
    "            pred_df = pred_df.sort_values(by=\"score\", ascending=False, ignore_index=True) # descending order\n",
    "            \n",
    "            # add scores where values within top k\n",
    "            bottom_score = score_df['score'][0]\n",
    "            pred_df_highest = pred_df['score'][0]\n",
    "            if verbose:\n",
    "                print(\"bottom_score/pred_df_highest: \" + str(bottom_score) + \" / \" + str(pred_df_highest))\n",
    "            if pred_df_highest > bottom_score:\n",
    "                \n",
    "                # find how many scores to add\n",
    "                if verbose:\n",
    "                    print(\"Adding new rows\")\n",
    "                num_rows_add = 0\n",
    "                for index, row in pred_df.iterrows():\n",
    "                    row_score = row['score']\n",
    "                    if row_score > bottom_score:\n",
    "                        num_rows_add += 1\n",
    "                    else:\n",
    "                        break  # found all rows that are within current top k\n",
    "                if num_rows_add > k:\n",
    "                    num_rows_add = k\n",
    "                \n",
    "                # add scores to score_df and sort\n",
    "                score_df.loc[range(num_rows_add), score_df.columns] = pred_df[0:num_rows_add]\n",
    "                score_df = score_df.sort_values(by=\"score\", ignore_index=True, ascending=True)\n",
    "                \n",
    "                if len(score_df) > k:\n",
    "                    raise Exception(\"Score df len \" + str(len(score_df)) + \" is greater than k \" + str(k))\n",
    "            else:\n",
    "                print(\"No new rows to be added\")\n",
    "            if verbose:\n",
    "                print(\"score_df\")\n",
    "                print(display(score_df))\n",
    "        \n",
    "        # save df in tuner_dir\n",
    "        if save:\n",
    "            save_df_file_type(score_df, save_path, verbose=verbose)\n",
    "        return score_df\n",
    "    \n",
    "    def compare_sim_same_near_nug(self, sample_embs, emb_dir, tuner_dir=None, verbose=True, force_reload=False,\n",
    "                                 save=True):\n",
    "        def read_pickle(obj_to_load, path, verbose=True):\n",
    "            with open(path, 'rb') as handle:\n",
    "                obj_to_load = pickle.load(handle)\n",
    "            if verbose:\n",
    "                print(\"object loaded from \" + str(path))\n",
    "            return obj_to_load\n",
    "        \n",
    "        def save_pickle(obj_to_save, path, verbose=True):\n",
    "            with open(path, 'wb') as handle:\n",
    "                pickle.dump(obj_to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            if verbose:\n",
    "                print(\"Saved to \" + str(path))\n",
    "        \n",
    "        # load values if already calculated and force reload set to False\n",
    "        share_nug_path = None\n",
    "        share_nug_sims= None\n",
    "        if tuner_dir is not None:\n",
    "            share_nug_path = gen_shared_dif_nug_paths(tuner_dir)\n",
    "            if not force_reload:\n",
    "                if os.path.exists(share_nug_path):\n",
    "                    share_nug_sims = read_pickle(share_nug_sims, share_nug_path)\n",
    "                    return share_nug_sims\n",
    "        \n",
    "        # load nugget embeddings\n",
    "        if verbose:\n",
    "            print(\"Gathering nugget embeddings\")\n",
    "        nug_embs_path = os.path.join(emb_dir, \"embedding_list_all.pickle\")\n",
    "        nug_embs = None\n",
    "        with open(nug_embs_path, 'rb') as handle:\n",
    "            nug_embs = pickle.load(handle)\n",
    "        \n",
    "        # convert to numpy array for greater efficiency in lookup\n",
    "        nug_embs = np.asarray(nug_embs)\n",
    "        \n",
    "        # create a list of argument tuples for starmap\n",
    "        nearest_nug_args = [(emb, nug_embs) for emb in sample_embs]\n",
    "        \n",
    "        \n",
    "        # for each emb in sample_embs, find the index of the nearest nug_emb (by cosine similarity)\n",
    "        if verbose:\n",
    "            print(\"Finding the nearest nugget embedding for \" + str(len(sample_embs)) + \" sample_embs \"\n",
    "                 + \" and \" + str(len(nug_embs)) + \" nug_embs\")\n",
    "            \n",
    "        # debug to fix kernel crashing issue, save this\n",
    "        nearest_nug_idxs_path = os.path.join(tuner_dir, \"nearest_nug_idxs_debug.pickle\")\n",
    "        nearest_nug_idxs = None\n",
    "        if os.path.exists(nearest_nug_idxs_path):\n",
    "            nearest_nug_idxs = read_pickle(nearest_nug_idxs, nearest_nug_idxs_path, verbose=verbose)\n",
    "        else:   \n",
    "            nearest_nug_idxs = run_starmap(find_nearest_nug_emb_idx, nearest_nug_args)\n",
    "            save_pickle(nearest_nug_idxs, nearest_nug_idxs_path, verbose=verbose)\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Completed finding nearest nugget for embeddings\")\n",
    "            \n",
    "        del nearest_nug_args\n",
    "        del nug_embs  # next processes don't require this in mem\n",
    "        \n",
    "                # create list of argument tuples for finding cos sim of those that share same nearest nug and don't\n",
    "        def arg_list_share_nearest_nug():\n",
    "            if verbose:\n",
    "                print(\"creating arg_list for nearest nug\")\n",
    "            share_nug_args = []\n",
    "            for x in range(len(nearest_nug_idxs) - 1):\n",
    "                for y in range(x + 1, len(nearest_nug_idxs)): # skip self-comparisons\n",
    "                    # check if x and y have same nearest nug\n",
    "                    x_nearest = nearest_nug_idxs[x]\n",
    "                    y_nearest = nearest_nug_idxs[y]\n",
    "                    \n",
    "                    # append tuple of embeddings to category\n",
    "                    if x_nearest == y_nearest:\n",
    "                        # get emb pairs\n",
    "                        x_emb = sample_embs[x]\n",
    "                        y_emb = sample_embs[y]\n",
    "                        emb_tuple = (x_emb, y_emb)\n",
    "                        share_nug_args.append(emb_tuple)\n",
    "            return share_nug_args\n",
    "\n",
    "        \n",
    "        # find cos_sims between embeddings that share nearest nug\n",
    "        share_nug_args = arg_list_share_nearest_nug()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Finding cosine similarity of \" + str(len(share_nug_args)) \n",
    "                  + \" embedding pairs that share same nearest nugget\")\n",
    "            \n",
    "        share_nug_sims = run_starmap(cosine_similarity, share_nug_args)\n",
    "        del share_nug_args\n",
    "        \n",
    "        # sort similarities\n",
    "        if verbose:\n",
    "            print(\"Sorting similarities\")\n",
    "        share_nug_sims = np.sort(np.asarray(share_nug_sims))\n",
    "        \n",
    "        # save values\n",
    "        if tuner_dir is not None:\n",
    "            if save:\n",
    "                save_pickle(share_nug_sims, share_nug_path, verbose=verbose)\n",
    "            else:\n",
    "                raise Exception(\"tuner_dir must be set to save files\")\n",
    "        \n",
    "        return share_nug_sims\n",
    "        \n",
    "        \n",
    "    def plot_sim_nugget_distributions(self, same_nug_sims, save=True, tuner_dir=None,\n",
    "                                     verbose=True):\n",
    "        # https://stackoverflow.com/a/24567715\n",
    "        fig = plt.gcf()  # might need to change so gets new figure?\n",
    "        \n",
    "        same_nug_sims = copy.deepcopy(same_nug_sims)\n",
    "#         dif_nug_sims = copy.deepcopy(dif_nug_sims)\n",
    "        \n",
    "        plt.step(np.concatenate([same_nug_sims, same_nug_sims[[-1]]]), \n",
    "                 np.linspace(0,1, num=same_nug_sims.size + 1))\n",
    "#         plt.step(np.concatenate([dif_nug_sims, dif_nug_sims[[-1]]]), \n",
    "#                  np.linspace(0,1, num=dif_nug_sims.size + 1),\n",
    "#                 label='different most similar nugget')\n",
    "        \n",
    "        plt.title(\"Distribution of cosine similarities of embedding pairs that share the same nearest nugget (cumulative)\")\n",
    "        plt.ylabel('Ratio of Total Count (' + str(len(same_nug_sims)) + ')')\n",
    "        plt.xlabel('Cosine Similarity')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        if save:\n",
    "            if tuner_dir is not None:\n",
    "                plot_save_path = gen_plot_path(tuner_dir, \"same_nearest_nug_distribution\", file_type=\".png\")\n",
    "                plt.savefig(plot_save_path)\n",
    "                if verbose:\n",
    "                    print(\"plot saved to \" + str(plot_save_path))\n",
    "            else:\n",
    "                raise Exception(\"tuner_dir must be set to save the plot\")\n",
    "        \n",
    "print(\"cell loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from .hdf file\n"
     ]
    }
   ],
   "source": [
    "train_topics = np.arange(11, 47).tolist()  # 11 - 46\n",
    "batch_size = 1024\n",
    "layer_type = \"DENSE\"\n",
    "corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "nested_dir = \"stsb-roberta-base\"\n",
    "X_col = \"embedding\"\n",
    "y_col = \"cos_sim_nearest_nug\"\n",
    "\n",
    "force_reload = False\n",
    "save = True\n",
    "verbose = True\n",
    "\n",
    "k = 100000\n",
    "min_tokens = None\n",
    "tolerable_ratio = 0.05\n",
    "\n",
    "thresh_gen = RedundancyThresholdGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering embedding paths\n",
      "Loading prediction model from /nfs/proj-repo/AAARG-dissertation/tuning_models/mine-trects-kba2014-filtered/stsb-roberta-base/DENSE_embedding_cos_sim_nearest_nug_1024/best_models/0\n",
      "Gathering top sentence scores\n",
      "loaded from .hdf file\n",
      "Loaded top train scores from file\n",
      "score_df removed from mem\n",
      "Gathering similarities between embeddings\n",
      "object loaded from /nfs/proj-repo/AAARG-dissertation/tuning_models/mine-trects-kba2014-filtered/stsb-roberta-base/embedding_cos_sim_nearest_nug_1024/share_nug_sims.pickle\n",
      "Forming a plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot saved to /nfs/proj-repo/AAARG-dissertation/tuning_models/mine-trects-kba2014-filtered/stsb-roberta-base/embedding_cos_sim_nearest_nug_1024/same_nearest_nug_distribution.png\n",
      "df saved as hdf complevel 9 at: /nfs/proj-repo/AAARG-dissertation/tuning_models/nn_path_df.hdf\n",
      "Redundancy threshold saved as 0.4854188715149244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:2449: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['corpus_name', 'nested_dir', 'X_col', 'y_col', 'tuner_dir',\n",
      "       'tuner_name', 'best_hyperparams', 'batch_size', 'best_model_path',\n",
      "       'input_param_text_path', 'layer_type'],\n",
      "      dtype='object')]\n",
      "\n",
      "  encoding=encoding,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresh_gen.generate_threshold(corpus_name, nested_dir, X_col, y_col, batch_size, layer_type,\n",
    "                          train_topics, k=k, verbose=verbose, force_reload=force_reload, save=save,\n",
    "                             min_tokens=min_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_debug_model():\n",
    "#     model_path = '/nfs/proj-repo/debug_model'\n",
    "#     model = tf.keras.models.load_model(model_path)\n",
    "#     print(\"loaded model\")\n",
    "#     return model, model_path\n",
    "\n",
    "# # model_path_handler = ModelPathHandler()\n",
    "# # print(display(model_path_handler.df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_name = \"mine-trects-kba2014-filtered\"\n",
    "# nested_dir = \"stsb-roberta-base\"\n",
    "# X_col = \"embedding\"\n",
    "# y_col = \"cos_sim_nearest_nug\"\n",
    "# batch_size = 1024\n",
    "\n",
    "# model, debug_tuner_dir = get_debug_model()\n",
    "\n",
    "# # model = model_path_handler.load_best_model(corpus_name, nested_dir, X_col, y_col, batch_size)\n",
    "# # tuner_inst_mask = model_path_handler.create_df_mask(corpus_name, nested_dir, X_col, y_col, batch_size)\n",
    "# # model_df = model_path_handler.df\n",
    "# # tuner_inst = model_df.loc[tuner_inst_mask]\n",
    "# # best_hyperparams = list(tuner_inst['best_hyperparams'])[0]\n",
    "# # print(\"best_hyperparams\")\n",
    "# # print(best_hyperparams)\n",
    "\n",
    "# # model_path = \"/nfs/proj-repo/AAARG-dissertation/tuning_models/mine-trects-kba2014-filtered/stsb-roberta-base/embedding_cos_sim_nearest_nug_1024/best_models/0\"\n",
    "# # model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_gen = RedundancyThresholdGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ids = np.arange(11, 15).tolist()\n",
    "# test_paths, test_emb_dir = sum_gen.get_emb_paths(corpus_name, nested_dir, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_df_test(emb_paths):\n",
    "#     for emb_path in tqdm_notebook(emb_paths):\n",
    "#         emb_df = load_embeddings(emb_path, verbose=False)\n",
    "#         cosnugs = list(emb_df['cos_sim_nearest_nug'])\n",
    "#         for cos in cosnugs:\n",
    "#             if cos > 0:\n",
    "#                 print(\"positive\")\n",
    "# #     print(display(emb_df[0:200]))\n",
    "    \n",
    "# # show_df_test(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 100000\n",
    "# batch_size = 256\n",
    "# save = True\n",
    "# force_reload = False\n",
    "# test_top_scores = sum_gen.get_top_train_set_sentence_scores(test_paths, debug_tuner_dir, model, k=k, \n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                  save=save, force_reload=force_reload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_gen.get_top_train_set_sentence_scores(test_paths, debug_tuner_dir, model, k=k, \n",
    "#                                                             batch_size=batch_size,\n",
    "#                                                  save=False, force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(display(test_top_scores[(len(test_top_scores) - 300):]))\n",
    "# print(display(test_top_scores[test_top_scores['sentence'].str.contains(\"Hostages from at least 10 nations in Algeria\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# emb_dir = \"/nfs/proj-repo/AAARG-dissertation/dataset/mine-trects-kba2014-filtered/embeddings/stsb-roberta-base\"\n",
    "# sample_embs = list(test_top_scores['embedding'][(len(test_top_scores) - 1000):])\n",
    "\n",
    "# # print(\"sample_embs: \" + str(sample_embs))\n",
    "\n",
    "# same_nug_sims, dif_nug_sims = sum_gen.compare_sim_nuggets(emb_dir, sample_embs)\n",
    "# print(\"cell complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_gen.plot_sim_nugget_distributions(same_nug_sims, dif_nug_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_score_df_path(tuner_dir, topic_id, X_col, corpus_name):\n",
    "    fn = str(topic_id) + \"_\" + str(X_col) + \"_\" + str(corpus_name)\n",
    "    fn += \".hdf\"\n",
    "    path = os.path.join(tuner_dir, fn)\n",
    "    return path\n",
    "\n",
    "def float_only_decimal_place(num, num_after_dp=3):\n",
    "    \"\"\"Transform a float to a string with only selected numbers after decimal place\n",
    "    \n",
    "    e.g. 0.23492 (float) -> 23 (str)\n",
    "    \"\"\"\n",
    "    num = str(num)\n",
    "    num = num.split(\".\")\n",
    "    num = num[1]\n",
    "    num = num[0:num_after_dp]\n",
    "    return num\n",
    "\n",
    "def gen_ranked_df_path(tuner_dir, rank_method, topic_id, X_col, corpus_name, min_tokens=None,\n",
    "                      redundancy_threshold=None):\n",
    "    fn = str(rank_method)\n",
    "    if redundancy_threshold is not None:\n",
    "        # get redundancy threshold (float) as string\n",
    "        redund_str = float_only_decimal_place(num)\n",
    "        fn += \"_\" + str(redund_str)\n",
    "    if min_tokens is not None:\n",
    "        fn += \"_\" + str(min_tokens)\n",
    "    fn += \"_\" + str(topic_id) + \"_\" + str(X_col) + \"_\" + str(corpus_name)\n",
    "    fn += \".hdf\"\n",
    "    path = os.path.join(tuner_dir, fn)\n",
    "    return path\n",
    "\n",
    "def gen_summary_path_from_ranked_df_path(ranked_df_path):\n",
    "    # remove .hdf extension\n",
    "    summary_path = str(ranked_df_path)\n",
    "    if summary_path[-4] == \".hdf\":\n",
    "        str_path = summary_path[0:len(summary_path) - 4]\n",
    "        str_path += \".txt\"\n",
    "        return str_path\n",
    "    else:\n",
    "        raise Exception(\"Path does not end in .hdf as expected: \" + str(ranked_df_path))\n",
    "\n",
    "def gen_varname_path(tuner_dir, varname, rank_method, topic_id, X_col, corpus_name):\n",
    "    fn = \"ranked_idxs\" + \"_\" + str(rank_method) + \"_\" + str(topic_id) + \"_\" + str(X_col) + str(corpus_name)\n",
    "    fn += \".pickle\"\n",
    "    path = os.path.join(tuner_dir, fn)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryGenerator:\n",
    "    def __init__(self, model_path_handler, proj_repo='/nfs/proj-repo/AAARG-dissertation'):\n",
    "        self.model_path_handler = self.model_path_handler\n",
    "        self.path_ret = PathRetriever(proj_repo)\n",
    "        self.proj_repo = proj_repo\n",
    "        self.rank_methods = ['k', 'k_non_redund']\n",
    "        self.ordering_methods = [\"score\", \"sent_position\", None]\n",
    "        \n",
    "    def generate_summary(self, corpus_name, nested_dir, X_col, y_col, batch_size, layer_type, test_topic_ids,\n",
    "                        k=10000, verbose=True, force_reload=False, save=True, remove_emb_df_redundancy=True,\n",
    "                        rank_method=\"k_non_redund\", min_tokens=3, ordering=\"sent_position\"):\n",
    "        \n",
    "        # get model entry on path df\n",
    "        model_entry = self.model_path_handler.get_tuner_instance(corpus_name, nested_dir, X_col, y_col, \n",
    "                                                                 batch_size, layer_type)\n",
    "        # get tuner dir to save files\n",
    "        tuner_dir = str(model_entry['tuner_dir'])\n",
    "        \n",
    "        # model to make predictions\n",
    "        model_path = str(model_entry['best_model_path'])\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        # execute for each topic in test set\n",
    "        for topic_id in tqdm_notebook(test_topic_ids):\n",
    "            if topic_id == 7:  # special empty case\n",
    "                continue\n",
    "        \n",
    "            # create a df with a prediction score for each sentence in topic\n",
    "            score_df = self.generate_topic_score_df(tuner_dir, corpus_name, nested_dir, topic_id, X_col, \n",
    "                                                    layer_type,\n",
    "                                      batch_size=batch_size, verbose=verbose,\n",
    "                                      force_reload=force_reload, save=save)\n",
    "            \n",
    "            # rank sentences by relevancy\n",
    "            ranked_df, ranked_df_path = self.generate_ranked_df(rank_method, score_df, tuner_dir, corpus_name, \n",
    "                                                topic_id, X_col,\n",
    "                                                k=k, redundancy_score=redundancy_score, \n",
    "                                                verbose=verbose, force_reload=force_reload,\n",
    "                                               min_tokens=min_tokens, save=save)\n",
    "            \n",
    "            # generate text summary\n",
    "            self.create_text_summary(ranked_df, ranked_df_path, ordering=sent_position, \n",
    "                                     force_reload=force_reload)\n",
    "        \n",
    "        print(\"Summaries generated\")\n",
    "            \n",
    "        \n",
    "    def generate_topic_score_df(self, tuner_dir, corpus_name, nested_dir, topic_id, X_col, layer_type,\n",
    "                                batch_size=256, verbose=verbose,\n",
    "                                  force_reload=False, save=True):\n",
    "        # create/load predicted scores\n",
    "        score_df_path = gen_score_df_path(tuner_dir, topic_id, X_col, corpus_name)\n",
    "        score_df = None\n",
    "        if os.path.exists(score_df_path) and not force_reload:  # load\n",
    "            score_df = read_df_file_type(score_df_path, verbose=verbose)\n",
    "        else: # generate new score_df\n",
    "            # get paths for topic\n",
    "            emb_paths = self.path_ret.get_embedding_paths(corpus_name, nested_dir, topic_ids=topic_id, \n",
    "                                                          return_dir_path=False, verbose=verbose)\n",
    "            emb_paths = list(emb_paths['path'])\n",
    "            \n",
    "            # load model\n",
    "            model = self.model_path_handler.load_best_model(corpus_name, nested_dir, X_col, y_col, \n",
    "                                                            batch_size, layer_type)\n",
    "            \n",
    "            # create df of predicted scores\n",
    "            score_df = self.get_prediction_scores(model, emb_paths, X_col=X_col, batch_size=batch_size,\n",
    "                                                 sort_scores=True)\n",
    "            # save df\n",
    "            if save:\n",
    "                save_df_file_type(score_df, score_df_path, verbose=verbose)\n",
    "        return score_df\n",
    "    \n",
    "    def get_prediction_scores(self, model, emb_paths, X_col='embedding',\n",
    "                             batch_size=256, sort_scores=True, add_X_col_to_df=True):\n",
    "        \"\"\"\n",
    "        Differs from RedundancyThresholdGenerator process by assumption all emb_dfs can be loaded into memory\n",
    "        \n",
    "        Return dataframe with columns:\n",
    "            sentence: string of sentence\n",
    "            score: predicted score from model\n",
    "            relative_pos: relative_position of sentence in its article (sent_id)\n",
    "            streamid: streamid of article sentence is from\n",
    "            embedding (optional): embedding of sentence (determined by X_col parameter)\n",
    "        \"\"\"\n",
    "        \n",
    "        # predict scores for target embeddings to place into df\n",
    "        score_df_dict = defaultdict(list)\n",
    "        print(\"Predicting scores...\")\n",
    "        for emb_path in tqdm_notebook(emb_paths):\n",
    "            # load emb_df\n",
    "            emb_df = load_embeddings(emb_path, verbose=False)\n",
    "            embs = list(embs[X_col])\n",
    "            \n",
    "            # predict scores\n",
    "            scores = predict_emb_list(model, emb_list, batch_size=batch_size)\n",
    "            \n",
    "            score_df_dict['sentence'].extend(list(emb_df['sentence']))\n",
    "            score_df_dict['score'].extend(scores.tolist())\n",
    "            score_df_dict['relative_pos'].extend(list(emb_df['sent_id']))  # relative position in article\n",
    "            score_df_dict['streamid'].extend(list(emb_df['streamid']))\n",
    "            if add_X_col_to_df:\n",
    "                score_df_dict['embedding'].extend(embs)\n",
    "        \n",
    "        # tie sentence <-> score together and access DataFrame functionality\n",
    "        score_df = pd.DataFrame(score_df_dict)\n",
    "        if sort_scores:\n",
    "            score_df = score_df.sort_values(by=\"score\", ascending=False)  # descending order\n",
    "        return score_df\n",
    "        \n",
    "    def generate_ranked_df(self, rank_method, score_df, tuner_dir, corpus_name, topic_id, X_col, k=10000,\n",
    "                           redundancy_score=None, verbose=verbose, force_reload=False,\n",
    "                          min_tokens=None, save=True):\n",
    "        \n",
    "        # get ranked df path\n",
    "        ranked_df_path = None\n",
    "        if rank_method == \"k\":\n",
    "                ranked_df_path = gen_ranked_df_path(tuner_dir, rank_method, topic_id, X_col, corpus_name)\n",
    "        elif rank_method == \"k_non_redund\":\n",
    "            ranked_df_path = gen_ranked_df_path(tuner_dir, rank_method, topic_id, X_col, corpus_name, \n",
    "                                        min_tokens=min_tokens, redundancy_threshold=redundancy_threshold)\n",
    "        \n",
    "        # determine if load ranked_df or create new one\n",
    "        ranked_df = None\n",
    "        if not os.path.exists(ranked_df_path) or force_reload:\n",
    "            # load new ranked_df with selected method\n",
    "            ranked_idxs = None\n",
    "            ranked_df_path = None\n",
    "            if rank_method == \"k\":\n",
    "                ranked_df = self.retrieve_top_k_sentences(score_df, k, is_sorted=True)\n",
    "            elif rank_method == \"k_non_redund\":\n",
    "                ranked_df, ranked_idxs = self.ret_top_k_non_redundant(score_df, \n",
    "                                                                    k, \n",
    "                                                                    redundancy_threshold, \n",
    "                                                                    X_col='embedding', \n",
    "                                                                    is_sorted=True,\n",
    "                                                                    return_indexes=True,\n",
    "                                                                    min_tokens=min_tokens)\n",
    "            else:\n",
    "                raise Exception(str(rank_method) + \" is not a valid rank_method: \" + str(self.rank_methods))\n",
    "                \n",
    "            \n",
    "            # save ranked_idxs\n",
    "            if ranked_idxs is not None:\n",
    "                ranked_idxs_path = gen_varname_path(tuner_dir, \"ranked_idxs\", rank_method, topic_id, X_col,\n",
    "                                                   corpus_name)\n",
    "                with open(ranked_idxs_path, 'wb') as handle:\n",
    "                    pickle.dump(ranked_idxs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "            # save ranked_df\n",
    "            if save:\n",
    "                save_df_file_type(ranked_df, ranked_df_path, verbose=verbose)\n",
    "        else:\n",
    "            ranked_df = read_df_file_type(ranked_df_path, verbose=verbose)\n",
    "            \n",
    "        return ranked_df, ranked_df_path  # return path to create path for summary\n",
    "        \n",
    "    def retrieve_top_k_sentences(self, score_df, k, is_sorted=True):\n",
    "        \"\"\"\n",
    "        Retrieve top k scoring sentences from score_df\n",
    "        \"\"\"\n",
    "        if not is_sorted:\n",
    "            score_df = score_df.sort_values(by=\"score\", ascending=False)\n",
    "        \n",
    "        top_k_df = score_df[0:k]\n",
    "        \n",
    "        return top_k_df\n",
    "    \n",
    "    def ret_top_k_non_redundant(self, score_df, k, redundancy_threshold, X_col='embedding', is_sorted=True,\n",
    "                               return_indexes=True, min_tokens=None):\n",
    "        \"\"\"\n",
    "        Retrieve top k sentences from df, not repeating redundant sentences\n",
    "        \"\"\"\n",
    "        if not is_sorted:\n",
    "            score_df = score_df.sort_values(by=\"score\", ascending=False)\n",
    "        \n",
    "#         score_df_copy = copy.deepcopy(score_df)\n",
    "        \n",
    "        top_k_rows = []\n",
    "        top_k_row_idxs = None\n",
    "        if return_indexes:  # for statistics\n",
    "            top_k_row_idxs = []\n",
    "            \n",
    "        nlp = None  # for min tokens\n",
    "        if min_tokens is not None:\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # iteratively find top k sentences where cosine similarity is under redundancy_threshold\n",
    "        for index, row in tqdm_notebook(score_df.iterrows()):\n",
    "            # check if sentence meets minimum tokens\n",
    "            if min_tokens is not None:\n",
    "                num_tokens = len(nlp(str(row['embedding'])))\n",
    "                if num_tokens < min_tokens:\n",
    "                    continue\n",
    "            \n",
    "            # for each sentence, compare cosine similarity with existing chosen sentences\n",
    "            under_threshold = True\n",
    "            row_emb = row['embedding']\n",
    "            for k_row in top_k_rows:\n",
    "                \n",
    "                k_emb = k_row['embedding']\n",
    "                cos_sim = cosine_similarity(row_emb, k_emb)\n",
    "                \n",
    "                # if sentence is above allowed level of similarity with other sentences\n",
    "                if cos_sim > redundancy_threshold:  \n",
    "                    under_threshold = False\n",
    "                    break  # break from nested loop\n",
    "            \n",
    "            # add sentence that is above allowed level of similarity\n",
    "            if under_threshold: \n",
    "                top_k_rows.append(row)\n",
    "                if return_indexes:  # for statistics\n",
    "                    top_k_row_idxs.append(index)\n",
    "            \n",
    "            # end when we have collected k sentences\n",
    "            if len(top_k_rows) == k:\n",
    "                break\n",
    "        \n",
    "        top_k_df = pd.DataFrame(top_k_rows)\n",
    "#         top_k_df = self.order_by_article_position(top_k_df)\n",
    "        return top_k_df\n",
    "\n",
    "    def create_text_summary(self, ranked_df, ranked_df_path, ordering=None, force_reload=False):\n",
    "        # order by select ordering\n",
    "        if ordering == \"score\":\n",
    "            ranked_df = ranked_df.sort_values(by=\"score\", ascending=False)\n",
    "        elif ordering == \"sent_position\":\n",
    "            ranked_df = self.order_by_sent_position(ranked_df)\n",
    "        else:\n",
    "            if ordering not in self.ordering_methods:\n",
    "                raise Exception(\"Invalid ordering parameter\")\n",
    "        \n",
    "        # get path to save to\n",
    "        summary_path = gen_summary_path_from_ranked_df_path(ranked_df_path)\n",
    "        if not os.path.exists(summary_path) or force_reload:\n",
    "            # get sents (previously ordered)\n",
    "            sents = ranked_df['sentence']\n",
    "            # write to text file\n",
    "            with open(summary_path, \"w\") as f:\n",
    "                for sent in sents:\n",
    "                    out = str(sent) + \" \"  # add spacing between sentences\n",
    "                    f.write(out)\n",
    "        print(\"Text summary saved to: \" + str(summary_path))\n",
    "    \n",
    "    def order_by_sent_position(self, ranked_df):\n",
    "        \"\"\"Order sentences by relative article position\"\"\"\n",
    "        ranked_df = ranked_df.sort_values(by=\"relative_pos\", ascending=True)\n",
    "        return ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

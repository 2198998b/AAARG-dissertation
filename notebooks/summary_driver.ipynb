{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Generating Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp while cluster is full\n",
    "# !pip install keras-tuner\n",
    "# !pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_threads = 32\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = str(num_threads)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ipynb.fs\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "import kerastuner as kt\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from kerastuner.tuners import Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_col_labels = ['cosine_similarity', 'cos_sim_nearest_nug']\n",
    "default_input_col = \"embedding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTuner:\n",
    "    def __init__(self, save_dir, save_name, input_dim=768, tuning_iterations=50, batch_size=32, force_reload=False):\n",
    "        \"\"\"Can save using project_name param, if overwrite false then will reload where it started\n",
    "        In Tuner Class documentation\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.models = []\n",
    "        self.tuner = Hyperband(self.build_model, \n",
    "                          objective='mean_squared_error', \n",
    "                          max_epochs=25,\n",
    "                          hyperband_iterations=tuning_iterations,\n",
    "                          directory=save_dir,\n",
    "                          project_name=save_name,\n",
    "                          overwrite=force_reload)\n",
    "        \n",
    "    def build_model(self, hp):\n",
    "        model = Sequential()\n",
    "        ilayer = InputLayer(input_shape=(self.input_dim,), batch_size=self.batch_size)\n",
    "        model.add(ilayer)\n",
    "        for i in range(hp.Int('num_layers', min_value=1, max_value=4)):\n",
    "            model.add(Dense(units=hp.Int('units_' + str(i),\n",
    "                                        min_value=1, max_value=1024, step=32),\n",
    "                            activation=hp.Choice('activ_' + str(i),\n",
    "                                                ['relu', 'tanh', 'sigmoid'])))\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "                learning_rate=hp.Float('learning_rate', min_value=0.00001, max_value=0.1))           \n",
    "        losses = hp.Choice('loss_func', ['MSE', 'huber', 'binary_crossentropy', 'categorical_crossentropy'])\n",
    "        model.compile(optimizer=opt, loss=losses, metrics=['mean_squared_error'])  # add metrics here\n",
    "        self.models.append(model)\n",
    "        return model\n",
    "    \n",
    "    def search(self, batch_generator, save_path=None, return_hyperparams=False):\n",
    "        \"\"\"Find optimal model given dataset\n",
    "        \"\"\"\n",
    "        self.tuner.search(x=batch_generator, verbose=1, use_multiprocessing=False, workers=num_threads)\n",
    "        best_model = self.tuner.get_best_models(num_models=1)\n",
    "        if save_path is not None:\n",
    "            tf.keras.save(save_path)\n",
    "        if return_hyperparams:\n",
    "            hyperparams = self.tuner.get_best_hyperparameters(num_trials=1)\n",
    "            return best_model, hyperparams\n",
    "        return best_model\n",
    "    \n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class BatchGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Class to load in dataset that is too large to load into memory at once\n",
    "    \n",
    "    Do check in class before to make sure all X lists and y lists are same length\n",
    "    \n",
    "    https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self, X_path_dict, y_path_dict, num_samples, batch_size, max_topics_in_mem=4):\n",
    "        if batch_size is None:\n",
    "            self.batch_size = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        \n",
    "        self.X_path_dict = X_path_dict\n",
    "        self.y_path_dict = y_path_dict\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.max_topics_in_mem = self.max_topics_in_mem\n",
    "        self.cur_idx_bounds = []\n",
    "        self.loaded_topic_id = [] \n",
    "        self.loaded_inputs = OrderedDict()\n",
    "        self.loaded_labels = OrderedDict()\n",
    "#         self.cur_idx_bounds = []\n",
    "#         self.loaded_topic_id = None \n",
    "#         self.loaded_inputs = None\n",
    "#         self.loaded_labels = None\n",
    "        \n",
    "        self.batch_count = 0\n",
    "        self.total_batches = math.ceil(self.num_samples / self.batch_size)\n",
    "#         self.shuffle = False  # make sure linear progression through dataset for sake of memory efficiency\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return self.total_batches\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one batch of data\"\"\"\n",
    "        # get end index of batch\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "        print(\"idx: \" + str(idx) + \"\\nstart/end_idx: \" + str(start_idx) + \" / \" + str(end_idx))\n",
    "        # find what topic the start and end index are in\n",
    "        topic_ids, topic_bounds_list = self.resolve_idx(start_idx, end_idx)\n",
    "        # load inputs and labels from the correct topic(s)\n",
    "        inputs, labels = self.get_batch(start_idx, end_idx, topic_ids, topic_bounds_list)\n",
    "        \n",
    "        self.batch_count += 1\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        labels = tf.expand_dims(tf.convert_to_tensor(labels), 1)\n",
    "        return inputs, labels\n",
    "    \n",
    "    def get_batch(self, start_idx, end_idx, topic_ids, bounds_list):\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        for topic_id, topic_bounds in zip(topic_ids, bounds_list):   # handle batches whose index is between topics\n",
    "            if topic_id != self.loaded_topic_id:\n",
    "                # load new topic into memory\n",
    "                print(\"Loading new topic_id \" + str(topic_id) + \" old topic_id \" + str(self.loaded_topic_id))\n",
    "                self.loaded_inputs = self.load_samples(self.X_path_dict[topic_id]['path'])\n",
    "                self.loaded_labels = self.load_samples(self.y_path_dict[topic_id]['path'])\n",
    "                self.loaded_topic_id = topic_id\n",
    "                self.cur_idx_bounds = topic_bounds\n",
    "            \n",
    "            # find relative indexes of the batch on the currently loaded lists\n",
    "            rel_start = self.get_relative_idx(start_idx, topic_bounds)\n",
    "            rel_end = self.get_relative_idx(end_idx, topic_bounds)\n",
    "            if rel_end > topic_bounds[1]:  # end_idx is in next topic\n",
    "                rel_end = len(self.loaded_inputs)  # set end of slice to rest of list\n",
    "            elif rel_start < 0:  # start_idx was in previous topic\n",
    "                rel_start = 0  # set start of slice to start of list\n",
    "            \n",
    "            # slice list for this topic\n",
    "            input_slice = self.loaded_inputs[rel_start:rel_end]\n",
    "            label_slice = self.loaded_labels[rel_start:rel_end]\n",
    "            inputs.extend(input_slice)\n",
    "            labels.extend(label_slice)\n",
    "        return inputs, labels\n",
    "            \n",
    "            \n",
    "    def get_relative_idx(self, idx,  bounds):\n",
    "        \"\"\"Index on loaded list, used to slice\"\"\"\n",
    "        rel_idx = idx - bounds[0]\n",
    "        return rel_idx\n",
    "    \n",
    "    def resolve_idx(self, start_idx, end_idx):\n",
    "        \"\"\"Check what topic the idx falls under\n",
    "        \n",
    "        Return the topic(s) and their bounds as defined in X_input_dict\n",
    "        \"\"\"\n",
    "        topic_ids = None\n",
    "        topic_bounds_list = None\n",
    "        in_cur_topic = False\n",
    "        # check currently loaded topic first\n",
    "        if self.loaded_topic_id is not None:\n",
    "            in_bounds = []\n",
    "            indexes = [start_idx, end_idx]\n",
    "            for index in indexes:\n",
    "                in_bound = self.idx_in_bounds(index, self.cur_idx_bounds[0], self.cur_idx_bounds[1])\n",
    "                in_bounds.append(in_bound)\n",
    "            if all(in_bounds):  # both start and end_idx is in current topic\n",
    "                topic_ids = [self.loaded_topic_id]\n",
    "                topic_bounds_list = [self.cur_idx_bounds]\n",
    "                in_cur_topic = True\n",
    "        if not in_cur_topic:\n",
    "            # iteratively find what topic start and end idx are in\n",
    "            start_topic, end_topic = None, None\n",
    "            start_bounds, end_bounds = None, None\n",
    "            for topic_id, topic_dict in self.X_path_dict.items():\n",
    "                idx_bounds = [topic_dict['start_idx'], topic_dict['end_idx']]\n",
    "                if self.idx_in_bounds(start_idx, idx_bounds[0], idx_bounds[1]):\n",
    "                    start_topic = topic_id\n",
    "                    start_bounds = idx_bounds\n",
    "                if self.idx_in_bounds(end_idx, idx_bounds[0], idx_bounds[1]):\n",
    "                    end_topic = topic_id\n",
    "                    end_bounds = idx_bounds\n",
    "                if start_topic is not None and end_topic is not None:\n",
    "                    break\n",
    "                    \n",
    "            if start_topic is None:\n",
    "                raise Exception(\"idx \" + str(start_idx) + \" not found in bounds of topics, where num_samples==\"\n",
    "                                + str(self.num_samples) + \" and batch_size==\" + str(self.batch_size))\n",
    "            \n",
    "            # check case where is final batch of dataset\n",
    "            if end_topic is None:\n",
    "                if self.batch_count == self.total_batches - 1:\n",
    "                    # assume end_idx is past end of dataset (i.e. final batch < batch_size)\n",
    "                    # set end index to end of last topic\n",
    "                    end_topic = start_topic\n",
    "                    end_bounds = start_bounds\n",
    "                else:\n",
    "                    raise Exception(\"end idx \" + str(end_idx)\n",
    "                                + \" not found in bounds of topics, where num_samples==\"\n",
    "                                + str(self.num_samples) + \" and batch_size==\" + str(self.batch_size))\n",
    "            \n",
    "            # check if start and end_idx are in same topic\n",
    "            topic_ids = None\n",
    "            topic_bounds_list = None\n",
    "            if start_topic == end_topic:\n",
    "                topic_ids = [start_topic]\n",
    "                topic_bounds_list = [start_bounds]\n",
    "            else:\n",
    "                topic_ids = [start_topic, end_topic]\n",
    "                topic_bounds_list = [start_bounds, end_bounds]\n",
    "            \n",
    "        return topic_ids, topic_bounds_list\n",
    "            \n",
    "                \n",
    "    def idx_in_bounds(self, idx, start, end):\n",
    "        if idx >= start and idx <= end:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def load_samples(self, path):\n",
    "        samples = None\n",
    "        with open(path, 'rb') as handle:\n",
    "            samples = pickle.load(handle)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .defs.corpus_loader import PathRetriever, load_embeddings, load_topics, read_df_file_type, save_df_file_type\n",
    "from .defs.corpus_loader import convert_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_input_params(path_ret, corpus_names, nested_dirs, col_labels, input_col=None):\n",
    "    \"\"\"Helper function to resolve the selection of input params that determine what data to load/generate\"\"\"\n",
    "    # resolve corpus_names\n",
    "    if corpus_names is None:\n",
    "        corpus_names = path_ret.get_corpus_names()\n",
    "        if len(corpus_names) == 0:\n",
    "            raise Exception(\"There are no corpuses to load from\")\n",
    "    # resolve col_labels\n",
    "    if col_labels is None:  # our columns to generate files for\n",
    "        col_labels = default_col_labels.copy()\n",
    "        if input_col is not None:\n",
    "            col_labels.append(input_col)\n",
    "    # resolve nested_dirs\n",
    "    if type(nested_dirs) != dict:  # if output gets passed through again\n",
    "        nested_dict = {}\n",
    "        for corpus_name in corpus_names:  # get the nested dir for each corpus name\n",
    "            nested_dict[corpus_name] = path_ret.get_nested_dirs(corpus_name, \"embeddings\")\n",
    "            if nested_dirs is not None:\n",
    "                # add only selected nested_dirs for this corpus_name\n",
    "                nested_dict[corpus_name] = [x for x in nested_dict[corpus_name] if x in nested_dirs]\n",
    "        nested_dirs = nested_dict\n",
    "    # make sure there is at least one entry in nested_dict\n",
    "    empty_dirs = [len(x) == 0 for x in nested_dirs.values()]  # get if empty for each item\n",
    "    if all(empty_dirs):\n",
    "        raise Exception(\"There are no nested_dirs matching the selection\")\n",
    "    return corpus_names, nested_dirs, col_labels\n",
    "\n",
    "def corpus_name_topic_ids(path_retriever, corpus_name):\n",
    "    topic_path = path_retriever.get_topic_path(corpus_name, verbose=False)\n",
    "    topic_df = load_topics(topic_path, verbose=False)\n",
    "    topic_ids = list(topic_df['id'].unique())\n",
    "    return topic_ids\n",
    "\n",
    "def find_combinations(path_df, corpus_names, nested_dirs, col_labels, add_topics=False, col_labels_as_list=False,\n",
    "                      as_tuples=True, force_reload=False, path_retriever=None):\n",
    "    \"\"\"Find the combinations that have not been generated/trained already in path_df\n",
    "    \n",
    "    Tuple ordering: (corpus_name, nested_dir, col_label/[col_labels], **topic_id**)\n",
    "    \"\"\"\n",
    "    topic_ids = {}\n",
    "    if add_topics:  # find topic_ids for each corpus\n",
    "        for corpus_name in corpus_names:\n",
    "            if path_retriever is not None:\n",
    "                topic_ids[corpus_name] = corpus_name_topic_ids(path_retriever, corpus_name)\n",
    "            else:\n",
    "                raise Exception(\"If add_topics is True then path_retriever must be set to an instance of PathRetriever\")\n",
    "    # get possible combinations\n",
    "    combinations = []\n",
    "    for corpus_name in corpus_names:\n",
    "        for nested_dir in nested_dirs[corpus_name]:\n",
    "            combo_path = path_df[(path_df['corpus_name'] == corpus_name)\n",
    "                                    & (path_df['nested_dir'] == nested_dir)]\n",
    "            combo = [corpus_name, nested_dir]\n",
    "            if add_topics:  # create permutations with topic_ids\n",
    "                topic_combo_dict = defaultdict(list)\n",
    "                for label in col_labels:\n",
    "                    for topic_id in topic_ids[corpus_name]:  # check if label exists for topic_id\n",
    "                        topic_path = combo_path[(combo_path['col_label'] == label)\n",
    "                                               & (combo_path['topic_id'] == topic_id)]\n",
    "                        if len(topic_path) == 0 or force_reload:\n",
    "                            topic_combo_dict[topic_id].append(label)\n",
    "                topic_combos = []\n",
    "                for topic_id, labels in topic_combo_dict.items():\n",
    "                    topic_combos = []\n",
    "                    if col_labels_as_list:  # add single tuple with all missing col_labels for topic_id\n",
    "                        topic_combo = copy.deepcopy(combo)\n",
    "                        topic_combo.append(labels)\n",
    "                        topic_combo.append(topic_id)\n",
    "                        topic_combos.append(topic_combo)\n",
    "                    else:\n",
    "                        for label in labels:  # add a tuple for each missing col_label for topic_id\n",
    "                            topic_combo = copy.deepcopy(combo)\n",
    "                            topic_combo.append(topic_id)\n",
    "                            topic_combos.append(topic_combo)\n",
    "                    combinations.extend(topic_combos)\n",
    "            else:  # create permutations with col_labels only\n",
    "                label_combos = []\n",
    "                add_labels = None\n",
    "                if not force_reload:  # find which col_labels don't exist already\n",
    "                    exist_labels = list(combo_path['col_label'].unique())\n",
    "                    add_labels = [x for x in col_labels if x not in exist_labels]\n",
    "                else:\n",
    "                    add_labels = copy.deepcopy(col_labels)  # force_reload add all labels\n",
    "                if col_labels_as_list:  # add single tuple\n",
    "                    label_combo = copy.deepcopy(combo)\n",
    "                    label_combo.append(add_labels)\n",
    "                    label_combos.append(label_combo)\n",
    "                else:\n",
    "                    for add_label in add_labels:  # add tuple for each col_label\n",
    "                        label_combo = copy.deepcopy(combo)\n",
    "                        label_combo.append(add_label)\n",
    "                        label_combos.append(label_combo)\n",
    "                combinations.extend(label_combos)\n",
    "                \n",
    "    if as_tuples:\n",
    "        combinations = [tuple(x) for x in combinations]\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLabelHandler:\n",
    "    \"\"\"Class that will load and store an instance of the dataset to be fed to a model\"\"\"\n",
    "    def __init__(self, proj_dir=None, input_col_name=\"embedding\"):\n",
    "        if proj_dir is None:\n",
    "            self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "        else:\n",
    "            self.proj_dir = proj_dir\n",
    "        self.default_file_type = \".hdf\"\n",
    "        self.path_ret = PathRetriever(proj_dir)\n",
    "        self.label_options = ['cosine_similarity', 'cos_sim_nearest_nug']\n",
    "        self.input_col_name = input_col_name\n",
    "#         self.default_test_topics = [1,2,3,4,5,6,8,9,10]\n",
    "        # label_path_df variables\n",
    "        self.label_path_df_dir = self.path_ret.path_handler.dataset_dir\n",
    "        self.label_path_df_path = os.path.join(self.label_path_df_dir, \"label_path_df.hdf\")\n",
    "        self.label_path_df_cols = ['corpus_name', 'nested_dir', 'topic_id', 'col_label', 'list_len', \n",
    "                                   'input_dim', 'batch_size', 'path']\n",
    "        \n",
    "        \n",
    "    def generate(self, corpus_names=None, nested_dirs=None, col_labels=None, emb_file_type=None, batch_size=1,\n",
    "                               force_reload=False, verbose=True):\n",
    "        \"\"\"Generate easily loadable inputs/labels files to be fed to NN when needed\"\"\"\n",
    "        \n",
    "        self.label_path_df = self.load_label_path_df(verbose=verbose)\n",
    "        \n",
    "        corpus_names, nested_dirs, col_labels = resolve_input_params(self.path_ret, corpus_names,\n",
    "                                                                    nested_dirs, col_labels, input_col=\"embedding\")\n",
    "        \n",
    "        if emb_file_type is None:  # target file type to load from\n",
    "            emb_file_type = self.default_file_type\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Retrieving the following: \" + str(\", \".join(col_labels)))\n",
    "            \n",
    "        combinations = find_combinations(self.label_path_df, corpus_names, nested_dirs, col_labels,\n",
    "                                         add_topics=True, col_labels_as_list=True, as_tuples=True, \n",
    "                                         force_reload=force_reload, path_retriever=self.path_ret)\n",
    "\n",
    "        if len(combinations) > 0:\n",
    "            for corpus_name, nested_dir, col_labels, topic_id in tqdm_notebook(combinations):\n",
    "                if verbose:\n",
    "                    print(\"corpus_name: \" + str(corpus_name) + \"\\n\"\n",
    "                         + \"nested_dir: \" + str(nested_dir) + \"\\n\"\n",
    "                         + \"col_labels: \" + str(col_labels) + \"\\n\"\n",
    "                         + \"topic_id: \" + str(topic_id))\n",
    "\n",
    "                emb_paths, nested_dir_path = self.path_ret.get_embedding_paths(corpus_name, nested_dir, \n",
    "                                                            file_type=emb_file_type, verbose=False, \n",
    "                                                            return_dir_path=True, topic_ids=[topic_id])\n",
    "                if len(emb_paths) == 0:\n",
    "                    raise Exception(\"No paths for \" + str(corpus_name) + \", \" + str(nested_dir) + \", \"\n",
    "                                   + str(emb_file_type) + \", topic_id: \" + str(topic_id))\n",
    "                # load labels and save to pickled file\n",
    "                save_paths = {}\n",
    "                for col_label in col_labels:\n",
    "                    save_paths[col_label] = self.generate_path(nested_dir_path, topic_id, col_label)\n",
    "                    \n",
    "                # load the selected labels\n",
    "                loaded_labels = self.retrieve_col_data(emb_paths, col_labels, verbose=verbose)\n",
    "                \n",
    "                # save the results to separate files\n",
    "                for label, label_data in loaded_labels.items():\n",
    "                    label_path = save_paths[label]\n",
    "                    with open(label_path, 'wb') as handle:\n",
    "                        pickle.dump(label_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    # save path in label_path_df to keep track of saved files\n",
    "                    input_dim = label_data[0].size\n",
    "                    self.add_path_to_df(corpus_name, nested_dir, topic_id, label, len(label_data), \n",
    "                                        input_dim, label_path)\n",
    "                    if verbose:\n",
    "                        print(\"File saved to: \" + str(label_path))\n",
    "                self.save_label_path_df()\n",
    "            print(\"Completed generating inputs/labels\")\n",
    "        else:\n",
    "            print(\"Input/label combinations fully loaded\")\n",
    "    \n",
    "    def get_paths(self, corpus_name, nested_dir, col_label, topic_ids=None, return_indices=False,\n",
    "                 return_total_len=False, return_input_dim=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            return_indices: add start and end index for topic into dict, if topics were to be loaded as a\n",
    "                            continuous list\n",
    "        Return:\n",
    "            Dict where:\n",
    "                    key: a topic_id or 'input_dim'\n",
    "                    value: nested_dict  : keys = \"path\", \"length\", (\"start_idx\", \"end_idx\")\n",
    "        \"\"\"\n",
    "        paths = self.label_path_df\n",
    "        paths = paths[(paths['corpus_name'] == corpus_name) \n",
    "                      & (paths['nested_dir'] == nested_dir)\n",
    "                      & (paths['col_label'] == col_label)]\n",
    "        if topic_ids is not None:\n",
    "            paths = paths[paths['topic_id'].isin(topic_ids)]\n",
    "        \n",
    "        # create dict entry for each topic_id, store its path the length of the target list\n",
    "        path_dict = {}\n",
    "        for index, row in paths.iterrows():\n",
    "            topic_id = int(row['topic_id'])\n",
    "            path_dict[topic_id] = {\"path\":row['path'], \"length\":int(row['list_len'])}\n",
    "        \n",
    "        input_dim = list(paths['input_dim'].unique())\n",
    "        if len(input_dim) > 1:\n",
    "            raise Exception(\"Dimensions of list objects varies: \" + str(input_dim))\n",
    "        else:\n",
    "            input_dim = input_dim[0]\n",
    "            \n",
    "        cur_len = 0\n",
    "        cur_idx = 0\n",
    "        if return_indices:  \n",
    "            # indices represent where indices would be if each topic's list was loaded into one big list\n",
    "            # add where start and end index would be to each topic_id's dict\n",
    "            for topic_id, topic_dict in path_dict.items():\n",
    "                cur_len += topic_dict[\"length\"]\n",
    "                start_idx = cur_idx\n",
    "                end_idx = cur_len - 1\n",
    "                topic_dict[\"start_idx\"] = start_idx\n",
    "                topic_dict[\"end_idx\"] = end_idx\n",
    "                cur_idx = end_idx + 1  # where next topic will start\n",
    "        \n",
    "        if return_total_len or return_input_dim:\n",
    "            ret_var = [path_dict]\n",
    "            if return_total_len:\n",
    "                ret_var.append(cur_len)\n",
    "            if return_input_dim:\n",
    "                ret_var.append(input_dim)\n",
    "            ret_var = tuple(ret_var)\n",
    "            return ret_var\n",
    "        else:\n",
    "            return path_dict\n",
    "    \n",
    "    def path_dicts_match(self, X_path_dict, y_path_dict, both_indexed=False):\n",
    "        \"\"\"Check that the X and y path dicts have same topics and same indexing information\"\"\"\n",
    "        for topic_id, X_topic_dict in X_path_dict.items():\n",
    "            if topic_id not in y_path_dict:  # contains different topics\n",
    "                return False\n",
    "            \n",
    "            # check indexing information\n",
    "            y_topic_dict = y_path_dict[topic_id]\n",
    "            if both_indexed:  # both have info on indexing for generator\n",
    "                if X_topic_dict[\"start_idx\"] != y_topic_dict[\"start_idx\"]:\n",
    "                    return False\n",
    "                if X_topic_dict[\"end_idx\"] != y_topic_dict[\"end_idx\"]:\n",
    "                    return False\n",
    "            if X_topic_dict[\"length\"] != y_topic_dict[\"length\"]:\n",
    "                return False\n",
    "            \n",
    "            if X_topic_dict['path'] == y_topic_dict['path']:  # if x_path == y_path\n",
    "                raise Exception(\"X and y paths are equal for topic_id \" + str(topic_id) +\":\\n\"\n",
    "                               + str(value))\n",
    "        return True\n",
    "        \n",
    "    def load(self, corpus_name, nested_dir, col_label, topic_ids=None, return_as_list=False, verbose=True):\n",
    "        \"\"\"Load selected generated input/label\n",
    "        \n",
    "        Return:\n",
    "            dict where keys are equal to topic_ids, values equal to a list of the input/label data\n",
    "            \n",
    "            if return_as_list is True, return list in order of topic_id\n",
    "        \"\"\"\n",
    "        paths = self.label_path_df\n",
    "        paths = paths[(paths['corpus_name'] == corpus_name) \n",
    "                      & (paths['nested_dir'] == nested_dir)\n",
    "                      & (paths['col_label'] == col_label)]\n",
    "        if topic_ids is not None:  # otherwise load all\n",
    "            before_len = len(paths)\n",
    "            paths = paths[paths['topic_id'].isin(topic_ids)]\n",
    "            after_len = len(paths)\n",
    "        if len(paths) == 0:\n",
    "            raise Exception(\"There are no paths to load the selected inputs/labels\")\n",
    "        \n",
    "        loaded_data = {}\n",
    "        loaded_topics = []\n",
    "        pbar = None\n",
    "        if verbose:\n",
    "            print(\"Loading pickled files for: \" + str(col_label))\n",
    "            pbar = tqdm_notebook(total=len(paths))\n",
    "        for index, row in paths.iterrows():\n",
    "            save_path = row['path']\n",
    "            if not os.path.exists(save_path):\n",
    "                raise Exception(\"Generated \" + str(col_label) + \" for \" + str(corpus_name) \n",
    "                                + \", \" + str(nested_dir) + \" does not exist at: \" + str(save_path))\n",
    "            topic_data = []\n",
    "            with open(save_path, 'rb') as handle:\n",
    "                topic_data = pickle.load(handle)\n",
    "            \n",
    "            topic_id = int(row['topic_id'])\n",
    "            loaded_topics.append(topic_id)  # for verbose printing\n",
    "            loaded_data[topic_id] = topic_data\n",
    "            if verbose:\n",
    "                pbar.update()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Loaded \" + str(col_label) + \" for \" + str(corpus_name) \n",
    "                                + \", \" + str(nested_dir) + \"\\ntopic_ids loaded: \" + str(loaded_topics))\n",
    "        if return_as_list:  # transform dictionary entries into single list\n",
    "            as_list = []\n",
    "            for data in loaded_data.values():\n",
    "                as_list.extend(data)\n",
    "            return as_list\n",
    "        else:\n",
    "            return loaded_data\n",
    "    \n",
    "    def corpus_topic_ids(self, corpus_name):\n",
    "        \"\"\"Num topics for given corpus_name\"\"\"\n",
    "        topic_ids = list(self.label_path_df[self.label_path_df['corpus_name'] == corpus_name]['topic_id'].unique())\n",
    "        return topic_ids\n",
    "                        \n",
    "    def load_label_path_df(self, verbose=True):\n",
    "        label_path_df = None\n",
    "        if verbose:\n",
    "            print(\"Loading label_path_df\")\n",
    "        if os.path.exists(self.label_path_df_path):\n",
    "            label_path_df = read_df_file_type(self.label_path_df_path, verbose=True)\n",
    "        else:\n",
    "            label_path_df = pd.DataFrame(columns=self.label_path_df_cols)\n",
    "            if verbose:\n",
    "                print(\"label_path_df created from scratch\")\n",
    "        return label_path_df\n",
    "    \n",
    "    def add_path_to_df(self, corpus_name, nested_dir, topic_id, col_label, list_len, input_dim, path):\n",
    "        row = {\"corpus_name\":corpus_name, \"nested_dir\":nested_dir, \"topic_id\":topic_id, \"list_len\":list_len,\n",
    "               \"input_dim\":input_dim, \"col_label\":col_label, \"path\":path}\n",
    "        self.label_path_df = self.label_path_df.append(row, ignore_index=True)\n",
    "        \n",
    "    def save_label_path_df(self):\n",
    "        save_df_file_type(self.label_path_df, self.label_path_df_path, verbose=False)\n",
    "                \n",
    "    def retrieve_col_data(self, emb_paths, col_labels, verbose=True):\n",
    "        # setup return variables\n",
    "        labels = {}\n",
    "        for col_label in col_labels:\n",
    "            labels[col_label] = []\n",
    "        # search through paths for labels\n",
    "        pbar = None\n",
    "        if verbose:\n",
    "            pbar = tqdm_notebook(total=len(emb_paths))\n",
    "        for emb_path in emb_paths['path']:\n",
    "            emb_df = load_embeddings(emb_path, verbose=False)\n",
    "            for col_label in col_labels:\n",
    "                if col_label not in emb_df.columns:\n",
    "                    raise ValueError(\"Target label \" + str(col_label) + \" is not in file at \" + str(emb_path))\n",
    "                # collect label values from df\n",
    "                labs = list(emb_df[col_label])\n",
    "                # put scalars in numpy arrays to fit keras output format\n",
    "                if np.isscalar(labs[0]):\n",
    "                    labs = [np.array(x) for x in labs if np.isscalar(x)]\n",
    "#                     print(\"isscalar after np.array: \" + str(np.isscalar(labs[0])))\n",
    "#                 for item in labs:\n",
    "#                     if np.isscalar(item):\n",
    "#                         item = np.array(item)\n",
    "                labels[col_label].extend(labs)\n",
    "            if verbose:\n",
    "                pbar.update()\n",
    "        return labels\n",
    "    \n",
    "    def generate_path(self, nested_dir_path, topic_id, col_label):\n",
    "        filename = str(topic_id) + '_' + str(col_label) + \".pickle\"  # e.g. 1_embeddings.pickle\n",
    "        path = os.path.join(nested_dir_path, filename)\n",
    "        return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Tuning Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNTrainer:\n",
    "    def __init__(self, proj_dir=None, nn_base_save_dir_name=None):\n",
    "        if proj_dir is None:\n",
    "            self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "        else:\n",
    "            self.proj_dir = proj_dir\n",
    "        self.input_handler = InputLabelHandler(self.proj_dir)\n",
    "        self.nn_base_save_dir_name = nn_base_save_dir_name\n",
    "        if self.nn_base_save_dir_name is None:\n",
    "            self.nn_base_save_dir_name = \"summarization_models\"\n",
    "        self.nn_base_save_dir_path = os.path.join(self.proj_dir, self.nn_base_save_dir_name)\n",
    "        self.nn_path_df_name = \"nn_path_df.hdf\"\n",
    "        self.nn_path_df_path = os.path.join(self.nn_base_save_dir_path, self.nn_path_df_name)\n",
    "        self.nn_path_df_cols = ['corpus_name', 'nested_dir', 'col_label', 'dir_path']\n",
    "        self.default_test_topics = [1,2,3,4,5,6,8,9,10]\n",
    "        self.default_train_ratio = 0.8\n",
    "        self.min_train_ratio = 0.5\n",
    "    \n",
    "    def train(self, corpus_names=None, nested_dirs=None, col_labels=None, tuning_iterations=100,\n",
    "              train_topics = None, test_topics=None, input_col_name=\"embedding\", batch_size=32,\n",
    "              train_ratio=None, force_reload=False, verbose=True):\n",
    "        \"\"\"\n",
    "        1. Generate Data if needed\n",
    "        2. Determine combinations to try\n",
    "        3. Load combination\n",
    "        4. Train network on it\n",
    "        5. Generate summary on test topics\n",
    "        5. Save tuned network, metrics, database entries\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "        self.nn_path_df = self.load_nn_path_df(verbose=verbose)\n",
    "        \n",
    "        # generate data\n",
    "        self.input_handler.generate(corpus_names=corpus_names, nested_dirs=nested_dirs, col_labels=col_labels,\n",
    "                                   force_reload=force_reload, verbose=verbose)\n",
    "        \n",
    "        # get our dataset identifiers, used to load correct inputs/labels\n",
    "        corpus_names, nested_dirs, col_labels = resolve_input_params(self.input_handler.path_ret,\n",
    "                                                                     corpus_names, nested_dirs, col_labels)\n",
    "        \n",
    "        # resolve train/test split\n",
    "        if train_ratio is None:\n",
    "            train_ratio = self.default_train_ratio\n",
    "        else:\n",
    "            if train_ratio < self.min_train_ratio:\n",
    "                raise Exception(\"Train ratio must be at least 0.5\")\n",
    "        corpus_topics = self.resolve_topics_per_corpus(corpus_names, train_topics, test_topics, train_ratio)\n",
    "        \n",
    "        combinations = find_combinations(self.nn_path_df, corpus_names, nested_dirs, col_labels, add_topics=False,\n",
    "                                        col_labels_as_list=False, as_tuples=True, force_reload=force_reload)\n",
    "        if len(combinations) > 0:\n",
    "            for corpus_name, nested_dir, col_label in tqdm_notebook(combinations):   \n",
    "                train_topics = corpus_topics[corpus_name]['train']\n",
    "                if verbose:\n",
    "                    print(\"corpus_name: \" + str(corpus_name) + \"\\n\"\n",
    "                         + \"nested_dir: \" + str(nested_dir) + \"\\n\"\n",
    "                         + \"X_input: \" + str(input_col_name) + \"\\n\"\n",
    "                         + \"y_labels: \" + str(col_label) + \"\\n\"\n",
    "                         + \"train_topics: \" + str(train_topics))\n",
    "\n",
    "                # get paths for inputs and total_len of samples\n",
    "                X_path_dict, total_len, input_dim = self.input_handler.get_paths(corpus_name, nested_dir, \n",
    "                                            input_col_name, topic_ids=train_topics, return_indices=True, \n",
    "                                            return_total_len=True, return_input_dim=True)\n",
    "                # get paths for labels\n",
    "                y_path_dict = self.input_handler.get_paths(corpus_name, nested_dir, col_label,\n",
    "                                            topic_ids=train_topics, return_indices=False, return_total_len=False)\n",
    "                \n",
    "                # ensure matching path dicts\n",
    "                if not self.input_handler.path_dicts_match(X_path_dict, y_path_dict):\n",
    "                    raise Exception(\"Path dicts do not match \\nX_path_dict: \" + str(X_path_dict)\n",
    "                                   +\"\\ny_path_dict: \" + str(y_path_dict))\n",
    "                \n",
    "                # create a generator to feed NN samples/batches\n",
    "                batch_generator = BatchGenerator(X_path_dict, y_path_dict, total_len, batch_size)\n",
    "                \n",
    "                save_dir, save_name = self.generate_nn_save_path(corpus_name, nested_dir, col_label,\n",
    "                                                                create_dir=True)\n",
    "                # generate optimised neural network\n",
    "                tuner = NNTuner(save_dir, save_name, tuning_iterations=tuning_iterations, \n",
    "                                input_dim=input_dim, force_reload=force_reload, batch_size=batch_size)\n",
    "\n",
    "                best_model_path = os.path.join(save_dir, \"best_model\")\n",
    "                best_model, best_hyperparams = tuner.search(batch_generator, save_path=best_model_path, \n",
    "                             return_hyperparams=True)\n",
    "\n",
    "                self.add_path_to_nn_path_df(corpus_name, nested_dir, input_col_name, col_label, save_dir,\n",
    "                                           save_name, best_hyperparams, best_model_path, verbose=verbose)\n",
    "            print(\"Finished tuning neural networks\")\n",
    "        else:\n",
    "            print(\"All neural networks have previously been tuned\")\n",
    "            \n",
    "    def resolve_topics_per_corpus(self, corpus_names, train_topics, test_topics, train_ratio):\n",
    "        \"\"\"Resolve the train/test corpus for each corpus\n",
    "        This is a bit off in logic\n",
    "        \"\"\"\n",
    "        corpus_topics_dict = defaultdict(dict)\n",
    "        for corpus_name in corpus_names:\n",
    "            corpus_topics = self.input_handler.corpus_topic_ids(corpus_name)\n",
    "            corp_test, corp_train = test_topics, train_topics\n",
    "            # resolve test_topics for corpus\n",
    "            if corp_test is None:\n",
    "                corp_test = self.default_test_topics\n",
    "            if corp_train is None:\n",
    "                corp_train = [x for x in corpus_topics if x not in corp_test]\n",
    "            \n",
    "            # get rid of repeats\n",
    "            corp_test, corp_train = set(corp_test), set(corp_train)\n",
    "            \n",
    "            cur_train_ratio = len(corpus_topics) / len(corp_train)\n",
    "            if cur_train_ratio < self.min_train_ratio:\n",
    "                # set to train_ratio instead\n",
    "                num_train = math.floor(len(corpus_topics) * train_ratio)\n",
    "                num_test = len(corpus_topics) - num_train\n",
    "                corp_test = corpus_topics[0:num_test]\n",
    "                corp_train = corpus_topics[num_test:]\n",
    "                \n",
    "            # check for overlap in train/test topics\n",
    "            if not corp_test.isdisjoint(corp_train):  # overlap between topics\n",
    "                raise Exception(\"Train and test sets contain overlapping topic_ids\\nTrain: \" + str(corp_train)\n",
    "                               +\"\\nTest: \" + str(corp_test))\n",
    "            \n",
    "            corp_train, corp_test = list(corp_train), list(corp_test)\n",
    "            corpus_topics_dict[corpus_name]['train'] = corp_train\n",
    "            corpus_topics_dict[corpus_name]['test'] = corp_test\n",
    "        return corpus_topics_dict\n",
    "                \n",
    "    \n",
    "    def load_nn_path_df(self, verbose=True):\n",
    "        if os.path.exists(self.nn_path_df_path):\n",
    "            nn_path_df = read_df_file_type(self.nn_path_df, verbose=verbose)\n",
    "        else:\n",
    "            nn_path_df = pd.DataFrame(columns=self.nn_path_df_cols)\n",
    "            if verbose:\n",
    "                print(\"nn_path_df created from scratch\")\n",
    "        return nn_path_df\n",
    "    \n",
    "    def add_to_nn_path_df(self, corpus_name, nested_dir, input_col_name, label_col_name, tuner_dir, tuner_name,\n",
    "                          best_hyperparams, best_model_path, verbose=True):\n",
    "        row = {\"corpus_name\":corpus_name, \"nested_dir\":nested_dir, \"input_col_name\":input_col_name,\n",
    "              \"label_col_names\":label_col_name, \"tuner_dir\":tuner_dir, \"tuner_name\":tuner_name,\n",
    "              \"best_hyperparams\":best_hyperparams, \"best_model_path\":best_model_path}\n",
    "        \n",
    "        self.nn_path_df = self.nn_path_df.append(row, ignore_index=True)\n",
    "        save_df_file_type(self.nn_path_df, self.nn_path_df_path, verbose=verbose)\n",
    "    \n",
    "    def generate_nn_save_path(self, corpus_name, nested_dir, col_labels, create_dir=True):\n",
    "        col_dir = \"_\".join(convert_to_list(col_labels))\n",
    "        dir_list = [self.nn_base_save_dir_path, corpus_name, nested_dir, col_dir]\n",
    "        # combine directories to form path of subdirectories, create dirs if necessary\n",
    "        dir_path = None\n",
    "        for cur_dir in dir_list:\n",
    "            if dir_path is None:  # first iteration\n",
    "                dir_path = dir_list[0]\n",
    "            else:\n",
    "                dir_path = os.path.join(dir_path, cur_dir)\n",
    "            if not os.path.exists(dir_path) and create_dir:\n",
    "                os.makedirs(dir_path)\n",
    "        # generate name\n",
    "        save_name = \"tuner_proj\"\n",
    "        return dir_path, save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_path_df created from scratch\n",
      "Loading label_path_df\n",
      "loaded from .hdf file\n",
      "Retrieving the following: cosine_similarity, cos_sim_nearest_nug, embedding\n",
      "Input/label combinations fully loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93034cc5b7d0486eb3dd7286dd411940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0210 12:15:32.571674 139778840557376 base_tuner.py:71] Reloading Oracle from existing project /nfs/proj-repo/AAARG-dissertation/summarization_models/mine-trects-kba2014-filtered/distilbert-base-nli-stsb-mean-tokens/cosine_similarity/tuner_proj/oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_name: mine-trects-kba2014-filtered\n",
      "nested_dir: distilbert-base-nli-stsb-mean-tokens\n",
      "X_input: embedding\n",
      "y_labels: cosine_similarity\n",
      "train_topics: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "num_layers        |1                 |?                 \n",
      "units_0           |769               |?                 \n",
      "activ_0           |sigmoid           |?                 \n",
      "learning_rate     |0.045979          |?                 \n",
      "loss_func         |binary_crossent...|?                 \n",
      "tuner/epochs      |3                 |?                 \n",
      "tuner/initial_e...|0                 |?                 \n",
      "tuner/bracket     |2                 |?                 \n",
      "tuner/round       |0                 |?                 \n",
      "\n",
      "idx: 0\n",
      "start/end_idx: 0 / 32\n",
      "Loading new topic_id 11 old topic_id None\n",
      "Epoch 1/3\n",
      "idx: 677336\n",
      "start/end_idx: 21674752 / 21674784\n",
      "Loading new topic_id 33 old topic_id 11\n",
      "idx: 557817\n",
      "start/end_idx: 17850144 / 17850176\n",
      "Loading new topic_id 25 old topic_id 11\n",
      "idx: 853814\n",
      "start/end_idx: 27322048 / 27322080\n",
      "Loading new topic_id 42 old topic_id 11\n",
      "idx: 878448\n",
      "start/end_idx: 28110336 / 28110368\n",
      "Loading new topic_id 43 old topic_id 11\n",
      "idx: 863406\n",
      "start/end_idx: 27628992 / 27629024\n",
      "Loading new topic_id 43 old topic_id 11\n",
      "idx: 50283\n",
      "start/end_idx: 1609056 / 1609088\n",
      "Loading new topic_id 13 old topic_id 11\n",
      "idx: 95426\n",
      "start/end_idx: 3053632 / 3053664\n",
      "Loading new topic_id 14 old topic_id 11\n",
      "idx: 844227\n",
      "start/end_idx: 27015264 / 27015296\n",
      "Loading new topic_id 42 old topic_id 11\n",
      "idx: 481731\n",
      "start/end_idx: 15415392 / 15415424\n",
      "Loading new topic_id 24 old topic_id 11\n",
      "idx: 41165\n",
      "start/end_idx: 1317280 / 1317312\n",
      "Loading new topic_id 13 old topic_id 11\n",
      "idx: 718842\n",
      "start/end_idx: 23002944 / 23002976\n",
      "Loading new topic_id 34 old topic_id 11\n",
      "idx: 756246\n",
      "start/end_idx: 24199872 / 24199904\n",
      "Loading new topic_id 35 old topic_id 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4f2c8330a90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-1288e0138991>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_names, nested_dirs, col_labels, tuning_iterations, train_topics, test_topics, input_col_name, batch_size, train_ratio, force_reload, verbose)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mbest_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 best_model, best_hyperparams = tuner.search(batch_generator, save_path=best_model_path, \n\u001b[0;32m---> 87\u001b[0;31m                              return_hyperparams=True)\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 self.add_path_to_nn_path_df(corpus_name, nested_dir, input_col_name, col_label, save_dir,\n",
      "\u001b[0;32m<ipython-input-4-2c7492772099>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, batch_generator, save_path, return_hyperparams)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"Find optimal model given dataset\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerastuner/tuners/hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tuner/epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'initial_epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tuner/initial_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHyperband\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerastuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'callbacks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kerastuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m    140\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mutli_processing is off because of tensorflow warning, might try tf.data\n",
    "\n",
    "topic_ids aren't being loaded sequentially it seems, see if there's issue with not loading linearly\n",
    "or if there's issue with how the indexes are being taken. Maybe order of topic_id in list even\n",
    "Maybe idx passed to get_item is batch index (i.e. so num_indexes = len(dataset) / self.batch_size) (probs true)\n",
    "(already changed idx to be * by batch_size)\n",
    "\n",
    "Might need to save generated files as batches (already edited class)\n",
    "Consider what happens when reach end of dataset and there isn't enough for a complete batch_size\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "corpus_names = [\"mine-trects-kba2014-filtered\"]\n",
    "\n",
    "trainer = NNTrainer()\n",
    "\n",
    "trainer.train(corpus_names=corpus_names, verbose=True, force_reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NNTrainer:\n",
    "#     def __init__(self, proj_dir=None):\n",
    "#         if proj_dir is None:\n",
    "#             self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "#         else:\n",
    "#             self.proj_dir = proj_dir\n",
    "#         self.paths = PathRetriever(self.proj_dir)\n",
    "#         self.model_path = self.proj_dir + \"/\" + \"test_nn\"\n",
    "        \n",
    "#     def train(self, force_reload=False):\n",
    "#         print(\"Creating NN\")\n",
    "#         if os.path.exists(self.model_path) and not force_reload:\n",
    "#             nn = tf.keras.models.load_model(self.model_path)\n",
    "#             print(\"loaded from file at \" + str(self.model_path))\n",
    "#         else:\n",
    "#             tuner = NNTuner()\n",
    "#             print(\"Getting X,y\")\n",
    "#             x,y = self.get_x_y()\n",
    "#             print(\"Fitting NN\")\n",
    "#             nn = tuner.search(inputs=x,labels=y, save_path=self.model_path)\n",
    "#             print(\"Completed fitting\")\n",
    "#         print(\"Comparing predictions\")\n",
    "#         results = self.compare_predict(nn)\n",
    "#         print(self.format_results(results))\n",
    "        \n",
    "#     def compare_predict(self, nn):\n",
    "#         knear = KNearest()\n",
    "#         k_sents = knear.get_k_nearest(k=20)\n",
    "#         results = []\n",
    "        \n",
    "#         embs = []\n",
    "#         for index, sent in k_sents.iterrows():\n",
    "#             emb = sent['embedding']\n",
    "#             embs.append(emb)\n",
    "#             result = [sent['cosine_similarity']]\n",
    "#             results.append(result)\n",
    "#         embs = np.asarray(embs)  # turn into matrix\n",
    "#         preds = nn.predict(embs)\n",
    "#         for result, pred in zip(results, preds):\n",
    "#             result.append(pred)\n",
    "        \n",
    "# #         for index, sent in k_sents.iterrows():\n",
    "# #             emb = sent['embedding']\n",
    "# # #             emb = np.expand_dims(emb, axis=1)\n",
    "# #             emb = (emb,)\n",
    "# # #             print(\"shape emb: \" + str(emb.shape))\n",
    "# #             results[str(emb)] = []\n",
    "# #             results[str(emb)].append(sent['cosine_similarity'])  # actual\n",
    "# #             pred = nn.predict(emb)\n",
    "# #             results[embedding].append(pred)  # prediction\n",
    "        \n",
    "#         return results\n",
    "    \n",
    "#     def format_results(self, results):\n",
    "#         outstr = \"\"\n",
    "#         for result in results:\n",
    "#             outstr += \"actual: \" + str(result[0]) + \"\\n\"\n",
    "#             outstr += \"pred: \" + str(result[1]) + \"\\n\"\n",
    "#             outstr += \"\\n\"\n",
    "# #             outstr += str(emb) + \"\\n\"\n",
    "# #             outstr += \"actual: \" + str(result[0]) + \"\\n\"\n",
    "# #             outstr += \"pred: \" + str(result[1]) + \"\\n\"\n",
    "# #             outstr += \"\\n\"\n",
    "#         return outstr\n",
    "        \n",
    "#     def get_x_y(self):\n",
    "#         corpus_name = \"original-trects-kba2014-filtered\"\n",
    "#         nested_dir = 'distilbert-base-nli-stsb-mean-tokens'\n",
    "#         x_y_paths = self.paths.get_embedding_paths(corpus_name, nested_dir)\n",
    "#         x = []\n",
    "#         y = []\n",
    "#         for path in tqdm_notebook(list(x_y_paths['path'])):\n",
    "#             emb_df = load_embeddings(path, verbose=False)\n",
    "#             emb_x = list(emb_df['embedding'])\n",
    "#             emb_y = list(emb_df['cosine_similarity'])\n",
    "# #             # keras compatibility wrap singular floats in ndarrays\n",
    "#             emb_y = [np.asarray(num) for num in emb_y]\n",
    "#             x.extend(emb_x)\n",
    "#             y.extend(emb_y)\n",
    "#         x = np.asarray(x)\n",
    "#         y = np.asarray(y)\n",
    "#         return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = NNTrainer()\n",
    "# trainer.train(force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNetwork:\n",
    "#     def __init__(self, nn_config, learning_rate=0.0001, input_dim=768, output_dim=1, loss_func=\"huber\",\n",
    "#                 epochs=10):\n",
    "#         # nn structure params\n",
    "#         self.nn_config = nn_config  # currently just list of layer sizes, can expand to include diff types layers\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.loss_func = loss_func\n",
    "        \n",
    "#         # nn fit execution params\n",
    "#         self.epochs = epochs\n",
    "        \n",
    "#         self.model = self.build_model()\n",
    "        \n",
    "#     def build_model(self):\n",
    "#         model = Sequential()\n",
    "#         ilayer = InputLayer(input_shape=(self.input_dim,))\n",
    "#         model.add(ilayer)\n",
    "#         for num_neurons in self.nn_config:\n",
    "#             # GRU has better memory performance\n",
    "#             # use tanh bc cos similarity is between -1 and 1\n",
    "#             model.add(Dense(num_neurons, activation='tanh'))  \n",
    "#         # output layer\n",
    "# #         model.add(Dense(self.output_dim, activation='tanh'))\n",
    "#         # build model\n",
    "#         opt = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "#         model.compile(loss=self.loss_func, optimizer=opt)\n",
    "#         return model\n",
    "    \n",
    "#     def fit(self, X, y=None, save_path=None):\n",
    "#         if y is not None:\n",
    "#             self.model.fit(x=X, y=y, epochs=self.epochs, verbose=1,\n",
    "#                           use_multiprocessing=True, workers=32)\n",
    "#         else:\n",
    "#             self.model.fit(x=X, epochs=self.epochs, verbose=1,\n",
    "#                           use_multiprocessing=True, workers=32)\n",
    "#         if save_path is not None:\n",
    "#             self.model.save(save_path)\n",
    "    \n",
    "#     def predict(self, s, a=None):              \n",
    "#         if a==None:            \n",
    "#             return self._predict_nn(s)\n",
    "#         else:                        \n",
    "#             return self._predict_nn(s)[a]\n",
    "        \n",
    "#     def _predict_nn(self,state_hat):                          \n",
    "#         \"\"\"\n",
    "#         Predict the output of the neural network (note: these can be vectors)\n",
    "#         \"\"\"                \n",
    "#         x = self.model.predict(state_hat)                                                    \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NNTrainer:\n",
    "#     def __init__(self, proj_dir=None):\n",
    "#         if proj_dir is None:\n",
    "#             self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "#         else:\n",
    "#             self.proj_dir = proj_dir\n",
    "#         self.paths = PathRetriever(self.proj_dir)\n",
    "#         self.nn_config = [752, 128]\n",
    "#         self.model_path = self.proj_dir + \"/\" + \"test_nn\"\n",
    "        \n",
    "#     def train(self, force_reload=False):\n",
    "#         print(\"Creating NN\")\n",
    "#         if os.path.exists(self.model_path) and not force_reload:\n",
    "#             nn = tf.keras.models.load_model(self.model_path)\n",
    "#             print(\"loaded from file at \" + str(self.model_path))\n",
    "#         else:\n",
    "#             nn = HyperNNs(self.nn_config, epochs=1)\n",
    "#             print(\"Getting X,y\")\n",
    "#             x,y = self.get_x_y()\n",
    "#             print(\"Fitting NN\")\n",
    "#             nn.fit(x,y=y, save_path=self.model_path)\n",
    "#             print(\"Completed fitting\")\n",
    "#         print(\"Comparing predictions\")\n",
    "#         results = self.compare_predict(nn)\n",
    "#         print(self.format_results(results))\n",
    "        \n",
    "#     def compare_predict(self, nn):\n",
    "#         knear = KNearest()\n",
    "#         k_sents = knear.get_k_nearest(k=20)\n",
    "#         results = []\n",
    "        \n",
    "#         embs = []\n",
    "#         for index, sent in k_sents.iterrows():\n",
    "#             emb = sent['embedding']\n",
    "#             embs.append(emb)\n",
    "#             result = [sent['cosine_similarity']]\n",
    "#             results.append(result)\n",
    "#         embs = np.asarray(embs)  # turn into matrix\n",
    "#         preds = nn.predict(embs)\n",
    "#         for result, pred in zip(results, preds):\n",
    "#             result.append(pred)\n",
    "        \n",
    "# #         for index, sent in k_sents.iterrows():\n",
    "# #             emb = sent['embedding']\n",
    "# # #             emb = np.expand_dims(emb, axis=1)\n",
    "# #             emb = (emb,)\n",
    "# # #             print(\"shape emb: \" + str(emb.shape))\n",
    "# #             results[str(emb)] = []\n",
    "# #             results[str(emb)].append(sent['cosine_similarity'])  # actual\n",
    "# #             pred = nn.predict(emb)\n",
    "# #             results[embedding].append(pred)  # prediction\n",
    "        \n",
    "#         return results\n",
    "    \n",
    "#     def format_results(self, results):\n",
    "#         outstr = \"\"\n",
    "#         for result in results:\n",
    "#             outstr += \"actual: \" + str(result[0]) + \"\\n\"\n",
    "#             outstr += \"pred: \" + str(result[1]) + \"\\n\"\n",
    "#             outstr += \"\\n\"\n",
    "# #             outstr += str(emb) + \"\\n\"\n",
    "# #             outstr += \"actual: \" + str(result[0]) + \"\\n\"\n",
    "# #             outstr += \"pred: \" + str(result[1]) + \"\\n\"\n",
    "# #             outstr += \"\\n\"\n",
    "#         return outstr\n",
    "        \n",
    "#     def get_x_y(self):\n",
    "#         corpus_name = \"original-trects-kba2014-filtered\"\n",
    "#         nested_dir = 'distilbert-base-nli-stsb-mean-tokens'\n",
    "#         x_y_paths = self.paths.get_embedding_paths(corpus_name, nested_dir)\n",
    "#         x = []\n",
    "#         y = []\n",
    "#         for path in tqdm_notebook(list(x_y_paths['path'])):\n",
    "#             emb_df = load_embeddings(path, verbose=False)\n",
    "#             emb_x = list(emb_df['embedding'])\n",
    "#             emb_y = list(emb_df['cosine_similarity'])\n",
    "# #             # keras compatibility wrap singular floats in ndarrays\n",
    "#             emb_y = [np.asarray(num) for num in emb_y]\n",
    "#             x.extend(emb_x)\n",
    "#             y.extend(emb_y)\n",
    "#         x = np.asarray(x)\n",
    "#         y = np.asarray(y)\n",
    "#         return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = NNTrainer()\n",
    "trainer.train(force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple K-Nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearest:\n",
    "    def __init__(self, proj_dir=None):\n",
    "        if proj_dir is None:\n",
    "            self.proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "        self.paths = PathRetriever(self.proj_dir)\n",
    "    \n",
    "    def get_k_nearest(self, k=10):\n",
    "        topic_ids = None\n",
    "        emb_paths = self.get_emb_paths(topic_ids=topic_ids)\n",
    "        \n",
    "        emb_df = []\n",
    "        for path in tqdm_notebook(list(emb_paths['path'])):\n",
    "            add_df = load_embeddings(path, verbose=False)\n",
    "#             debug = add_df[0:1]\n",
    "#             debug = list(debug['embedding'])[0]\n",
    "#             print(\"type emb: \" + str(type(debug)))\n",
    "#             print(\"emb shape: \" + str(debug.shape))\n",
    "            emb_df.append(load_embeddings(path, verbose=False))\n",
    "        emb_df = pd.concat(emb_df, ignore_index=True)\n",
    "\n",
    "        k_sents = emb_df.nlargest(k, columns=['cosine_similarity'])\n",
    "#         top_emb = k_sents.iloc[0]['embedding']\n",
    "#         print(top_emb)\n",
    "#         print(display(k_sents))\n",
    "        return k_sents\n",
    "        \n",
    "        \n",
    "#     def tokens_embs(self):\n",
    "#         \"\"\"Retrieve sentences and their embeddings\"\"\"\n",
    "#         emb_paths = self.get_emb_paths()\n",
    "#         toks = []\n",
    "#         embs = []\n",
    "        \n",
    "#         for path in tqdm_notebook(list(emb_paths['path'])):\n",
    "#             emb_df = load_embeddings(path, verbose=False)\n",
    "#             toks.extend(list(emb_df['sentence']))\n",
    "#             embs.extend(list(emb_df['embedding']))\n",
    "#         return toks, embs\n",
    "        \n",
    "            \n",
    "    def get_emb_paths(self, topic_ids=None):\n",
    "        corpus_name = \"original-trects-kba2014-filtered\"\n",
    "        nested_dir = 'distilbert-base-nli-stsb-mean-tokens'\n",
    "        emb_paths = self.paths.get_embedding_paths(corpus_name, nested_dir, topic_ids=topic_ids)\n",
    "        return emb_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knear = KNearest()\n",
    "knear.get_k_nearest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1,5,6,2,2])\n",
    "arr = np.expand_dimensions(arr)\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

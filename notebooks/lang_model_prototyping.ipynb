{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data set dependencies successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import sqlite3\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))  # sqlite3 won't accept int > 8 bytes, turns into blob datatype\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "import ipynb.fs\n",
    "print (\"loading data set dependencies successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET FILE META VARIABLES\n",
    "\n",
    "corpus_path = \"/nfs/trects-kba2014-filtered\" # directory of corpus of gzipped html files\n",
    "topics_path = corpus_path + \"/test-topics.xml\"\n",
    "doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "topic_tags = ['id', 'title', 'description', 'start','end','query','type'] # topic fields\n",
    "test_file_addr = corpus_path + \"/1/2012-02-22-15.gz\"\n",
    "proj_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "# csv file addresses\n",
    "corp_csv_name = 'corpus_loaded.csv.gz'\n",
    "corp_csv_path = proj_dir + '/' + corp_csv_name\n",
    "topics_csv_name = 'topics_loaded.csv.gz'\n",
    "topics_csv_path = proj_dir + '/' + topics_csv_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and get beautifulsoup object from markup file\n",
    "def open_markup_file(addr, gz=True, xml=False, verbose=False):\n",
    "    markup = None\n",
    "    f = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(addr)\n",
    "\n",
    "    if gz:\n",
    "        f = gzip.open(addr)\n",
    "    else:\n",
    "        f = open(addr)\n",
    "        \n",
    "    if xml == False:\n",
    "        markup = bs(f)  # open as html\n",
    "    else:\n",
    "        markup = bs(f, \"xml\")\n",
    "        \n",
    "    f.close()\n",
    "    return markup\n",
    "\n",
    "\n",
    "# parse markup and return 2D list [entry:tags]\n",
    "def parse_markup(markup, entry_list, find_tag=\"doc\", tag_list=doc_tags, topic_id=None):\n",
    "    for e in markup.find_all(find_tag):\n",
    "        entry = OrderedDict.fromkeys(tag_list)\n",
    "        if topic_id is not None:\n",
    "            entry['topic_id'] = topic_id\n",
    "        for c in e.children:  # children use direct children, descendants uses all\n",
    "            if c.name in entry:\n",
    "                entry[c.name] = str(c.string)\n",
    "            elif c.name is None and c.string != '\\n':  # inner body of <doc> tag\n",
    "                entry['text'] = str(c.string)\n",
    "        entry_list.append(list(entry.values()))\n",
    "        \n",
    "            \n",
    "# recursively find gz html files from a directory address\n",
    "def search_dir(path):    \n",
    "    # separate the subdirectories and html files \n",
    "    # (help maintain sequential order of insertion)\n",
    "    gz_paths = []\n",
    "    for f in os.scandir(path):\n",
    "        if os.path.splitext(f.path)[-1].lower() == \".gz\":\n",
    "            gz_paths.append(f.path)\n",
    "    \n",
    "    return gz_paths\n",
    "\n",
    "\n",
    "def list_to_dataframe(markup_list, tags):\n",
    "    return pd.DataFrame(markup_list, columns=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(path):\n",
    "    return os.path.isfile(path)\n",
    "\n",
    "def load_df_control(path, load_func, save=True, force_reload=False, compression='gzip'):\n",
    "    df = None\n",
    "    if not file_exists(path) or force_reload:\n",
    "        df = load_func()\n",
    "        print(\"df loaded\")\n",
    "        if save:\n",
    "            df.to_csv(path, compression=compression)\n",
    "            print(\"saved at: \" + str(path))\n",
    "    else:\n",
    "        df = pd.read_csv(path, compression=compression)\n",
    "        print(\"loaded from file\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from file\n"
     ]
    }
   ],
   "source": [
    "# load topics into dataframe\n",
    "def __load_topics(path):\n",
    "    topics_list = []\n",
    "    parse_markup(open_markup_file(path, gz=False, xml=True), \n",
    "                    topics_list, find_tag=\"event\", tag_list=topic_tags)\n",
    "    df = list_to_dataframe(topics_list, topic_tags)\n",
    "    df['id'] = pd.to_numeric(df['id'])\n",
    "    return df\n",
    "\n",
    "def load_topics(save=True, force_reload=False):\n",
    "    topics = load_df_control(topics_csv_path, lambda: load_topics(topics_path), save=save, force_reload=force_reload)\n",
    "    return topics\n",
    "\n",
    "topics = load_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics loaded successfuly\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>query</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012 Buenos Aires Rail Disaster</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2012_Buenos_Aires...</td>\n",
       "      <td>1329910380</td>\n",
       "      <td>1330774380</td>\n",
       "      <td>buenos aires train crash</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2012 Pakistan garment factory fires</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2012_Pakistan_gar...</td>\n",
       "      <td>1347368400</td>\n",
       "      <td>1348232400</td>\n",
       "      <td>pakistan factory fire</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2012 Aurora shooting</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2012_Aurora_shooting</td>\n",
       "      <td>1342766280</td>\n",
       "      <td>1343630280</td>\n",
       "      <td>colorado shooting</td>\n",
       "      <td>shooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Wisconsin Sikh temple shooting</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...</td>\n",
       "      <td>1344180300</td>\n",
       "      <td>1345044300</td>\n",
       "      <td>sikh temple shooting</td>\n",
       "      <td>shooting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id                                title  \\\n",
       "0           0   1      2012 Buenos Aires Rail Disaster   \n",
       "1           1   2  2012 Pakistan garment factory fires   \n",
       "2           2   3                 2012 Aurora shooting   \n",
       "3           3   4       Wisconsin Sikh temple shooting   \n",
       "\n",
       "                                         description       start         end  \\\n",
       "0  http://en.wikipedia.org/wiki/2012_Buenos_Aires...  1329910380  1330774380   \n",
       "1  http://en.wikipedia.org/wiki/2012_Pakistan_gar...  1347368400  1348232400   \n",
       "2  http://en.wikipedia.org/wiki/2012_Aurora_shooting  1342766280  1343630280   \n",
       "3  http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...  1344180300  1345044300   \n",
       "\n",
       "                      query      type  \n",
       "0  buenos aires train crash  accident  \n",
       "1     pakistan factory fire  accident  \n",
       "2         colorado shooting  shooting  \n",
       "3      sikh temple shooting  shooting  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics loaded successfuly\")\n",
    "print(display(topics[0:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Topics Into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed adding tables\n",
      "is_empty_table count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipynb.fs.defs.database_management_mysql:143: UserWarning: The topics table already has entries\n"
     ]
    }
   ],
   "source": [
    "from .defs.database_management_mysql import get_connection, create_tables, populate_topics  # import database_management functions\n",
    "conn, cursor = get_connection()\n",
    "create_tables(conn, cursor)\n",
    "populate_topics(conn, cursor, topics)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from file\n",
      "Corpus loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# load all formatted gzipped html files into dataframe\n",
    "def __load_corpus(path, doc_tags=doc_tags, save=True, force_reload=False):\n",
    "    df = pd.DataFrame(columns=doc_tags)\n",
    "    for topic_id in topics['id'].to_numpy():\n",
    "        print(\"Loading topic \" + str(topic_id) + \"...\")\n",
    "        topic_list = []\n",
    "        id_path = corpus_path + \"/\" + str(topic_id) + \"/\"  # every topic id correlates to subfolder named after it\n",
    "        gz_paths = search_dir(id_path)\n",
    "        for gz_path in tqdm(gz_paths, position=0, leave=True):\n",
    "            parse_markup(open_markup_file(gz_path, verbose=False),\n",
    "                            topic_list, topic_id=topic_id)\n",
    "        topic_df = list_to_dataframe(topic_list, doc_tags)\n",
    "        df = df.append(topic_df)\n",
    "    df['epoch'] = pd.to_numeric(df['epoch'])\n",
    "    return df\n",
    "\n",
    "def load_corpus(save=True, force_reload=False):\n",
    "    corpus = load_df_control(corp_csv_path, lambda: load_corpus(corpus_path), save=save, force_reload=force_reload)\n",
    "    return corpus\n",
    "\n",
    "corpus = load_corpus()\n",
    "\n",
    "print(\"Corpus loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded succesfully: 12261 documents loaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>streamid</th>\n",
       "      <th>docid</th>\n",
       "      <th>yyyymmddhh</th>\n",
       "      <th>kbastream</th>\n",
       "      <th>zulu</th>\n",
       "      <th>epoch</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1330269540-995ed81eafa60498872335da7dce1386</td>\n",
       "      <td>995ed81eafa60498872335da7dce1386</td>\n",
       "      <td>2012-02-26-15</td>\n",
       "      <td>news</td>\n",
       "      <td>2012-02-26T15:19:00.000000Z</td>\n",
       "      <td>1330269540</td>\n",
       "      <td>US says it's steadfast in rebuilding Afghanist...</td>\n",
       "      <td>\\nUS says it's steadfast in rebuilding Afghani...</td>\n",
       "      <td>http://www.elpasotimes.com/politics/ci_20049216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1330268520-f42a863b58b2cc53cc716953c40f6065</td>\n",
       "      <td>f42a863b58b2cc53cc716953c40f6065</td>\n",
       "      <td>2012-02-26-15</td>\n",
       "      <td>news</td>\n",
       "      <td>2012-02-26T15:02:00.000000Z</td>\n",
       "      <td>1330268520</td>\n",
       "      <td>Argentina Train Crash: Driver Blames Faulty Br...</td>\n",
       "      <td>\\nArgentina Train Crash: Driver Blames Faulty ...</td>\n",
       "      <td>http://www.thisdaylive.com/articles/argentina-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1330270020-e47e013ec518f5fdd253ce28231f509f</td>\n",
       "      <td>e47e013ec518f5fdd253ce28231f509f</td>\n",
       "      <td>2012-02-26-15</td>\n",
       "      <td>news</td>\n",
       "      <td>2012-02-26T15:27:00.000000Z</td>\n",
       "      <td>1330270020</td>\n",
       "      <td>The Alaska Journal of Commerce Local News Oil ...</td>\n",
       "      <td>\\nThe Alaska Journal of Commerce Local News Oi...</td>\n",
       "      <td>http://ap.alaskajournal.com/pstories/20120226/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1330268700-8078290575c82c8dd0e4e99370447bd2</td>\n",
       "      <td>8078290575c82c8dd0e4e99370447bd2</td>\n",
       "      <td>2012-02-26-15</td>\n",
       "      <td>news</td>\n",
       "      <td>2012-02-26T15:05:00.000000Z</td>\n",
       "      <td>1330268700</td>\n",
       "      <td>U.S. military receives remains of last soldier...</td>\n",
       "      <td>\\nU.S. military receives remains of last soldi...</td>\n",
       "      <td>http://www.islandpacket.com/2012/02/26/1978117...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  topic_id                                     streamid  \\\n",
       "0           0         1  1330269540-995ed81eafa60498872335da7dce1386   \n",
       "1           1         1  1330268520-f42a863b58b2cc53cc716953c40f6065   \n",
       "2           2         1  1330270020-e47e013ec518f5fdd253ce28231f509f   \n",
       "3           3         1  1330268700-8078290575c82c8dd0e4e99370447bd2   \n",
       "\n",
       "                              docid     yyyymmddhh kbastream  \\\n",
       "0  995ed81eafa60498872335da7dce1386  2012-02-26-15      news   \n",
       "1  f42a863b58b2cc53cc716953c40f6065  2012-02-26-15      news   \n",
       "2  e47e013ec518f5fdd253ce28231f509f  2012-02-26-15      news   \n",
       "3  8078290575c82c8dd0e4e99370447bd2  2012-02-26-15      news   \n",
       "\n",
       "                          zulu       epoch  \\\n",
       "0  2012-02-26T15:19:00.000000Z  1330269540   \n",
       "1  2012-02-26T15:02:00.000000Z  1330268520   \n",
       "2  2012-02-26T15:27:00.000000Z  1330270020   \n",
       "3  2012-02-26T15:05:00.000000Z  1330268700   \n",
       "\n",
       "                                               title  \\\n",
       "0  US says it's steadfast in rebuilding Afghanist...   \n",
       "1  Argentina Train Crash: Driver Blames Faulty Br...   \n",
       "2  The Alaska Journal of Commerce Local News Oil ...   \n",
       "3  U.S. military receives remains of last soldier...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nUS says it's steadfast in rebuilding Afghani...   \n",
       "1  \\nArgentina Train Crash: Driver Blames Faulty ...   \n",
       "2  \\nThe Alaska Journal of Commerce Local News Oi...   \n",
       "3  \\nU.S. military receives remains of last soldi...   \n",
       "\n",
       "                                                 url  \n",
       "0    http://www.elpasotimes.com/politics/ci_20049216  \n",
       "1  http://www.thisdaylive.com/articles/argentina-...  \n",
       "2  http://ap.alaskajournal.com/pstories/20120226/...  \n",
       "3  http://www.islandpacket.com/2012/02/26/1978117...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus loaded succesfully: \" + str(len(corpus)) + \" documents loaded.\")\n",
    "print(display(corpus[0:4]))\n",
    "# there is an error in the dataset that article at 1 is misplaced in topic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # duplicates are updates to the page\n",
    "# find_nug = corpus[corpus['streamid'] == \"1329915660-47ed792a77d798dda8697654e8fcbb43\"]\n",
    "# # 1329915300-46c9b2db03fbaf7d2a903bbfa7ff3c93-3\n",
    "# # duplicate found when -3 taken away\n",
    "# dup_nug = corpus[corpus['streamid'] == \"1329915300-46c9b2db03fbaf7d2a903bbfa7ff3c93\"]\n",
    "# print(corpus[corpus['docid'] == \"47ed792a77d798dda8697654e8fcbb43\"])\n",
    "# print(find_nug)\n",
    "# print(dup_nug)\n",
    "# # print(dup_nug['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Topic Modelling needs better preprocessing (stop words/lemmas etc.)\n",
    "    - stop words\n",
    "    - lemmatization (stemming is faster but is rule-based with more false transformations)\n",
    "    - special char removal\n",
    "- Could try removing junk at top of docs through REs/spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## IMPORT DEPENDENCIES\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# print(\"preprocessing dependencies import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")  # try experimenting disabling parts of spacy pipeline see if .sents still works\n",
    "\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_docs = corpus.loc[0:3,:]  # work on just top 3 for now\n",
    "\n",
    "# # in data frame, split sentences into list by the newline delimiter\n",
    "# #test_docs['text'] = test_docs['text'].map(lambda x: x.splitlines())\n",
    "\n",
    "# # map the non-preprocessed string list to a preprocessed string list\n",
    "\n",
    "# #@Tokenize\n",
    "# def spacy_tokenize(string):\n",
    "#     tokens = list()\n",
    "#     doc = nlp(string)\n",
    "#     for token in doc:\n",
    "#         if not token.is_stop:\n",
    "#             tokens.append(token)\n",
    "#     return tokens\n",
    "\n",
    "# #@Normalize\n",
    "# def normalize(tokens):\n",
    "#     normalized_tokens = list()\n",
    "#     for token in tokens:\n",
    "#         normalized = token.text.lower().strip()\n",
    "#         if ((token.is_alpha or token.is_digit)):\n",
    "#             normalized_tokens.append(normalized)\n",
    "#     return normalized_tokens\n",
    "\n",
    "# #@Tokenize and normalize\n",
    "# def tokenize_normalize(string):\n",
    "#     return normalize(spacy_tokenize(string))\n",
    "\n",
    "# # test_prep = []\n",
    "# # for doc in test_docs['text']:\n",
    "# #     d = []\n",
    "# #     for sent in doc:\n",
    "# #         d.append(tokenize_normalize(sent))\n",
    "# #     test_prep.append(d)\n",
    "        \n",
    "# print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_prep[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Word and Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# #sent_model = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "# sent_tokenizer = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# #word_model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "# word_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "- LDA uses K-means clustering\n",
    "- HDA learns num topics automatically (Bayesian non-parametric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # word level topic modelling\n",
    "# # needs better preprocessing (remove stopwords/lemmitization etc)\n",
    "# # maybe add REs/other preprocessing remove uninformative junk at top of docs\n",
    "\n",
    "# import gensim\n",
    "# from gensim import corpora\n",
    "\n",
    "# print(\"loaded dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TopicModeller: \n",
    "#     def __init__(self):\n",
    "#         self.model = None\n",
    "#         self.corpus_dict = None\n",
    "#         self.weighted_tokens = None\n",
    "#         self.print_topics = None\n",
    "        \n",
    "#     def weigh_tokens(self, texts, method=\"bow\"):\n",
    "#         \"\"\"Perform token weighting scheme on text and return with dict\"\"\"\n",
    "#         def create_dictionary(texts):\n",
    "#             \"\"\"Create a gensim dictionary of index-word mappings\"\"\"\n",
    "#             return corpora.Dictionary(texts)\n",
    "    \n",
    "#         flat_texts = [token for sent in texts for token in sent]  # should be fast\n",
    "#         self.corpus_dict = create_dictionary(flat_texts)\n",
    "#         if method == \"bow\":\n",
    "#             self.weighted_tokens = [self.corpus_dict.doc2bow(text) for text in flat_texts]\n",
    "#         else:\n",
    "#             raise Exception(\"Incorrect method parameter\")\n",
    "            \n",
    "#     def model_topics(self, method=\"lda\", num_topics=10):\n",
    "#         if method == \"lda\":\n",
    "#     #         model = gensim.models.ldamodel.LdaModel(weighted_tokens, num_topics=NUM_TOPICS, \n",
    "#     #                                                 id2word=corpus_dict, passes=15)\n",
    "#             self.model = gensim.models.ldamulticore.LdaMulticore(self.weighted_tokens, num_topics=num_topics, \n",
    "#                                                     id2word=self.corpus_dict, passes=15)\n",
    "#         else:\n",
    "#             raise Exception(\"Incorrect method parameter\")\n",
    "            \n",
    "#         self.print_topics = self.model.print_topics()\n",
    "#         return self.print_topics\n",
    "# print(\"loaded cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model = TopicModeller()\n",
    "# topic_model.weigh_tokens(test_prep)\n",
    "# print(topic_model.model_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first get sentences which are nearest neighbors to the identified topics\n",
    "# # https://scikit-learn.org/stable/modules/neighbors.html\n",
    "# # https://stackoverflow.com/questions/60996584/bert-embedding-for-semantic-similarity\n",
    "# # https://stackoverflow.com/questions/59865719/how-to-find-the-closest-word-to-a-vector-using-bert\n",
    "# # https://gist.github.com/avidale/c6b19687d333655da483421880441950\n",
    "\n",
    "\n",
    "# # then compare sentence results from pure extractive summariser maybe?\n",
    "\n",
    "# from sklearn.neighbors import KDTree\n",
    "# #import mxnet as mx\n",
    "# from bert_embedding import BertEmbedding\n",
    "\n",
    "# # ctx = mx.gpu(0)\n",
    "# # bert = BertEmbedding(ctx=ctx)\n",
    "# bert_emb = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Trying code from here\n",
    "# https://gist.github.com/avidale/c6b19687d333655da483421880441950\n",
    "\n",
    "# Preprocess embeddings in a formatted way as such can track sentences, words, embeddings\n",
    "\n",
    "# do this, then pass the LDA topics into the query\n",
    "# \"\"\" \n",
    "\n",
    "# class EmbeddingHandler:\n",
    "#     def __init__(self, sentences, model):\n",
    "#         self.sentences = sentences\n",
    "#         self.model = model\n",
    "        \n",
    "#     def generate_embeddings(self):\n",
    "#         result = self.model(self.sentences)\n",
    "# #         result = list()\n",
    "# #         for sent in self.sentences:\n",
    "# #             result.append(self.model.encode(sent, is_pretokenized=True))\n",
    "# #         #result = self.model.encode(self.sentences, is_pretokenized=True, show_progress_bar=True)\n",
    "#         #print(result)\n",
    "        \n",
    "#         self.sent_ids = []\n",
    "#         self.token_ids = []\n",
    "#         self.tokens = []\n",
    "#         embeddings = []\n",
    "#         for i, (toks, embs) in enumerate(tqdm(result)):\n",
    "#             for j, (tok, emb) in enumerate(zip(toks, embs)):\n",
    "#                 self.sent_ids.append(i)\n",
    "#                 self.token_ids.append(j)\n",
    "#                 self.tokens.append(tok)\n",
    "#                 embeddings.append(emb)\n",
    "#         embeddings = np.stack(embeddings)\n",
    "#         # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
    "#         self.normed_embeddings = (embeddings.T / (embeddings**2).sum(axis=1) ** 0.5).T\n",
    "        \n",
    "#     def generate_sent_embeddings(self):\n",
    "#         \"\"\"test sent vs word embeddings\"\"\"\n",
    "#         # use sentence-transformers embeddings\n",
    "#         result = self.model.encode(self.sentences)\n",
    "#         self.sent_ids = []\n",
    "#         self.tokens = []\n",
    "#         embeddings = []\n",
    "#         for i, (tok, emb) in enumerate(tqdm(zip(self.sentences,result))):\n",
    "#             self.sent_ids.append(i)\n",
    "#             self.tokens.append(tok)\n",
    "#             embeddings.append(emb)\n",
    "#         embeddings = np.stack(embeddings)\n",
    "#         # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
    "#         self.normed_embeddings = (embeddings.T / (embeddings**2).sum(axis=1) ** 0.5).T\n",
    "        \n",
    "#     def create_comparitor(self):\n",
    "#         # this takes some time\n",
    "#         self.indexer = KDTree(self.normed_embeddings)\n",
    "#         print(\"created KDTree\")\n",
    "    \n",
    "#     def query(self, query_sent, query_word, k=10, filter_same_word=False):\n",
    "#         toks, embs = self.model([query_sent])[0]\n",
    "\n",
    "#         found = False\n",
    "#         for tok, emb in zip(toks, embs):\n",
    "#             if tok == query_word:\n",
    "#                 found = True\n",
    "#                 break\n",
    "#         if not found:\n",
    "#             raise ValueError('The query word {} is not a single token in sentence {}'.format(query_word, toks))\n",
    "#         emb = emb / sum(emb**2)**0.5\n",
    "\n",
    "#         if filter_same_word:\n",
    "#             initial_k = max(k, 100)\n",
    "#         else:\n",
    "#             initial_k = k\n",
    "#         di, idx = self.indexer.query(emb.reshape(1, -1), k=initial_k)  # this is returning our neighbours\n",
    "#         distances = []\n",
    "#         neighbors = []\n",
    "#         contexts = []\n",
    "#         # this is filtering for word matching\n",
    "#         for i, index in enumerate(idx.ravel()):\n",
    "#             token = self.tokens[index]\n",
    "#             if filter_same_word and (query_word in token or token in query_word):  # take this away\n",
    "#                 continue\n",
    "#             distances.append(di.ravel()[i])\n",
    "#             neighbors.append(token)\n",
    "#             contexts.append(self.sentences[self.sent_ids[index]])\n",
    "#             if len(distances) == k:\n",
    "#                 break\n",
    "#         return distances, neighbors, contexts\n",
    "    \n",
    "#     def topic_neighbors(self, topic_word, k=10):\n",
    "#         # get average embedding of topic word\n",
    "#         # maybe instead return context sentence that is closest to averaged embedding?\n",
    "#         # that way can use context to get right meaning\n",
    "#         topic_emb = self.avg_embedding(self.retrieve_embeddings(topic_word))\n",
    "        \n",
    "#         # get neighbors\n",
    "#         # do I need reshape?\n",
    "#         di, idx = self.indexer.query(topic_emb.reshape(1,-1), k=k)\n",
    "#         distances = []\n",
    "#         neighbors = []\n",
    "#         contexts = []\n",
    "#         for i, index in enumerate(idx.ravel()):\n",
    "#             token = self.tokens[index]\n",
    "#             distances.append(di.ravel()[i])\n",
    "#             neighbors.append(token)\n",
    "#             contexts.append(self.sentences[self.sent_ids[index]])\n",
    "#         return distances, neighbors, contexts\n",
    "        \n",
    "        \n",
    "#     def retrieve_embeddings(self, token):\n",
    "#         idxs = []\n",
    "#         for i, t in enumerate(self.tokens):\n",
    "#             if t == token:\n",
    "#                 idxs.append(i)\n",
    "#             elif token in t:  # sent-embeddings temp workaround\n",
    "#                 idxs.append(i)\n",
    "#         embs = []\n",
    "#         for i in idxs:\n",
    "#             embs.append(self.normed_embeddings[i])\n",
    "#         return embs\n",
    "    \n",
    "#     def avg_embedding(self, emb_list):\n",
    "#         return np.mean(emb_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_handler = EmbeddingHandler(emb_handler_corp, bert_emb)  # [0] index taking first doco\n",
    "# emb_handler.generate_embeddings()\n",
    "# emb_handler.create_comparitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0107 12:20:15.007286 140183057614656 filelock.py:274] Lock 140182366188488 acquired on /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b325db3b41b4dcaad845c226cb018cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=434, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0107 12:20:15.425395 140183057614656 filelock.py:318] Lock 140182366188488 released on /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0107 12:20:18.944191 140183057614656 filelock.py:274] Lock 140177465659176 acquired on /root/.cache/torch/transformers/73e65a4648c1a5eab31ecea94e04a92a7168cd7089d588b68e5bc057aff40421.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c63b4ccc6564023b9b97f1610f8d8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=1344997306, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0107 12:20:35.279127 140183057614656 filelock.py:318] Lock 140177465659176 released on /root/.cache/torch/transformers/73e65a4648c1a5eab31ecea94e04a92a7168cd7089d588b68e5bc057aff40421.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0107 12:20:44.631939 140183057614656 filelock.py:274] Lock 140177274339224 acquired on /root/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3354a2cbdad741238460320c390c946a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0107 12:20:45.761188 140183057614656 filelock.py:318] Lock 140177274339224 released on /root/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded summarisation model\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "#from summarizer.coreference_handler import CoreferenceHandler\n",
    "#co_handler = CoreferenceHandler(greedyness=0.4)\n",
    "#sum_model = Summarizer(sentence_handler=co_handler)\n",
    "sum_model = Summarizer()\n",
    "print(\"loaded summarisation model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for processing massive strings\n",
    "#!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ValueError: [E088] Text of length 3496277 exceeds maximum of 1000000. \n",
    "The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. \n",
    "This means long texts may cause memory allocation errors. \n",
    "If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. \n",
    "The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n",
    "\"\"\"\n",
    "\n",
    "class SummarizationHandler:\n",
    "#     def __init__(self):\n",
    "# #         self.model = model\n",
    "        \n",
    "    def summarize(self, texts, max_len=500000, smallest_ratio=0.1):\n",
    "        def sum_texts(texts_list, ratio=None):\n",
    "            sums = []\n",
    "            print(\"Summarising text in \" + str(len(texts_list)) + \" pieces.\")\n",
    "            for text in tqdm(texts_list):\n",
    "                if ratio is None:\n",
    "                    sums.append(self.model(text))\n",
    "                else:\n",
    "                    sums.append(self.model(text, ratio=ratio))\n",
    "            return \". \".join(sums)\n",
    "        \n",
    "        def half_list(a_list):\n",
    "            half = len(a_list)//2\n",
    "            return a_list[:half], a_list[half:]\n",
    "        \n",
    "        self.model = Summarizer()  # try reset with a fresh model each time\n",
    "        \n",
    "        total_len = self.total_length(texts)\n",
    "        split_texts, size_split = self.split_texts(texts, total_len, max_len)\n",
    "        split_ratio = size_split / total_len\n",
    "        \n",
    "        new_splits = [split_texts]\n",
    "        new_ratios = [split_ratio]\n",
    "        while split_ratio < smallest_ratio:\n",
    "            new_ratios = []\n",
    "            n_s = []\n",
    "            for split in new_splits:  # loop for each list:\n",
    "                t1, t2 = half_list(split)\n",
    "                n_s.append(t1)\n",
    "                n_s.append(t2)\n",
    "            new_splits = n_s\n",
    "            for s in new_splits:\n",
    "                new_ratios.append(size_split/self.total_length(s))\n",
    "            split_ratio = min(new_ratios)\n",
    "        \n",
    "        split_sums = []\n",
    "        if new_ratios[0] <= 1:\n",
    "            for i in range(len(new_splits)):\n",
    "                split_sum = sum_texts(new_splits[i], ratio=new_ratios[i])\n",
    "                split_sums.append(split_sum)\n",
    "            s = \". \".join(split_sums)\n",
    "            s = sum_texts([s])\n",
    "            return s\n",
    "        else:\n",
    "            return sum_texts(new_splits[0])\n",
    "        \n",
    "#         if split_ratio >= 1:\n",
    "#             cur_sum = sum_texts(split_texts)\n",
    "#         else:\n",
    "#             sums = sum_texts(split_texts, split_ratio)\n",
    "#             cur_sum = \". \".join(sums)\n",
    "#             cur_sum = sum_texts([cur_sum])\n",
    "#         return cur_sum\n",
    "        \n",
    "    def split_texts(self, texts, total_len, max_len):\n",
    "        \"\"\"Split texts into list of strings under max_len\"\"\"\n",
    "        num_split, size_split = self.optimal_split(total_len, max_len)\n",
    "        splits = []\n",
    "        cur_split = \"\"\n",
    "        i = 1\n",
    "        for text in texts:\n",
    "            if i == num_split:\n",
    "                # just add to last no check\n",
    "                cur_split += text\n",
    "            else:\n",
    "                if (len(cur_split) + len(text) > size_split):\n",
    "                    splits.append(cur_split)\n",
    "                    cur_split = text\n",
    "                    i += 1\n",
    "                else:\n",
    "                    cur_split += text\n",
    "        splits.append(cur_split)\n",
    "        return splits, size_split\n",
    "            \n",
    "    def optimal_split(self, total_len, max_len):\n",
    "        \"\"\"Find even split of text under max_len\"\"\"\n",
    "        under_len = int(max_len * 0.95)  # use slightly under max_len for safety\n",
    "        cur_div = 1\n",
    "        cur_size = total_len / cur_div\n",
    "        while (cur_size > under_len):\n",
    "            cur_div += 1\n",
    "            cur_size = total_len / cur_div\n",
    "        return cur_div, cur_size\n",
    "        \n",
    "    \n",
    "    def total_length(self,texts):\n",
    "        total = 0\n",
    "        for t in texts:\n",
    "            total += len(t)\n",
    "        return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sum_handler = SummarizationHandler(sum_model)\n",
    "# sum_handler = SummarizationHandler()\n",
    "\n",
    "# test_topic = corpus.loc[corpus['topic_id'] == 1]['text'][0:10]\n",
    "# test_topic_sum = sum_handler.summarize(test_topic)\n",
    "\n",
    "# print(test_topic_sum)\n",
    "# print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading 'Nugget' Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nugget_dir = \"/nfs/TemporalSummarization/ts13/results\"\n",
    "updates_sampled_path = nugget_dir + \"/updates_sampled.tsv\"\n",
    "nuggets_path = nugget_dir + \"/nuggets.tsv\"\n",
    "nug_matches_path = nugget_dir + \"/matches.tsv\"\n",
    "# saving nugget and update files\n",
    "nugget_csv = 'nugget_df.csv.gz'\n",
    "update_csv = 'update_df.csv.gz'\n",
    "nugget_csv_path = proj_dir + '/' + nugget_csv\n",
    "update_csv_path = proj_dir + '/' + update_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_duplicates(df):\n",
    "    seen = set()\n",
    "    seen_twice = set()\n",
    "    for docid in df['docid']:\n",
    "        if docid not in seen:\n",
    "            seen.add(docid)\n",
    "        else:\n",
    "            seen_twice.add(docid)\n",
    "    return seen_twice\n",
    "\n",
    "def create_update_df():\n",
    "    \"\"\"Data Frame containing information about docs which have updates/multiple instances in corpus\"\"\"\n",
    "    def create_entry(row, col_tags):\n",
    "        entry = {}\n",
    "        for col in col_tags:\n",
    "            entry[col] = row[col]\n",
    "        return entry\n",
    "    \n",
    "    col_tags = ['docid', 'streamid', 'epoch', 'yyyymmddhh', 'zulu']\n",
    "    entry_list = []\n",
    "    dups = find_duplicates(corpus)\n",
    "    for docid in tqdm(dups, position=0, leave=True):\n",
    "        d = corpus[corpus['docid'] == docid]\n",
    "        for index, row in d.iterrows():\n",
    "            entry = create_entry(row, col_tags)\n",
    "            entry_list.append(entry)\n",
    "             \n",
    "    update_df = pd.DataFrame(entry_list)\n",
    "    update_df = update_df.set_index(col_tags)\n",
    "    return update_df\n",
    "                    \n",
    "                \n",
    "def create_nugget_df():\n",
    "    \"\"\"Dataframe containing nugget data and its appearances in corpus\"\"\"\n",
    "    def create_entry(row, reg_cols, multi_col_vals=None):\n",
    "        entry_dict = {}\n",
    "        for col in reg_cols:\n",
    "            entry_dict[col] = row[col]\n",
    "        if multi_cols is not None:\n",
    "            for k,v in multi_col_vals.items():\n",
    "                entry_dict[k] = v\n",
    "        return entry_dict\n",
    "    nuggets_tsv = pd.read_csv(nuggets_path, \"\\t\")\n",
    "    entry_list = []\n",
    "    reg_cols = ['query_id', 'nugget_id', 'importance', 'nugget_len', 'nugget_text']\n",
    "    multi_cols = ['docid', 'streamid', 'epoch', 'yyyymmddhh']  # multiindex cols\n",
    "    num_cols = ['query_id', 'importance', 'nugget_len', 'epoch']\n",
    "    \n",
    "    pbar = tqdm(total=len(nuggets_tsv), position=0, leave=True)\n",
    "    for index, row in nuggets_tsv.iterrows():\n",
    "        # find where nugget appears in text\n",
    "        nug_text = row['nugget_text']\n",
    "        topic_id = 0\n",
    "        try:\n",
    "            topic_id = int(row['query_id'])  # make sure pattern match in correct topic\n",
    "        except ValueError:\n",
    "            pbar.update()\n",
    "            continue  # topic_id is unknown string in tsv file, e.g. \"TS13.07\"\n",
    "        appears = corpus[corpus['topic_id'] == topic_id]\n",
    "        appears = appears[appears['text'].str.contains(re.escape(nug_text))]  # make sure no accidental regex pattern\n",
    "        \n",
    "        # gather information on docs it appears in\n",
    "        dups = find_duplicates(appears)  # get docids where nugget appears\n",
    "        for docid in dups:\n",
    "            upd = appears[appears['docid'] == docid]  # get docs with this docid\n",
    "            for i, r in upd.iterrows():  # gather info on each doc with this docid (e.g. streamid, epoch etc.)\n",
    "                multi_col_vals = {}\n",
    "                for multi_col in multi_cols:\n",
    "                    multi_col_vals[multi_col] = r[multi_col]\n",
    "                entry = create_entry(row, reg_cols, multi_col_vals=multi_col_vals)\n",
    "                entry_list.append(entry)\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "    \n",
    "    # form multi-index nugget dataframe\n",
    "    reg_cols.extend(multi_cols)  # get new multiindex order\n",
    "    nugget_df = pd.DataFrame(entry_list)\n",
    "    nugget_df[num_cols] = nugget_df[num_cols].apply(pd.to_numeric, errors='coerce', axis=1)  # convert appropriate cols to numerical values\n",
    "    nugget_df.rename(columns={'query_id':'topic_id'}, inplace=True)  # topic_id matches other dataframes\n",
    "    return nugget_df\n",
    "\n",
    "def load_nugget_df(save=True, force_reload=False):\n",
    "    nugget_df = load_df_control(nugget_csv_path, create_nugget_df, save=save, force_reload=force_reload)\n",
    "    return nugget_df\n",
    "\n",
    "def load_update_df(save=True, force_reload=False):\n",
    "    update_df = load_df_control(update_csv_path, create_update_df, save=save, force_reload=force_reload)\n",
    "    return update_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from file\n"
     ]
    }
   ],
   "source": [
    "nugget_df = load_nugget_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>nugget_id</th>\n",
       "      <th>importance</th>\n",
       "      <th>nugget_len</th>\n",
       "      <th>nugget_text</th>\n",
       "      <th>docid</th>\n",
       "      <th>streamid</th>\n",
       "      <th>epoch</th>\n",
       "      <th>yyyymmddhh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.052</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Hundreds injured</td>\n",
       "      <td>dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420</td>\n",
       "      <td>2012-02-23-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.052</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Hundreds injured</td>\n",
       "      <td>dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420</td>\n",
       "      <td>2012-02-23-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.054</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>February 22, 2012</td>\n",
       "      <td>f66f6668504592a391345e012800469c</td>\n",
       "      <td>1329944400-f66f6668504592a391345e012800469c</td>\n",
       "      <td>1329944400</td>\n",
       "      <td>2012-02-22-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.054</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>February 22, 2012</td>\n",
       "      <td>f66f6668504592a391345e012800469c</td>\n",
       "      <td>1329944400-f66f6668504592a391345e012800469c</td>\n",
       "      <td>1329944400</td>\n",
       "      <td>2012-02-22-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.054</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>February 22, 2012</td>\n",
       "      <td>ecda22bcfc10da137b49f0089bd5d7f5</td>\n",
       "      <td>1329916140-ecda22bcfc10da137b49f0089bd5d7f5</td>\n",
       "      <td>1329916140</td>\n",
       "      <td>2012-02-22-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  topic_id      nugget_id  importance  nugget_len  \\\n",
       "0           0         1  VMTS13.01.052           3           2   \n",
       "1           1         1  VMTS13.01.052           3           2   \n",
       "2           2         1  VMTS13.01.054           1           3   \n",
       "3           3         1  VMTS13.01.054           1           3   \n",
       "4           4         1  VMTS13.01.054           1           3   \n",
       "\n",
       "         nugget_text                             docid  \\\n",
       "0   Hundreds injured  dd95d5dbbff443c3ddae4e34a5d2e9c1   \n",
       "1   Hundreds injured  dd95d5dbbff443c3ddae4e34a5d2e9c1   \n",
       "2  February 22, 2012  f66f6668504592a391345e012800469c   \n",
       "3  February 22, 2012  f66f6668504592a391345e012800469c   \n",
       "4  February 22, 2012  ecda22bcfc10da137b49f0089bd5d7f5   \n",
       "\n",
       "                                      streamid       epoch     yyyymmddhh  \n",
       "0  1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1  1330041420  2012-02-23-23  \n",
       "1  1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1  1330041420  2012-02-23-23  \n",
       "2  1329944400-f66f6668504592a391345e012800469c  1329944400  2012-02-22-21  \n",
       "3  1329944400-f66f6668504592a391345e012800469c  1329944400  2012-02-22-21  \n",
       "4  1329916140-ecda22bcfc10da137b49f0089bd5d7f5  1329916140  2012-02-22-13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(nugget_df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from file\n"
     ]
    }
   ],
   "source": [
    "update_df = load_update_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>streamid</th>\n",
       "      <th>epoch</th>\n",
       "      <th>yyyymmddhh</th>\n",
       "      <th>zulu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372454-be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372454</td>\n",
       "      <td>2012-09-11-14</td>\n",
       "      <td>2012-09-11T14:07:34.0Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372718-be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372718</td>\n",
       "      <td>2012-09-11-14</td>\n",
       "      <td>2012-09-11T14:11:58.0Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372377-be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372377</td>\n",
       "      <td>2012-09-11-14</td>\n",
       "      <td>2012-09-11T14:06:17.0Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372289-be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372289</td>\n",
       "      <td>2012-09-11-14</td>\n",
       "      <td>2012-09-11T14:04:49.0Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372452-be52a6b7690622e7ffb5fc82928ae889</td>\n",
       "      <td>1347372452</td>\n",
       "      <td>2012-09-11-14</td>\n",
       "      <td>2012-09-11T14:07:32.0Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              docid  \\\n",
       "0  be52a6b7690622e7ffb5fc82928ae889   \n",
       "1  be52a6b7690622e7ffb5fc82928ae889   \n",
       "2  be52a6b7690622e7ffb5fc82928ae889   \n",
       "3  be52a6b7690622e7ffb5fc82928ae889   \n",
       "4  be52a6b7690622e7ffb5fc82928ae889   \n",
       "\n",
       "                                      streamid       epoch     yyyymmddhh  \\\n",
       "0  1347372454-be52a6b7690622e7ffb5fc82928ae889  1347372454  2012-09-11-14   \n",
       "1  1347372718-be52a6b7690622e7ffb5fc82928ae889  1347372718  2012-09-11-14   \n",
       "2  1347372377-be52a6b7690622e7ffb5fc82928ae889  1347372377  2012-09-11-14   \n",
       "3  1347372289-be52a6b7690622e7ffb5fc82928ae889  1347372289  2012-09-11-14   \n",
       "4  1347372452-be52a6b7690622e7ffb5fc82928ae889  1347372452  2012-09-11-14   \n",
       "\n",
       "                     zulu  \n",
       "0  2012-09-11T14:07:34.0Z  \n",
       "1  2012-09-11T14:11:58.0Z  \n",
       "2  2012-09-11T14:06:17.0Z  \n",
       "3  2012-09-11T14:04:49.0Z  \n",
       "4  2012-09-11T14:07:32.0Z  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(update_df[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Nuggets into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_empty_table count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipynb.fs.defs.database_management_mysql:164: UserWarning: The nuggets table already has entries\n"
     ]
    }
   ],
   "source": [
    "from .defs.database_management_mysql import populate_nuggets  # import database_management functions\n",
    "\n",
    "conn, cursor = get_connection()\n",
    "populate_nuggets(conn, cursor, nugget_df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricHandler:\n",
    "    def __init__(self, rouge_scores=['rouge1']):   \n",
    "#         self.summary = summary\n",
    "        self.rouge = rouge_scorer.RougeScorer(rouge_scores)\n",
    "        self.nugget_df = None\n",
    "        self.nugget_dict = None\n",
    "        self.streamids = None\n",
    "        \n",
    "    def evaluate_summary(self, summary, streamids, rouge=True, importance=True):\n",
    "        self.streamids = streamids\n",
    "        self.nugget_df = self.nugget_frame(streamids)  # store in self.nugget_frame\n",
    "        self.add_in_summary_col(summary)\n",
    "        # create nested dictionary of metrics\n",
    "        metrics = {}\n",
    "        if importance:\n",
    "#             sum_nugs = self.nuggets_in_summary(summary)\n",
    "#             found_nugs = self.nugget_df[self.nugget_df['in_summary'] == True]\n",
    "#             found_nugs = found_nugs.drop_duplicates('nugget_id')  # no over counting sums\n",
    "            cur_imp, total_imp = self.importance_score()\n",
    "            imp_dict = {}\n",
    "            imp_dict['cur_imp'] = cur_imp\n",
    "            imp_dict['total_imp'] = total_imp\n",
    "            metrics['importance'] = imp_dict\n",
    "        if rouge:\n",
    "            target_text = self.target_nugget_text()\n",
    "            rouges = self.rouge_score(target_text, summary)\n",
    "            for k,v in rouges.items():\n",
    "                r_dict = {}\n",
    "                for label, value in v._asdict().items():\n",
    "                    # keys: precision, recall, fmeasure\n",
    "                    r_dict[label] = value\n",
    "                metrics[k] = r_dict\n",
    "        return metrics, self.nugget_df  # return metrics and potential nuggets\n",
    "#         return metrics, sum_nugs  # return found nuggets to pass to db\n",
    "        \n",
    "    def rouge_score(self, target, summary):\n",
    "        scores = self.rouge.score(target, summary)\n",
    "        return scores\n",
    "        \n",
    "    def target_nugget_text(self, str_divider=\" \"):\n",
    "        t_nugs = list(self.nugget_df['nugget_text'])\n",
    "        t_nugs = str_divider.join(t_nugs)\n",
    "        return t_nugs\n",
    "    \n",
    "    def importance_score(self):\n",
    "        nugs = self.nugget_df.drop_duplicates('nugget_id')  # no over counting\n",
    "        total_score = nugs['importance'].sum() # potential score\n",
    "        found_nugs = nugs[nugs['in_summary'] == True]\n",
    "        cur_score = found_nugs['importance'].sum()  # actual summary score\n",
    "        return cur_score, total_score\n",
    "        \n",
    "#     def importance_score(self, sum_nugs):\n",
    "#         cur_score = sum_nugs['importance'].sum()  # actual summary score\n",
    "#         total_score = self.nugget_df['importance'].sum() # potential score\n",
    "#         return cur_score, total_score\n",
    "    \n",
    "    def add_in_summary_col(self, summary):\n",
    "#         self.nugget_df['in_summary'] = self.nugget_df['nugget_text'] in summary\n",
    "#         self.nugget_df['in_summary'] = self.nugget_df.apply(lambda x: x['nugget_text'] in summary, axis=1)\n",
    "        in_summary = self.nugget_df[self.nugget_df.apply(lambda x: x['nugget_text'] in summary, axis=1)]\n",
    "        in_summary = list(in_summary['nugget_id'])\n",
    "        self.nugget_df['in_summary'] = self.nugget_df['nugget_id'].isin(in_summary)\n",
    "        \n",
    "    def nuggets_in_summary(self, summary):\n",
    "        # filter where nugget_text is in summary\n",
    "        sum_nugs = self.nugget_df[self.nugget_df.apply(lambda x: x['nugget_text'] in summary, axis=1)]\n",
    "        return sum_nugs\n",
    "    \n",
    "    def nugget_frame(self, streamids, keep_columns=None):\n",
    "        if keep_columns is None:\n",
    "            keep_columns = ['nugget_id', 'importance', 'nugget_text']\n",
    "        # get nuggets for each streamid\n",
    "        nug_rows = nugget_df[nugget_df['streamid'].isin(streamids)]\n",
    "        nug_rows = nug_rows[keep_columns]\n",
    "        self.nugget_df = nug_rows\n",
    "        return self.nugget_df\n",
    "        \n",
    "#     def nugget_frame(self, streamids, keep_columns=None):\n",
    "#         if keep_columns is None:\n",
    "#             keep_columns = ['nugget_id', 'importance', 'nugget_text']\n",
    "#         # get nuggets for each streamid\n",
    "#         nug_rows = nugget_df[nugget_df['streamid'].isin(streamids)]\n",
    "# #         nug_rows = nug_rows.drop_duplicates('nugget_id')\n",
    "#         nug_rows = nug_rows[keep_columns]\n",
    "#         self.nugget_df = nug_rows\n",
    "#         return self.nugget_df\n",
    "    \n",
    "    def update_summary(self, summary):\n",
    "        self.summary = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_handler = MetricHandler()\n",
    "# metric_handler.evaluate_summary(test_nug_sum, test_nug['streamid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find difference in epoch for a day\n",
    "# def epoch_diff():\n",
    "#     # find instances in epoch where there is a day gap\n",
    "#     day_gap['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class SummaryFeeder:\n",
    "    \"\"\"\n",
    "    Need to add nugget metric info to database table and add from here\n",
    "    Then also add to nugget_instances table\n",
    "    \"\"\"\n",
    "    def __init__(self, sum_handler, tech_name, tech_descr=None, is_temporal=False):\n",
    "        self.sum_handler = sum_handler\n",
    "        self.tech_name = tech_name\n",
    "        self.is_temporal = is_temporal\n",
    "        self.tech_descr = tech_descr\n",
    "        self.metric_handler = MetricHandler()\n",
    "        \n",
    "    def summarize_topics(self, corp_df):\n",
    "        # pre-determine some columns to be inputted into meta table\n",
    "        self.meta_columns = self.get_meta_columns(corp_df)\n",
    "        # pre-summary database operations\n",
    "        self.conn, self.cursor = get_connection()\n",
    "        \n",
    "        self.insert_technique(tech_descr = self.tech_descr)\n",
    "        start_exec = self.cur_datetime()\n",
    "        instance_id = self.store_instance(start_exec)\n",
    "        self.conn.commit()\n",
    "        topic_ids = corp_df['topic_id'].unique()\n",
    "        for topic_id in tqdm(topic_ids, position=0, leave=True):\n",
    "            # currently sum all \n",
    "            topic_df = corp_df[corp_df['topic_id'] == topic_id]\n",
    "            summary = self.sum_handler.summarize(topic_df['text'])\n",
    "            self.store_topic(topic_id, instance_id, summary)\n",
    "            self.conn.commit()\n",
    "        \n",
    "        end_exec = self.cur_datetime()\n",
    "        self.update_end_exec(end_exec, instance_id)\n",
    "        self.conn.commit()\n",
    "        self.conn.close()\n",
    "        print(\"summarize_topics complete\")\n",
    "        \n",
    "    def store_topic(self, topic_id, instance_id, summary, is_complete_summary=True, update_num=None):\n",
    "        # get metrics\n",
    "        metrics, nuggets = self.metric_handler.evaluate_summary(summary, self.meta_columns[topic_id]['streamids'])\n",
    "        # store meta table\n",
    "        self.store_meta(topic_id, instance_id, summary, metrics)\n",
    "        # store nugget info\n",
    "        self.store_nugget_instances(topic_id, instance_id, nuggets, is_complete_summary=is_complete_summary, update_num=update_num)\n",
    "        \n",
    "    def store_nugget_instances(self, topic_id, instance_id, nuggets, is_complete_summary=True, update_num=None):\n",
    "        \"\"\"Store fields of nugget_instances table as follows:\n",
    "        nugget_id, topic_id, instance, technique, update_num, is_update, is_complete_summary, found\n",
    "        \"\"\"\n",
    "        def create_tuple(nugget_id, found):\n",
    "            base = [nugget_id, int(topic_id), int(instance_id), self.tech_name, is_complete_summary, found]\n",
    "            return tuple(base)\n",
    "        \n",
    "        insert_list = []\n",
    "        for index, row in nuggets.iterrows():\n",
    "            insert_list.append(create_tuple(row['nugget_id'], row['in_summary']))\n",
    "#         self.cursor.executemany('insert into nugget_instances values (?,?,?,?,?,?)', insert_list)  # not changed\n",
    "        self.cursor.executemany(\"\"\"INSERT INTO nugget_instances (nugget_id,  topic_id, instance, technique, is_complete_summary, found)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s)\"\"\", insert_list)\n",
    "        self.conn.commit()\n",
    " \n",
    "    def store_meta(self, topic_id, instance_id, summary, metrics):\n",
    "        \"\"\"Store fields of meta table as follows:\n",
    "        topic_id, instance, summary, streamids (text), epoch_start, epoch_end, importance_score,\n",
    "        total_importance, r1_precision, r1_recall, r1_fmeasure, has_updates\n",
    "        \"\"\"\n",
    "        cur_meta = self.meta_columns[topic_id]\n",
    "        imp = metrics['importance']\n",
    "        r1 = metrics['rouge1']\n",
    "        insert_tuple = (int(topic_id), int(instance_id), summary, \",\".join(cur_meta['streamids']), int(cur_meta['epoch_start']),\n",
    "                       int(cur_meta['epoch_end']), int(imp['cur_imp']), int(imp['total_imp']),\n",
    "                       r1['precision'], r1['recall'], r1['fmeasure'])\n",
    "        \n",
    "#         self.cursor.execute('insert into instance_meta values (?,?,?,?,?,?,?,?,?,?,?)', insert_tuple) # not changed    \n",
    "        self.cursor.execute(\"\"\"INSERT INTO instance_meta \n",
    "                            (topic_id, instance, summary, streamids, epoch_start, epoch_end, importance_score,\n",
    "                            total_importance, r1_precision, r1_recall, r1_fmeasure)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\", insert_tuple)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def get_meta_columns(self, corp_df):\n",
    "        meta_fields = {}\n",
    "        for topic_id in corp_df['topic_id'].unique():\n",
    "            nest = {}\n",
    "            t = corp_df[corp_df['topic_id'] == topic_id]\n",
    "            t = t.sort_values(\"epoch\")\n",
    "            nest['epoch_start'] = t['epoch'].iloc[0]\n",
    "            nest['epoch_end'] = t['epoch'].iloc[-1]\n",
    "            streamids = list(t['streamid'])\n",
    "#             streamids = \",\".join(streamids)\n",
    "            nest['streamids'] = streamids\n",
    "            meta_fields[topic_id] = nest\n",
    "        return meta_fields\n",
    "            \n",
    "    def store_instance(self, start_exec):\n",
    "        # get number of rows to get instance value\n",
    "        self.cursor.execute('SELECT COUNT(instance) FROM instances')  # not changed\n",
    "        rowcount = self.cursor.fetchone()[0]\n",
    "        \n",
    "        # insert instance\n",
    "        self.cursor.execute('INSERT INTO instances (instance, technique, temporal, start_exec) VALUES (%s, %s, %s, %s)',\n",
    "                           (rowcount, self.tech_name, self.is_temporal, start_exec))  # miss end_exec col\n",
    "        self.conn.commit()\n",
    "        return rowcount # return instance id for ease of later storage\n",
    "    \n",
    "    def update_end_exec(self, end_exec, instance_id):\n",
    "        self.cursor.execute('UPDATE instances SET end_exec = %s WHERE instance = %s', (end_exec, instance_id))\n",
    "        \n",
    "    def fetch_technique_entry(self):\n",
    "        self.cursor.execute('SELECT * FROM techniques WHERE name=%s', (self.tech_name,))\n",
    "        entry = self.cursor.fetchone()\n",
    "        return entry\n",
    "        \n",
    "    def insert_technique(self, tech_descr=None):\n",
    "        entry = self.fetch_technique_entry()\n",
    "        if entry:  # technique in database\n",
    "            print(\"Technique \" + str(self.tech_name) + \" in database\")\n",
    "            return False\n",
    "        else:\n",
    "            if tech_descr is None:\n",
    "                raise ValueError(\"Tech description must not equal none if technique not in database\")\n",
    "            else:\n",
    "                self.cursor.execute('INSERT INTO techniques (name, description) VALUES(%s, %s)', (self.tech_name, tech_descr))\n",
    "                self.conn.commit()\n",
    "                print(\"Technique \" + str(self.tech_name) + \" inserted into database\")\n",
    "                return True\n",
    "        \n",
    "    def cur_datetime(self):\n",
    "#         time = datetime.now().strftime(\"%B %d, %Y %I:%M%p\")\n",
    "        time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technique bes_naive_datasplit_[0:5] in database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (167) found smaller than n_clusters (205). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.65s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\u001b[A\n",
      " 11%|█         | 1/9 [00:30<04:05, 30.70s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (131) found smaller than n_clusters (173). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.95s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.08s/it]\u001b[A\n",
      " 22%|██▏       | 2/9 [00:52<03:15, 27.98s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (187) found smaller than n_clusters (221). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.36s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.50s/it]\u001b[A\n",
      " 33%|███▎      | 3/9 [01:18<02:44, 27.48s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (240) found smaller than n_clusters (447). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "100%|██████████| 1/1 [01:26<00:00, 86.53s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:26<00:00, 26.26s/it]\u001b[A\n",
      " 44%|████▍     | 4/9 [03:19<04:37, 55.58s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (525) found smaller than n_clusters (946). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "100%|██████████| 1/1 [05:45<00:00, 345.78s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [01:22<00:00, 82.36s/it]\u001b[A\n",
      " 56%|█████▌    | 5/9 [10:35<11:18, 169.70s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (285) found smaller than n_clusters (524). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "100%|██████████| 1/1 [01:45<00:00, 105.09s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:31<00:00, 31.55s/it]\u001b[A\n",
      " 67%|██████▋   | 6/9 [13:01<08:07, 162.40s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (223) found smaller than n_clusters (337). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "100%|██████████| 1/1 [00:57<00:00, 57.21s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:18<00:00, 18.01s/it]\u001b[A\n",
      " 78%|███████▊  | 7/9 [14:24<04:37, 138.65s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (273) found smaller than n_clusters (366). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "100%|██████████| 1/1 [01:09<00:00, 69.70s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.88s/it]\u001b[A\n",
      " 89%|████████▉ | 8/9 [15:59<02:05, 125.71s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (141) found smaller than n_clusters (180). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.56s/it]\u001b[A\n",
      "100%|██████████| 9/9 [16:22<00:00, 109.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarize_topics complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Full corpus caused float divison by zero error at new_ratios.append(size_split/self.total_length(s))\n",
    "\n",
    "def small_corpus(start_topic, end_topic):\n",
    "    t_dfs = []\n",
    "    topic_ids = corpus['topic_id'].unique()\n",
    "    for topic_id in topic_ids:\n",
    "        t_dfs.append(corpus[corpus['topic_id'] == topic_id][start_topic:end_topic])\n",
    "    small = pd.concat(t_dfs)\n",
    "    return small\n",
    "\n",
    "test_feeder_name = \"bes_naive_datasplit_[0:5]\"\n",
    "test_feeder_descr = \"\"\"\n",
    "Using bert-extractive_summarizer with original naive datasplit.\n",
    "Uses first 5 documents of each topic\n",
    "Split entire topic into portions above 0.1 ratio, summarise iteratively\n",
    "\"\"\"\n",
    "# test_feeder = SummaryFeeder(SummarizationHandler(sum_model), test_feeder_name, tech_descr=test_feeder_descr)\n",
    "test_feeder = SummaryFeeder(SummarizationHandler(), test_feeder_name, tech_descr=test_feeder_descr)\n",
    "\n",
    "first_20 = small_corpus(0, 5)\n",
    "test_feeder.summarize_topics(first_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "\n",
    "# db_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "# db_name = 'sumresults.db'\n",
    "# db_path = db_dir + '/' + db_name\n",
    "\n",
    "# conn = sqlite3.connect(db_path)  # creates db if doesn't exist\n",
    "# c = conn.cursor()  # allows send commands to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.execute(\"\"\"CREATE TABLE results (\n",
    "#     topic_id integer,\n",
    "#     summary text\n",
    "# )\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

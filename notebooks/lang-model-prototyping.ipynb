{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on current discoveries:\n",
    "\n",
    "- All files in storage claim are accessed with the base address '/nfs/'\n",
    "- Target files are HTML wrapped in .gz, use gzip to unzip\n",
    "- The inner text of <doc> is stored as the None tag where tag.string != '\\n' (is more than just a newline character\n",
    "- .string of <doc>'s inner text parses inner <a> tags etc. into just their inner text\n",
    "\n",
    "### Ideas for language model\n",
    "- NLTK\n",
    "- BERT\n",
    "\n",
    "- Load data set -> preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "successfully installed packages\n"
     ]
    }
   ],
   "source": [
    "# # !pip3 install spacy-langdetect\n",
    "# # !pip3 install language-detector\n",
    "# # !pip3 install symspellpy\n",
    "# !pip3 install beautifulsoup4 lxml  # html/xml parser\n",
    "# !pip3 install torch torchvision\n",
    "# !pip3 install neuralcoref\n",
    "# !pip3 install transformers\n",
    "# !pip3 install sentence-transformers\n",
    "\n",
    "# #!pip3 install -U spacy   # preprocessing\n",
    "# #!python -m spacy download en_core_web_sm  # spacy model\n",
    "# #!pip3 install numpy==1.18\n",
    "\n",
    "# # !pip3 install spacy==2.1.3\n",
    "# # !pip3 install transformers==2.2.2\n",
    "# # !pip3 install neuralcoref\n",
    "\n",
    "# #!pip3 install tensorflow --upgrade  # for extractive summerizar\n",
    "# !pip3 install tensorflow\n",
    "# #!pip3 install bert-extractive-summarizer\n",
    "\n",
    "#!python -m spacy download en_core_web_md\n",
    "print(\"\\nsuccessfully installed packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data set dependencies successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "print (\"loading data set dependencies successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET FILE META VARIABLES\n",
    "\n",
    "corpus_path = \"/nfs/trects-kba2014-filtered\" # directory of corpus of gzipped html files\n",
    "topics_path = corpus_path + \"/test-topics.xml\"\n",
    "doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "topic_tags = ['id', 'title', 'description', 'start','end','query','type'] # topic fields\n",
    "test_file_addr = corpus_path + \"/1/2012-02-22-15.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and get beautifulsoup object from markup file\n",
    "def open_markup_file(addr, gz=True, xml=False, verbose=False):\n",
    "    markup = None\n",
    "    f = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(addr)\n",
    "\n",
    "    if gz:\n",
    "        f = gzip.open(addr)\n",
    "    else:\n",
    "        f = open(addr)\n",
    "        \n",
    "    if xml == False:\n",
    "        markup = bs(f)  # open as html\n",
    "    else:\n",
    "        markup = bs(f, \"xml\")\n",
    "        \n",
    "    f.close()\n",
    "    return markup\n",
    "\n",
    "\n",
    "# parse markup and return 2D list [entry:tags]\n",
    "def parse_markup(markup, entry_list, find_tag=\"doc\", tag_list=doc_tags, topic_id=None):\n",
    "    for e in markup.find_all(find_tag):\n",
    "        entry = OrderedDict.fromkeys(tag_list)\n",
    "        if topic_id is not None:\n",
    "            entry['topic_id'] = topic_id\n",
    "        for c in e.children:  # children use direct children, descendants uses all\n",
    "            if c.name in entry:\n",
    "                entry[c.name] = c.string\n",
    "            elif c.name is None and c.string != '\\n':  # inner body of <doc> tag\n",
    "                entry['text'] = c.string\n",
    "        entry_list.append(list(entry.values()))\n",
    "        \n",
    "            \n",
    "# recursively find gz html files from a directory address\n",
    "def search_dir(path):    \n",
    "    # separate the subdirectories and html files \n",
    "    # (help maintain sequential order of insertion)\n",
    "    gz_paths = []\n",
    "    for f in os.scandir(path):\n",
    "        if os.path.splitext(f.path)[-1].lower() == \".gz\":\n",
    "            gz_paths.append(f.path)\n",
    "    \n",
    "    return gz_paths\n",
    "\n",
    "\n",
    "def list_to_dataframe(markup_list, tags):\n",
    "    return pd.DataFrame(markup_list, columns=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topics into dataframe\n",
    "def load_topics(path):\n",
    "    topics_list = []\n",
    "    \n",
    "    parse_markup(open_markup_file(path, gz=False, xml=True), \n",
    "                    topics_list, find_tag=\"event\", tag_list=topic_tags)\n",
    "    \n",
    "    \n",
    "    return  list_to_dataframe(topics_list, topic_tags)\n",
    "\n",
    "topics = load_topics(topics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics loaded successfuly\n",
      "  id                                title  \\\n",
      "0  1      2012 Buenos Aires Rail Disaster   \n",
      "1  2  2012 Pakistan garment factory fires   \n",
      "2  3                 2012 Aurora shooting   \n",
      "3  4       Wisconsin Sikh temple shooting   \n",
      "\n",
      "                                         description       start         end  \\\n",
      "0  http://en.wikipedia.org/wiki/2012_Buenos_Aires...  1329910380  1330774380   \n",
      "1  http://en.wikipedia.org/wiki/2012_Pakistan_gar...  1347368400  1348232400   \n",
      "2  http://en.wikipedia.org/wiki/2012_Aurora_shooting  1342766280  1343630280   \n",
      "3  http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...  1344180300  1345044300   \n",
      "\n",
      "                      query      type  \n",
      "0  buenos aires train crash  accident  \n",
      "1     pakistan factory fire  accident  \n",
      "2         colorado shooting  shooting  \n",
      "3      sikh temple shooting  shooting  \n"
     ]
    }
   ],
   "source": [
    "print(\"Topics loaded successfuly\")\n",
    "print(topics.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:19<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# load all formatted gzipped html files into dataframe\n",
    "def load_corpus(path):\n",
    "    corpus_list = []\n",
    "    gz_paths = []\n",
    "    for topic_id in topics['id'].to_numpy():\n",
    "        id_path = corpus_path + \"/\" + topic_id + \"/\"  # every topic id correlates to subfolder named after it\n",
    "        gz_paths = search_dir(id_path)\n",
    "    for gz_path in tqdm(gz_paths, position=0, leave=True):\n",
    "        parse_markup(open_markup_file(gz_path, verbose=False),\n",
    "                        corpus_list, topic_id=topic_id)\n",
    "    return list_to_dataframe(corpus_list, doc_tags)\n",
    "\n",
    "corpus = load_corpus(corpus_path)\n",
    "#print(\"Corpus loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded succesfully: 1578 documents loaded.\n",
      "  topic_id                                     streamid  \\\n",
      "0       10  1354113657-a4417f055ea5ae84207a4edb4dad881b   \n",
      "1       10  1354112039-110cc86ea7a8a1b58306dfade5b300ec   \n",
      "2       10  1354114192-a4417f055ea5ae84207a4edb4dad881b   \n",
      "3       10  1354114426-6c8d58d994c0e3243ee8dca8f34516a4   \n",
      "\n",
      "                              docid     yyyymmddhh        kbastream  \\\n",
      "0  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "1  110cc86ea7a8a1b58306dfade5b300ec  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "2  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "3  6c8d58d994c0e3243ee8dca8f34516a4  2012-11-28-14           WEBLOG   \n",
      "\n",
      "                     zulu       epoch  \\\n",
      "0  2012-11-28T14:40:57.0Z  1354113657   \n",
      "1  2012-11-28T14:13:59.0Z  1354112039   \n",
      "2  2012-11-28T14:49:52.0Z  1354114192   \n",
      "3  2012-11-28T14:53:46.0Z  1354114426   \n",
      "\n",
      "                                               title  \\\n",
      "0  Morning Briefing: Support grows for bid by Pal...   \n",
      "1  BBC News - Egypt crisis: Appeals courts launch...   \n",
      "2  Morning Briefing: Support grows for bid by Pal...   \n",
      "3  Egypt News - Bodybuilder sets record for world...   \n",
      "\n",
      "                                                text  \\\n",
      "0  \\nMorning Briefing: Support grows for bid by P...   \n",
      "1  \\nBBC News - Egypt crisis: Appeals courts laun...   \n",
      "2  \\nMorning Briefing: Support grows for bid by P...   \n",
      "3  \\nEgypt News - Bodybuilder sets record for wor...   \n",
      "\n",
      "                                                 url  \n",
      "0  http://www.theglobeandmail.com/news/morning-br...  \n",
      "1  http://www.bbc.co.uk/news/world-middle-east-20...  \n",
      "2  http://www.theglobeandmail.com/news/morning-br...  \n",
      "3  http://www.egyptnews.net/index.php/sid/2110165...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus loaded succesfully: \" + str(len(corpus)) + \" documents loaded.\")\n",
    "print(corpus.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_file_df = list_to_dataframe(parse_markup(open_markup_file(test_file_addr)), doc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Notes\n",
    "\n",
    "####Links\n",
    "-  https://stackoverflow.com/questions/54938815/data-preprocessing-for-nlp-pre-training-models-e-g-elmo-bert\n",
    "- https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/\n",
    "\n",
    "####Notes\n",
    "- Thinking of using spacy as it provides faster simpler preprocessing options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "# import re\n",
    "# import matplotlib.pyplot as plt\n",
    "# from nltk.corpus import stopwords \n",
    "\n",
    "# import spacy\n",
    "\n",
    "# #from stop_words import get_stop_words\n",
    "# ##from nltk.stem.porter import PorterStemmer\n",
    "# import re\n",
    "# #import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from language_detector import detect_language\n",
    "\n",
    "#import pkg_resources\n",
    "#from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "print(\"preprocessing dependencies import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\")  # en_core_web_lg nearly 80x larger, marginal differences\n",
    "\n",
    "# was in TaD solution\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## LOAD SPELL CHECKER\n",
    "\n",
    "# sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "# dictionary_path = pkg_resources.resource_filename(\n",
    "#     \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# if sym_spell.word_count:\n",
    "#     pass\n",
    "# else:\n",
    "#     sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "## SENTENCE LEVEL PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@Tokenize\n",
    "# def spacy_tokenize(string):\n",
    "#   tokens = list()\n",
    "#   doc = nlp(string)\n",
    "#   for token in doc:\n",
    "#     tokens.append(token)\n",
    "#   return tokens\n",
    "\n",
    "# #@Normalize\n",
    "# def normalize(tokens):\n",
    "#   normalized_tokens = list()\n",
    "#   for token in tokens:\n",
    "#     normalized = token.text.lower().strip()\n",
    "#     if ((token.is_alpha or token.is_digit)):\n",
    "#       normalized_tokens.append(normalized)\n",
    "#   return normalized_tokens\n",
    "#   return normalized_tokens\n",
    "\n",
    "# #@Tokenize and normalize\n",
    "# def tokenize_normalize(string):\n",
    "#   return normalize(spacy_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenize_normalize(\"this string right here is an example, I'm just testing what spacy does\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-dbcd88385343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d023f447c82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-nli-mean-tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_sentence_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "sent_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "test_sentences = corpus['text'].iloc(0).item()\n",
    "\n",
    "test_sentence_embeddings = sent_model.encode(test_sentences)\n",
    "\n",
    "for sentence, embedding in zip(sentences, test_sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "### bert-extractive-summarizer is causing import/dependecy issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1110 15:30:34.615583 140009003521856 configuration_utils.py:160] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /root/.cache/torch/transformers/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "I1110 15:30:34.616859 140009003521856 configuration_utils.py:177] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1110 15:30:35.012464 140009003521856 modeling_utils.py:401] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "I1110 15:30:41.604715 140009003521856 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /root/.cache/torch/transformers/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "<pandas.core.indexing._iLocIndexer object at 0x7f551345cef8>\n"
     ]
    }
   ],
   "source": [
    "sum_model = Summarizer()\n",
    "print(\"loaded model\")\n",
    "# test_sum_text = str(corpus['text'].iloc(0))\n",
    "# test_sum = sum_model(test_sum_text, min_length=10)\n",
    "# print(\"\".join(test_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This repo is the generalization of the lecture-summarizer repo. The greedyness of the neuralcoref library can be tweaked in the CoreferenceHandler class.\n"
     ]
    }
   ],
   "source": [
    "#test_sum_text = corpus['text'].iloc(0)\n",
    "test_sum_text = \"This repo is the generalization of the lecture-summarizer repo. This tool utilizes the HuggingFace Pytorch transformers library to run extractive summarizations. This works by first embedding the sentences, then running a clustering algorithm, finding the sentences that are closest to the cluster's centroids. This library also uses coreference techniques, utilizing the https://github.com/huggingface/neuralcoref library to resolve words in summaries that need more context. The greedyness of the neuralcoref library can be tweaked in the CoreferenceHandler class.\"\n",
    "test_sum = sum_model(test_sum_text, min_length=1)\n",
    "print(\"\".join(test_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

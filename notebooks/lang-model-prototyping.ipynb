{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nsuccessfully installed packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data set dependencies successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "print (\"loading data set dependencies successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET FILE META VARIABLES\n",
    "\n",
    "corpus_path = \"/nfs/trects-kba2014-filtered\" # directory of corpus of gzipped html files\n",
    "topics_path = corpus_path + \"/test-topics.xml\"\n",
    "doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "topic_tags = ['id', 'title', 'description', 'start','end','query','type'] # topic fields\n",
    "test_file_addr = corpus_path + \"/1/2012-02-22-15.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and get beautifulsoup object from markup file\n",
    "def open_markup_file(addr, gz=True, xml=False, verbose=False):\n",
    "    markup = None\n",
    "    f = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(addr)\n",
    "\n",
    "    if gz:\n",
    "        f = gzip.open(addr)\n",
    "    else:\n",
    "        f = open(addr)\n",
    "        \n",
    "    if xml == False:\n",
    "        markup = bs(f)  # open as html\n",
    "    else:\n",
    "        markup = bs(f, \"xml\")\n",
    "        \n",
    "    f.close()\n",
    "    return markup\n",
    "\n",
    "\n",
    "# parse markup and return 2D list [entry:tags]\n",
    "def parse_markup(markup, entry_list, find_tag=\"doc\", tag_list=doc_tags, topic_id=None):\n",
    "    for e in markup.find_all(find_tag):\n",
    "        entry = OrderedDict.fromkeys(tag_list)\n",
    "        if topic_id is not None:\n",
    "            entry['topic_id'] = topic_id\n",
    "        for c in e.children:  # children use direct children, descendants uses all\n",
    "            if c.name in entry:\n",
    "                entry[c.name] = c.string\n",
    "            elif c.name is None and c.string != '\\n':  # inner body of <doc> tag\n",
    "                entry['text'] = str(c.string)\n",
    "        entry_list.append(list(entry.values()))\n",
    "        \n",
    "            \n",
    "# recursively find gz html files from a directory address\n",
    "def search_dir(path):    \n",
    "    # separate the subdirectories and html files \n",
    "    # (help maintain sequential order of insertion)\n",
    "    gz_paths = []\n",
    "    for f in os.scandir(path):\n",
    "        if os.path.splitext(f.path)[-1].lower() == \".gz\":\n",
    "            gz_paths.append(f.path)\n",
    "    \n",
    "    return gz_paths\n",
    "\n",
    "\n",
    "def list_to_dataframe(markup_list, tags):\n",
    "    return pd.DataFrame(markup_list, columns=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topics into dataframe\n",
    "def load_topics(path):\n",
    "    topics_list = []\n",
    "    \n",
    "    parse_markup(open_markup_file(path, gz=False, xml=True), \n",
    "                    topics_list, find_tag=\"event\", tag_list=topic_tags)\n",
    "    \n",
    "    \n",
    "    return  list_to_dataframe(topics_list, topic_tags)\n",
    "\n",
    "topics = load_topics(topics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics loaded successfuly\n",
      "  id                                title  \\\n",
      "0  1      2012 Buenos Aires Rail Disaster   \n",
      "1  2  2012 Pakistan garment factory fires   \n",
      "2  3                 2012 Aurora shooting   \n",
      "3  4       Wisconsin Sikh temple shooting   \n",
      "\n",
      "                                         description       start         end  \\\n",
      "0  http://en.wikipedia.org/wiki/2012_Buenos_Aires...  1329910380  1330774380   \n",
      "1  http://en.wikipedia.org/wiki/2012_Pakistan_gar...  1347368400  1348232400   \n",
      "2  http://en.wikipedia.org/wiki/2012_Aurora_shooting  1342766280  1343630280   \n",
      "3  http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...  1344180300  1345044300   \n",
      "\n",
      "                      query      type  \n",
      "0  buenos aires train crash  accident  \n",
      "1     pakistan factory fire  accident  \n",
      "2         colorado shooting  shooting  \n",
      "3      sikh temple shooting  shooting  \n"
     ]
    }
   ],
   "source": [
    "print(\"Topics loaded successfuly\")\n",
    "print(topics.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:17<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# load all formatted gzipped html files into dataframe\n",
    "def load_corpus(path):\n",
    "    corpus_list = []\n",
    "    gz_paths = []\n",
    "    for topic_id in topics['id'].to_numpy():\n",
    "        id_path = corpus_path + \"/\" + topic_id + \"/\"  # every topic id correlates to subfolder named after it\n",
    "        gz_paths = search_dir(id_path)\n",
    "    for gz_path in tqdm(gz_paths, position=0, leave=True):\n",
    "        parse_markup(open_markup_file(gz_path, verbose=False),\n",
    "                        corpus_list, topic_id=topic_id)\n",
    "    return list_to_dataframe(corpus_list, doc_tags)\n",
    "\n",
    "corpus = load_corpus(corpus_path)\n",
    "#print(\"Corpus loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded succesfully: 1578 documents loaded.\n",
      "  topic_id                                     streamid  \\\n",
      "0       10  1354113657-a4417f055ea5ae84207a4edb4dad881b   \n",
      "1       10  1354112039-110cc86ea7a8a1b58306dfade5b300ec   \n",
      "2       10  1354114192-a4417f055ea5ae84207a4edb4dad881b   \n",
      "3       10  1354114426-6c8d58d994c0e3243ee8dca8f34516a4   \n",
      "\n",
      "                              docid     yyyymmddhh        kbastream  \\\n",
      "0  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "1  110cc86ea7a8a1b58306dfade5b300ec  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "2  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "3  6c8d58d994c0e3243ee8dca8f34516a4  2012-11-28-14           WEBLOG   \n",
      "\n",
      "                     zulu       epoch  \\\n",
      "0  2012-11-28T14:40:57.0Z  1354113657   \n",
      "1  2012-11-28T14:13:59.0Z  1354112039   \n",
      "2  2012-11-28T14:49:52.0Z  1354114192   \n",
      "3  2012-11-28T14:53:46.0Z  1354114426   \n",
      "\n",
      "                                               title  \\\n",
      "0  Morning Briefing: Support grows for bid by Pal...   \n",
      "1  BBC News - Egypt crisis: Appeals courts launch...   \n",
      "2  Morning Briefing: Support grows for bid by Pal...   \n",
      "3  Egypt News - Bodybuilder sets record for world...   \n",
      "\n",
      "                                                text  \\\n",
      "0  \\nMorning Briefing: Support grows for bid by P...   \n",
      "1  \\nBBC News - Egypt crisis: Appeals courts laun...   \n",
      "2  \\nMorning Briefing: Support grows for bid by P...   \n",
      "3  \\nEgypt News - Bodybuilder sets record for wor...   \n",
      "\n",
      "                                                 url  \n",
      "0  http://www.theglobeandmail.com/news/morning-br...  \n",
      "1  http://www.bbc.co.uk/news/world-middle-east-20...  \n",
      "2  http://www.theglobeandmail.com/news/morning-br...  \n",
      "3  http://www.egyptnews.net/index.php/sid/2110165...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus loaded succesfully: \" + str(len(corpus)) + \" documents loaded.\")\n",
    "print(corpus.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_file_df = list_to_dataframe(parse_markup(open_markup_file(test_file_addr)), doc_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Topic Modelling needs better preprocessing (stop words/lemmas etc.)\n",
    "    - stop words\n",
    "    - lemmatization (stemming is faster but is rule-based with more false transformations)\n",
    "    - special char removal\n",
    "- Could try removing junk at top of docs through REs/spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing dependencies import successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "import spacy\n",
    "\n",
    "print(\"preprocessing dependencies import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # try experimenting disabling parts of spacy pipeline see if .sents still works\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_as_word_lists(docs):\n",
    "    \"\"\"Tokenize sentences into lists of word characters\"\"\"\n",
    "    doc_sents = []\n",
    "    for doc in nlp.pipe(docs):\n",
    "        #sents.extend([sent.text for sent in doc.sents])\n",
    "        sents = []\n",
    "        word_count = 0\n",
    "        for sent in doc.sents:\n",
    "            words = []\n",
    "            if (len(sent) + word_count > 512): # model takes maximum 512 length sequences (need a workaround)\n",
    "                break\n",
    "            for token in sent:\n",
    "                words.append(token.text)\n",
    "                word_count += 1\n",
    "            sents.append(words)\n",
    "        doc_sents.append(sents)\n",
    "    return doc_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Word and Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1120 13:32:23.812910 140405771921216 SentenceTransformer.py:38] Load pretrained SentenceTransformer: distilbert-base-nli-stsb-mean-tokens\n",
      "I1120 13:32:23.856336 140405771921216 SentenceTransformer.py:42] Did not find folder distilbert-base-nli-stsb-mean-tokens\n",
      "I1120 13:32:23.857095 140405771921216 SentenceTransformer.py:48] Try to download model from server: https://sbert.net/models/distilbert-base-nli-stsb-mean-tokens.zip\n",
      "I1120 13:32:23.857578 140405771921216 SentenceTransformer.py:99] Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_distilbert-base-nli-stsb-mean-tokens\n",
      "I1120 13:32:25.240849 140405771921216 SentenceTransformer.py:123] Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "#sent_model = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "sent_tokenizer = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "#word_model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "word_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1f9fce00664a56a895a20ab6a99700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len word/sent emb: 3/3\n"
     ]
    }
   ],
   "source": [
    "# take small portion of corpus for testing currently\n",
    "test_docs = corpus['text'].iloc[0:3]\n",
    "\n",
    "def get_word_embeddings(model, sentence_word_list):\n",
    "    \"\"\"Transform sentences into lists of word-level embeddings\"\"\"\n",
    "    word_embeddings = []\n",
    "    for sentence in sentence_word_list:\n",
    "        # input_ids dumps 'attention_mask part of dict'\n",
    "        word_embeddings.append(model(sentence, is_pretokenized=True)['input_ids'])\n",
    "    return word_embeddings  \n",
    "\n",
    "def word_to_sentence_embeddings(model, word_sentence_embeddings):\n",
    "    \"\"\"Transform lists of word embeddings split by sentence into np array of sentence embeddings\"\"\"\n",
    "    return model.encode(word_sentence_embeddings, is_pretokenized=True)\n",
    "\n",
    "test_sent_word_lists = get_sentences_as_word_lists(test_docs)\n",
    "test_word_emb = get_word_embeddings(word_tokenizer, test_sent_word_lists)\n",
    "test_sent_emb = word_to_sentence_embeddings(sent_tokenizer, test_word_emb)\n",
    "\n",
    "print(\"len word/sent emb: \" + str(len(test_word_emb)) + \"/\" + str(len(test_sent_emb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "- LDA uses K-means clustering\n",
    "- HDA learns num topics automatically (Bayesian non-parametric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dependencies\n"
     ]
    }
   ],
   "source": [
    "# word level topic modelling\n",
    "# needs better preprocessing (remove stopwords/lemmitization etc)\n",
    "# maybe add REs/other preprocessing remove uninformative junk at top of docs\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "print(\"loaded dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Weighting Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1120 21:35:38.907186 140405771921216 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I1120 21:35:38.909041 140405771921216 dictionary.py:216] built Dictionary(443 unique tokens: ['\\n', '&', '-', '.', ':']...) from 46 documents (total 1512 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of cell\n"
     ]
    }
   ],
   "source": [
    "def weigh_tokens(texts, method=\"bow\"):\n",
    "    \"\"\"Perform token weighting scheme on text and return with dict\"\"\"\n",
    "    def create_dictionary(texts):\n",
    "        \"\"\"Create a gensim dictionary of index-word mappings\"\"\"\n",
    "        return corpora.Dictionary(texts)\n",
    "    \n",
    "    flat_texts = [token for sent in texts for token in sent]  # should be fast\n",
    "    corpus_dict = create_dictionary(flat_texts)\n",
    "    weighed_tokens = []\n",
    "    \n",
    "    if method == \"bow\":\n",
    "        return [corpus_dict.doc2bow(text) for text in flat_texts], corpus_dict\n",
    "    else:\n",
    "        raise Exception(\"Incorrect method parameter\")\n",
    "\n",
    "test_topic_corpus, test_corpus_dict = weigh_tokens(test_sent_word_lists)\n",
    "\n",
    "print(\"end of cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1120 21:45:48.645277 140405771921216 ldamodel.py:557] using symmetric alpha at 0.2\n",
      "I1120 21:45:48.645919 140405771921216 ldamodel.py:557] using symmetric eta at 0.2\n",
      "I1120 21:45:48.646242 140405771921216 ldamodel.py:481] using serial LDA version on this node\n",
      "I1120 21:45:48.646937 140405771921216 ldamodel.py:929] running online (multi-pass) LDA training, 5 topics, 15 passes over the supplied corpus of 46 documents, updating model once every 46 documents, evaluating perplexity every 46 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "I1120 21:45:48.669025 140405771921216 ldamodel.py:824] -7.700 per-word bound, 208.0 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.669297 140405771921216 ldamodel.py:978] PROGRESS: pass 0, at document #46/46\n",
      "I1120 21:45:48.687117 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\",\" + 0.028*\"the\" + 0.020*\"a\" + 0.020*\"\n",
      "\" + 0.020*\"in\" + 0.018*\".\" + 0.018*\"to\" + 0.016*\"and\" + 0.014*\"of\" + 0.012*\"Palestinians\"\n",
      "I1120 21:45:48.687448 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.037*\"the\" + 0.033*\"to\" + 0.027*\".\" + 0.025*\"\n",
      "\" + 0.017*\"\"\" + 0.016*\",\" + 0.013*\"of\" + 0.013*\"a\" + 0.011*\"Palestinians\" + 0.011*\"at\"\n",
      "I1120 21:45:48.687875 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.039*\"the\" + 0.029*\".\" + 0.028*\"\n",
      "\" + 0.025*\",\" + 0.025*\"to\" + 0.022*\"and\" + 0.019*\"The\" + 0.014*\"of\" + 0.011*\"Mursi\" + 0.010*\"Mr\"\n",
      "I1120 21:45:48.688172 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.043*\"\"\" + 0.022*\"the\" + 0.021*\"an\" + 0.018*\"of\" + 0.017*\"-\" + 0.015*\"to\" + 0.015*\"\n",
      "\" + 0.014*\".\" + 0.013*\"as\" + 0.012*\"observer\"\n",
      "I1120 21:45:48.688455 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.017*\"to\" + 0.017*\"the\" + 0.016*\"for\" + 0.016*\"Globe\" + 0.014*\"\n",
      "\" + 0.014*\"bid\" + 0.013*\",\" + 0.013*\"by\" + 0.013*\":\" + 0.013*\"-\"\n",
      "I1120 21:45:48.688689 140405771921216 ldamodel.py:1049] topic diff=2.440720, rho=1.000000\n",
      "I1120 21:45:48.704304 140405771921216 ldamodel.py:824] -6.098 per-word bound, 68.5 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.704578 140405771921216 ldamodel.py:978] PROGRESS: pass 1, at document #46/46\n",
      "I1120 21:45:48.712727 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.030*\"the\" + 0.028*\",\" + 0.024*\"a\" + 0.022*\"in\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.017*\"of\" + 0.016*\"to\" + 0.015*\"and\" + 0.015*\"Abbas\"\n",
      "I1120 21:45:48.713061 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.039*\"the\" + 0.035*\"to\" + 0.029*\".\" + 0.027*\"\n",
      "\" + 0.017*\"\"\" + 0.013*\",\" + 0.011*\"in\" + 0.011*\"is\" + 0.011*\"'s\" + 0.011*\"a\"\n",
      "I1120 21:45:48.713488 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.041*\"the\" + 0.032*\".\" + 0.032*\"\n",
      "\" + 0.027*\",\" + 0.023*\"and\" + 0.023*\"to\" + 0.020*\"The\" + 0.015*\"Mursi\" + 0.014*\"Mr\" + 0.014*\"of\"\n",
      "I1120 21:45:48.713773 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.054*\"\"\" + 0.027*\"the\" + 0.027*\"an\" + 0.026*\"of\" + 0.021*\"-\" + 0.020*\"to\" + 0.018*\"\n",
      "\" + 0.017*\".\" + 0.015*\"as\" + 0.015*\"observer\"\n",
      "I1120 21:45:48.714202 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.022*\",\" + 0.021*\"to\" + 0.020*\"for\" + 0.020*\"bid\" + 0.019*\"by\" + 0.017*\"Globe\" + 0.016*\"the\" + 0.015*\"and\" + 0.014*\":\" + 0.014*\"Palestinians\"\n",
      "I1120 21:45:48.714445 140405771921216 ldamodel.py:1049] topic diff=0.500574, rho=0.577350\n",
      "I1120 21:45:48.725717 140405771921216 ldamodel.py:824] -5.826 per-word bound, 56.7 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.725984 140405771921216 ldamodel.py:978] PROGRESS: pass 2, at document #46/46\n",
      "I1120 21:45:48.731729 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.030*\"the\" + 0.029*\",\" + 0.027*\"a\" + 0.024*\"in\" + 0.020*\"of\" + 0.019*\".\" + 0.018*\"\n",
      "\" + 0.017*\"Abbas\" + 0.015*\"to\" + 0.015*\"and\"\n",
      "I1120 21:45:48.732055 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.040*\"the\" + 0.035*\"to\" + 0.030*\".\" + 0.029*\"\n",
      "\" + 0.016*\"\"\" + 0.012*\",\" + 0.012*\"in\" + 0.012*\"is\" + 0.012*\"'s\" + 0.012*\"would\"\n",
      "I1120 21:45:48.732483 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.041*\"the\" + 0.033*\".\" + 0.033*\"\n",
      "\" + 0.028*\",\" + 0.024*\"and\" + 0.022*\"to\" + 0.021*\"The\" + 0.016*\"Mr\" + 0.016*\"Mursi\" + 0.014*\"of\"\n",
      "I1120 21:45:48.732762 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.058*\"\"\" + 0.029*\"the\" + 0.029*\"an\" + 0.029*\"of\" + 0.022*\"-\" + 0.022*\"to\" + 0.019*\"\n",
      "\" + 0.018*\".\" + 0.015*\"as\" + 0.015*\"observer\"\n",
      "I1120 21:45:48.733056 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.024*\",\" + 0.021*\"to\" + 0.021*\"for\" + 0.021*\"bid\" + 0.020*\"by\" + 0.018*\"Globe\" + 0.016*\"the\" + 0.015*\"and\" + 0.014*\":\" + 0.014*\"Palestinians\"\n",
      "I1120 21:45:48.733299 140405771921216 ldamodel.py:1049] topic diff=0.305086, rho=0.500000\n",
      "I1120 21:45:48.744181 140405771921216 ldamodel.py:824] -5.736 per-word bound, 53.3 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.744453 140405771921216 ldamodel.py:978] PROGRESS: pass 3, at document #46/46\n",
      "I1120 21:45:48.749730 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.029*\",\" + 0.028*\"a\" + 0.024*\"in\" + 0.021*\"of\" + 0.019*\".\" + 0.018*\"Abbas\" + 0.018*\"\n",
      "\" + 0.015*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.750144 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.040*\"the\" + 0.035*\"to\" + 0.030*\".\" + 0.030*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"in\" + 0.013*\"is\" + 0.012*\"'s\" + 0.012*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.750518 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.034*\".\" + 0.034*\"\n",
      "\" + 0.028*\",\" + 0.024*\"and\" + 0.022*\"to\" + 0.021*\"The\" + 0.017*\"Mr\" + 0.017*\"Mursi\" + 0.014*\"of\"\n",
      "I1120 21:45:48.750808 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.059*\"\"\" + 0.030*\"the\" + 0.030*\"an\" + 0.029*\"of\" + 0.023*\"-\" + 0.022*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.015*\"as\" + 0.015*\"observer\"\n",
      "I1120 21:45:48.751256 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.024*\",\" + 0.021*\"to\" + 0.021*\"for\" + 0.021*\"bid\" + 0.021*\"by\" + 0.018*\"Globe\" + 0.016*\"the\" + 0.016*\"and\" + 0.014*\":\" + 0.014*\"Palestinians\"\n",
      "I1120 21:45:48.751508 140405771921216 ldamodel.py:1049] topic diff=0.198518, rho=0.447214\n",
      "I1120 21:45:48.761691 140405771921216 ldamodel.py:824] -5.700 per-word bound, 52.0 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.761955 140405771921216 ldamodel.py:978] PROGRESS: pass 4, at document #46/46\n",
      "I1120 21:45:48.767172 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.029*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.021*\"of\" + 0.019*\".\" + 0.018*\"Abbas\" + 0.017*\"\n",
      "\" + 0.016*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.767480 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.030*\".\" + 0.030*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"in\" + 0.013*\"is\" + 0.013*\"'s\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.767909 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.034*\".\" + 0.034*\"\n",
      "\" + 0.028*\",\" + 0.024*\"and\" + 0.022*\"to\" + 0.021*\"The\" + 0.017*\"Mr\" + 0.017*\"Mursi\" + 0.014*\"decree\"\n",
      "I1120 21:45:48.768300 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.059*\"\"\" + 0.030*\"the\" + 0.030*\"an\" + 0.030*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.015*\"as\" + 0.015*\"observer\"\n",
      "I1120 21:45:48.768585 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.024*\",\" + 0.021*\"to\" + 0.021*\"for\" + 0.021*\"bid\" + 0.021*\"by\" + 0.018*\"Globe\" + 0.016*\"the\" + 0.016*\"and\" + 0.014*\":\" + 0.014*\"Palestinians\"\n",
      "I1120 21:45:48.768826 140405771921216 ldamodel.py:1049] topic diff=0.131376, rho=0.408248\n",
      "I1120 21:45:48.780151 140405771921216 ldamodel.py:824] -5.686 per-word bound, 51.5 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.780416 140405771921216 ldamodel.py:978] PROGRESS: pass 5, at document #46/46\n",
      "I1120 21:45:48.785578 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.029*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.019*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.016*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.785975 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\".\" + 0.030*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"in\" + 0.013*\"is\" + 0.013*\"'s\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.786348 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\".\" + 0.035*\"\n",
      "\" + 0.028*\",\" + 0.024*\"and\" + 0.022*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.017*\"Mursi\" + 0.014*\"decree\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1120 21:45:48.786631 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.030*\"the\" + 0.030*\"an\" + 0.030*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"this\" + 0.016*\"as\"\n",
      "I1120 21:45:48.786903 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.024*\",\" + 0.021*\"to\" + 0.021*\"for\" + 0.021*\"bid\" + 0.021*\"by\" + 0.018*\"Globe\" + 0.016*\"the\" + 0.016*\"and\" + 0.014*\":\" + 0.014*\"Palestinians\"\n",
      "I1120 21:45:48.787130 140405771921216 ldamodel.py:1049] topic diff=0.087227, rho=0.377964\n",
      "I1120 21:45:48.797037 140405771921216 ldamodel.py:824] -5.679 per-word bound, 51.2 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.797300 140405771921216 ldamodel.py:978] PROGRESS: pass 6, at document #46/46\n",
      "I1120 21:45:48.802453 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.016*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.802760 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\".\" + 0.031*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"in\" + 0.013*\"is\" + 0.013*\"'s\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.803058 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\".\" + 0.035*\"\n",
      "\" + 0.028*\",\" + 0.024*\"and\" + 0.022*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.017*\"Mursi\" + 0.014*\"decree\"\n",
      "I1120 21:45:48.803534 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.030*\"the\" + 0.030*\"an\" + 0.030*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"this\" + 0.016*\"as\"\n",
      "I1120 21:45:48.803820 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"to\" + 0.021*\"for\" + 0.021*\"bid\" + 0.021*\"by\" + 0.018*\"Globe\" + 0.016*\"the\" + 0.016*\"and\" + 0.014*\":\" + 0.014*\"Palestinians\"\n",
      "I1120 21:45:48.804048 140405771921216 ldamodel.py:1049] topic diff=0.058062, rho=0.353553\n",
      "I1120 21:45:48.815175 140405771921216 ldamodel.py:824] -5.676 per-word bound, 51.1 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.815505 140405771921216 ldamodel.py:978] PROGRESS: pass 7, at document #46/46\n",
      "I1120 21:45:48.820785 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.017*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.821346 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\".\" + 0.031*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"in\" + 0.013*\"is\" + 0.013*\"'s\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.821766 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\"\n",
      "\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.017*\"Mursi\" + 0.014*\"decree\"\n",
      "I1120 21:45:48.822159 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.030*\"the\" + 0.030*\"an\" + 0.030*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"this\" + 0.016*\"at\"\n",
      "I1120 21:45:48.822560 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"to\" + 0.021*\"for\" + 0.021*\"bid\" + 0.021*\"by\" + 0.018*\"Globe\" + 0.016*\"the\" + 0.016*\"and\" + 0.014*\":\" + 0.014*\"Palestinians\"\n",
      "I1120 21:45:48.822924 140405771921216 ldamodel.py:1049] topic diff=0.038884, rho=0.333333\n",
      "I1120 21:45:48.832933 140405771921216 ldamodel.py:824] -5.675 per-word bound, 51.1 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.833246 140405771921216 ldamodel.py:978] PROGRESS: pass 8, at document #46/46\n",
      "I1120 21:45:48.839545 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.017*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.840006 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\".\" + 0.031*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"is\" + 0.013*\"in\" + 0.013*\"'s\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.840434 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\"\n",
      "\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.018*\"Mursi\" + 0.014*\"decree\"\n",
      "I1120 21:45:48.840732 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.030*\"an\" + 0.030*\"the\" + 0.030*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"at\" + 0.016*\"this\"\n",
      "I1120 21:45:48.841170 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"to\" + 0.021*\"for\" + 0.021*\"bid\" + 0.021*\"by\" + 0.018*\"Globe\" + 0.016*\"the\" + 0.016*\"and\" + 0.014*\":\" + 0.014*\"Palestinians\"\n",
      "I1120 21:45:48.841417 140405771921216 ldamodel.py:1049] topic diff=0.026287, rho=0.316228\n",
      "I1120 21:45:48.851717 140405771921216 ldamodel.py:824] -5.674 per-word bound, 51.1 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.851989 140405771921216 ldamodel.py:978] PROGRESS: pass 9, at document #46/46\n",
      "I1120 21:45:48.857355 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.017*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.857680 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\".\" + 0.031*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"is\" + 0.013*\"in\" + 0.013*\"'s\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.858119 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\"\n",
      "\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.018*\"Mursi\" + 0.014*\"decree\"\n",
      "I1120 21:45:48.858409 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.030*\"an\" + 0.030*\"the\" + 0.030*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"at\" + 0.016*\"this\"\n",
      "I1120 21:45:48.858832 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"to\" + 0.021*\"for\" + 0.021*\"by\" + 0.021*\"bid\" + 0.018*\"Globe\" + 0.016*\"and\" + 0.016*\"the\" + 0.014*\":\" + 0.014*\"The\"\n",
      "I1120 21:45:48.859085 140405771921216 ldamodel.py:1049] topic diff=0.017980, rho=0.301511\n",
      "I1120 21:45:48.868719 140405771921216 ldamodel.py:824] -5.674 per-word bound, 51.0 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.868987 140405771921216 ldamodel.py:978] PROGRESS: pass 10, at document #46/46\n",
      "I1120 21:45:48.874290 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.017*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.874614 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\".\" + 0.031*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"is\" + 0.013*\"in\" + 0.013*\"'s\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.875050 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\"\n",
      "\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.018*\"Mursi\" + 0.014*\"decree\"\n",
      "I1120 21:45:48.875337 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.031*\"an\" + 0.030*\"the\" + 0.030*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"at\" + 0.016*\"this\"\n",
      "I1120 21:45:48.875755 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"to\" + 0.021*\"for\" + 0.021*\"by\" + 0.021*\"bid\" + 0.018*\"Globe\" + 0.016*\"and\" + 0.016*\"the\" + 0.014*\":\" + 0.014*\"The\"\n",
      "I1120 21:45:48.875995 140405771921216 ldamodel.py:1049] topic diff=0.012457, rho=0.288675\n",
      "I1120 21:45:48.885988 140405771921216 ldamodel.py:824] -5.673 per-word bound, 51.0 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.886256 140405771921216 ldamodel.py:978] PROGRESS: pass 11, at document #46/46\n",
      "I1120 21:45:48.891683 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.017*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.892097 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\".\" + 0.031*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"is\" + 0.013*\"'s\" + 0.013*\"in\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.892552 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\"\n",
      "\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.018*\"Mursi\" + 0.014*\"decree\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1120 21:45:48.892849 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.031*\"an\" + 0.031*\"the\" + 0.030*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"at\" + 0.016*\"this\"\n",
      "I1120 21:45:48.893145 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"for\" + 0.021*\"to\" + 0.021*\"by\" + 0.021*\"bid\" + 0.018*\"Globe\" + 0.016*\"and\" + 0.016*\"the\" + 0.014*\":\" + 0.014*\"The\"\n",
      "I1120 21:45:48.893391 140405771921216 ldamodel.py:1049] topic diff=0.008746, rho=0.277350\n",
      "I1120 21:45:48.903224 140405771921216 ldamodel.py:824] -5.673 per-word bound, 51.0 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.903497 140405771921216 ldamodel.py:978] PROGRESS: pass 12, at document #46/46\n",
      "I1120 21:45:48.908767 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.017*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.909110 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\".\" + 0.031*\"\n",
      "\" + 0.016*\"\"\" + 0.013*\"is\" + 0.013*\"'s\" + 0.013*\"in\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.909413 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\"\n",
      "\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.018*\"Mursi\" + 0.015*\"decree\"\n",
      "I1120 21:45:48.909887 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.031*\"an\" + 0.031*\"the\" + 0.031*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"at\" + 0.016*\"this\"\n",
      "I1120 21:45:48.910212 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"for\" + 0.021*\"to\" + 0.021*\"by\" + 0.021*\"bid\" + 0.018*\"Globe\" + 0.016*\"and\" + 0.016*\"the\" + 0.014*\":\" + 0.014*\"The\"\n",
      "I1120 21:45:48.910459 140405771921216 ldamodel.py:1049] topic diff=0.006221, rho=0.267261\n",
      "I1120 21:45:48.920519 140405771921216 ldamodel.py:824] -5.673 per-word bound, 51.0 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.920801 140405771921216 ldamodel.py:978] PROGRESS: pass 13, at document #46/46\n",
      "I1120 21:45:48.926028 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.017*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.926368 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\"\n",
      "\" + 0.031*\".\" + 0.016*\"\"\" + 0.013*\"is\" + 0.013*\"'s\" + 0.013*\"in\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.926662 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\"\n",
      "\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.018*\"Mursi\" + 0.015*\"decree\"\n",
      "I1120 21:45:48.927150 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.031*\"an\" + 0.031*\"the\" + 0.031*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"at\" + 0.016*\"this\"\n",
      "I1120 21:45:48.927561 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"for\" + 0.021*\"to\" + 0.021*\"by\" + 0.021*\"bid\" + 0.018*\"Globe\" + 0.016*\"and\" + 0.016*\"the\" + 0.014*\":\" + 0.014*\"The\"\n",
      "I1120 21:45:48.927820 140405771921216 ldamodel.py:1049] topic diff=0.004482, rho=0.258199\n",
      "I1120 21:45:48.938971 140405771921216 ldamodel.py:824] -5.673 per-word bound, 51.0 perplexity estimate based on a held-out corpus of 46 documents with 1512 words\n",
      "I1120 21:45:48.939265 140405771921216 ldamodel.py:978] PROGRESS: pass 14, at document #46/46\n",
      "I1120 21:45:48.944569 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.017*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.944903 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\"\n",
      "\" + 0.031*\".\" + 0.016*\"\"\" + 0.013*\"is\" + 0.013*\"'s\" + 0.013*\"in\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.945345 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\"\n",
      "\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.018*\"Mursi\" + 0.015*\"decree\"\n",
      "I1120 21:45:48.945635 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.031*\"an\" + 0.031*\"the\" + 0.031*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"at\" + 0.016*\"will\"\n",
      "I1120 21:45:48.945951 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"for\" + 0.021*\"by\" + 0.021*\"to\" + 0.021*\"bid\" + 0.018*\"Globe\" + 0.016*\"and\" + 0.016*\"the\" + 0.014*\":\" + 0.014*\"The\"\n",
      "I1120 21:45:48.946206 140405771921216 ldamodel.py:1049] topic diff=0.003267, rho=0.250000\n",
      "I1120 21:45:48.946846 140405771921216 ldamodel.py:1171] topic #0 (0.200): 0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\n",
      "\" + 0.017*\"support\" + 0.015*\"and\"\n",
      "I1120 21:45:48.947196 140405771921216 ldamodel.py:1171] topic #1 (0.200): 0.041*\"the\" + 0.036*\"to\" + 0.031*\"\n",
      "\" + 0.031*\".\" + 0.016*\"\"\" + 0.013*\"is\" + 0.013*\"'s\" + 0.013*\"in\" + 0.013*\"would\" + 0.011*\",\"\n",
      "I1120 21:45:48.947482 140405771921216 ldamodel.py:1171] topic #2 (0.200): 0.042*\"the\" + 0.035*\"\n",
      "\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.018*\"Mursi\" + 0.015*\"decree\"\n",
      "I1120 21:45:48.947762 140405771921216 ldamodel.py:1171] topic #3 (0.200): 0.060*\"\"\" + 0.031*\"an\" + 0.031*\"the\" + 0.031*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\n",
      "\" + 0.019*\".\" + 0.016*\"at\" + 0.016*\"will\"\n",
      "I1120 21:45:48.948315 140405771921216 ldamodel.py:1171] topic #4 (0.200): 0.025*\",\" + 0.021*\"for\" + 0.021*\"by\" + 0.021*\"to\" + 0.021*\"bid\" + 0.018*\"Globe\" + 0.016*\"and\" + 0.016*\"the\" + 0.014*\":\" + 0.014*\"The\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics Found: \n",
      "(0, '0.031*\"the\" + 0.030*\"a\" + 0.029*\",\" + 0.025*\"in\" + 0.022*\"of\" + 0.020*\".\" + 0.019*\"Abbas\" + 0.017*\"\\n\" + 0.017*\"support\" + 0.015*\"and\"')\n",
      "(1, '0.041*\"the\" + 0.036*\"to\" + 0.031*\"\\n\" + 0.031*\".\" + 0.016*\"\"\" + 0.013*\"is\" + 0.013*\"\\'s\" + 0.013*\"in\" + 0.013*\"would\" + 0.011*\",\"')\n",
      "(2, '0.042*\"the\" + 0.035*\"\\n\" + 0.035*\".\" + 0.028*\",\" + 0.024*\"and\" + 0.021*\"to\" + 0.021*\"The\" + 0.018*\"Mr\" + 0.018*\"Mursi\" + 0.015*\"decree\"')\n",
      "(3, '0.060*\"\"\" + 0.031*\"an\" + 0.031*\"the\" + 0.031*\"of\" + 0.023*\"-\" + 0.023*\"to\" + 0.019*\"\\n\" + 0.019*\".\" + 0.016*\"at\" + 0.016*\"will\"')\n",
      "(4, '0.025*\",\" + 0.021*\"for\" + 0.021*\"by\" + 0.021*\"to\" + 0.021*\"bid\" + 0.018*\"Globe\" + 0.016*\"and\" + 0.016*\"the\" + 0.014*\":\" + 0.014*\"The\"')\n",
      "end of cell\n"
     ]
    }
   ],
   "source": [
    "NUM_TOPICS = 5\n",
    "\n",
    "def model_topics(weighted_tokens, corpus_dict, method=\"lda\"):\n",
    "    if method == \"lda\":\n",
    "        model = gensim.models.ldamodel.LdaModel(weighted_tokens, num_topics=NUM_TOPICS, \n",
    "                                                id2word=corpus_dict, passes=15)\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception(\"Incorrect method parameter\")\n",
    "        \n",
    "test_lda_model = model_topics(test_topic_corpus, test_corpus_dict)\n",
    "\n",
    "print_test_lda_topics = test_lda_model.print_topics(NUM_TOPICS)\n",
    "print(\"LDA Topics Found: \")\n",
    "for topic in print_test_lda_topics:\n",
    "    print(topic)\n",
    "\n",
    "print(\"end of cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"\\nsuccessfully installed packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data set dependencies successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "print (\"loading data set dependencies successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET FILE META VARIABLES\n",
    "\n",
    "corpus_path = \"/nfs/trects-kba2014-filtered\" # directory of corpus of gzipped html files\n",
    "topics_path = corpus_path + \"/test-topics.xml\"\n",
    "doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "topic_tags = ['id', 'title', 'description', 'start','end','query','type'] # topic fields\n",
    "test_file_addr = corpus_path + \"/1/2012-02-22-15.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and get beautifulsoup object from markup file\n",
    "def open_markup_file(addr, gz=True, xml=False, verbose=False):\n",
    "    markup = None\n",
    "    f = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(addr)\n",
    "\n",
    "    if gz:\n",
    "        f = gzip.open(addr)\n",
    "    else:\n",
    "        f = open(addr)\n",
    "        \n",
    "    if xml == False:\n",
    "        markup = bs(f)  # open as html\n",
    "    else:\n",
    "        markup = bs(f, \"xml\")\n",
    "        \n",
    "    f.close()\n",
    "    return markup\n",
    "\n",
    "\n",
    "# parse markup and return 2D list [entry:tags]\n",
    "def parse_markup(markup, entry_list, find_tag=\"doc\", tag_list=doc_tags, topic_id=None):\n",
    "    for e in markup.find_all(find_tag):\n",
    "        entry = OrderedDict.fromkeys(tag_list)\n",
    "        if topic_id is not None:\n",
    "            entry['topic_id'] = topic_id\n",
    "        for c in e.children:  # children use direct children, descendants uses all\n",
    "            if c.name in entry:\n",
    "                entry[c.name] = c.string\n",
    "            elif c.name is None and c.string != '\\n':  # inner body of <doc> tag\n",
    "                entry['text'] = str(c.string)\n",
    "        entry_list.append(list(entry.values()))\n",
    "        \n",
    "            \n",
    "# recursively find gz html files from a directory address\n",
    "def search_dir(path):    \n",
    "    # separate the subdirectories and html files \n",
    "    # (help maintain sequential order of insertion)\n",
    "    gz_paths = []\n",
    "    for f in os.scandir(path):\n",
    "        if os.path.splitext(f.path)[-1].lower() == \".gz\":\n",
    "            gz_paths.append(f.path)\n",
    "    \n",
    "    return gz_paths\n",
    "\n",
    "\n",
    "def list_to_dataframe(markup_list, tags):\n",
    "    return pd.DataFrame(markup_list, columns=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topics into dataframe\n",
    "def load_topics(path):\n",
    "    topics_list = []\n",
    "    \n",
    "    parse_markup(open_markup_file(path, gz=False, xml=True), \n",
    "                    topics_list, find_tag=\"event\", tag_list=topic_tags)\n",
    "    \n",
    "    \n",
    "    return  list_to_dataframe(topics_list, topic_tags)\n",
    "\n",
    "topics = load_topics(topics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics loaded successfuly\n",
      "  id                                title  \\\n",
      "0  1      2012 Buenos Aires Rail Disaster   \n",
      "1  2  2012 Pakistan garment factory fires   \n",
      "2  3                 2012 Aurora shooting   \n",
      "3  4       Wisconsin Sikh temple shooting   \n",
      "\n",
      "                                         description       start         end  \\\n",
      "0  http://en.wikipedia.org/wiki/2012_Buenos_Aires...  1329910380  1330774380   \n",
      "1  http://en.wikipedia.org/wiki/2012_Pakistan_gar...  1347368400  1348232400   \n",
      "2  http://en.wikipedia.org/wiki/2012_Aurora_shooting  1342766280  1343630280   \n",
      "3  http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...  1344180300  1345044300   \n",
      "\n",
      "                      query      type  \n",
      "0  buenos aires train crash  accident  \n",
      "1     pakistan factory fire  accident  \n",
      "2         colorado shooting  shooting  \n",
      "3      sikh temple shooting  shooting  \n"
     ]
    }
   ],
   "source": [
    "print(\"Topics loaded successfuly\")\n",
    "print(topics.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:20<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# load all formatted gzipped html files into dataframe\n",
    "def load_corpus(path):\n",
    "    corpus_list = []\n",
    "    gz_paths = []\n",
    "    for topic_id in topics['id'].to_numpy():\n",
    "        id_path = corpus_path + \"/\" + topic_id + \"/\"  # every topic id correlates to subfolder named after it\n",
    "        gz_paths = search_dir(id_path)\n",
    "    for gz_path in tqdm(gz_paths, position=0, leave=True):\n",
    "        parse_markup(open_markup_file(gz_path, verbose=False),\n",
    "                        corpus_list, topic_id=topic_id)\n",
    "    return list_to_dataframe(corpus_list, doc_tags)\n",
    "\n",
    "corpus = load_corpus(corpus_path)\n",
    "#print(\"Corpus loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded succesfully: 1578 documents loaded.\n",
      "  topic_id                                     streamid  \\\n",
      "0       10  1354113657-a4417f055ea5ae84207a4edb4dad881b   \n",
      "1       10  1354112039-110cc86ea7a8a1b58306dfade5b300ec   \n",
      "2       10  1354114192-a4417f055ea5ae84207a4edb4dad881b   \n",
      "3       10  1354114426-6c8d58d994c0e3243ee8dca8f34516a4   \n",
      "\n",
      "                              docid     yyyymmddhh        kbastream  \\\n",
      "0  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "1  110cc86ea7a8a1b58306dfade5b300ec  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "2  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "3  6c8d58d994c0e3243ee8dca8f34516a4  2012-11-28-14           WEBLOG   \n",
      "\n",
      "                     zulu       epoch  \\\n",
      "0  2012-11-28T14:40:57.0Z  1354113657   \n",
      "1  2012-11-28T14:13:59.0Z  1354112039   \n",
      "2  2012-11-28T14:49:52.0Z  1354114192   \n",
      "3  2012-11-28T14:53:46.0Z  1354114426   \n",
      "\n",
      "                                               title  \\\n",
      "0  Morning Briefing: Support grows for bid by Pal...   \n",
      "1  BBC News - Egypt crisis: Appeals courts launch...   \n",
      "2  Morning Briefing: Support grows for bid by Pal...   \n",
      "3  Egypt News - Bodybuilder sets record for world...   \n",
      "\n",
      "                                                text  \\\n",
      "0  \\nMorning Briefing: Support grows for bid by P...   \n",
      "1  \\nBBC News - Egypt crisis: Appeals courts laun...   \n",
      "2  \\nMorning Briefing: Support grows for bid by P...   \n",
      "3  \\nEgypt News - Bodybuilder sets record for wor...   \n",
      "\n",
      "                                                 url  \n",
      "0  http://www.theglobeandmail.com/news/morning-br...  \n",
      "1  http://www.bbc.co.uk/news/world-middle-east-20...  \n",
      "2  http://www.theglobeandmail.com/news/morning-br...  \n",
      "3  http://www.egyptnews.net/index.php/sid/2110165...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus loaded succesfully: \" + str(len(corpus)) + \" documents loaded.\")\n",
    "print(corpus.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_file_df = list_to_dataframe(parse_markup(open_markup_file(test_file_addr)), doc_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "import spacy\n",
    "\n",
    "print(\"preprocessing dependencies import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # try experimenting disabling parts of spacy pipeline see if .sents still works\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_as_word_lists(docs):\n",
    "    \"\"\"Tokenize sentences into lists of word characters\"\"\"\n",
    "    doc_sents = []\n",
    "    for doc in nlp.pipe(docs):\n",
    "        #sents.extend([sent.text for sent in doc.sents])\n",
    "        sents = []\n",
    "        word_count = 0\n",
    "        for sent in doc.sents:\n",
    "            words = []\n",
    "            if (len(sent) + word_count > 512): # model takes maximum 512 length sequences (need a workaround)\n",
    "                break\n",
    "            for token in sent:\n",
    "                words.append(token.text)\n",
    "                word_count += 1\n",
    "            sents.append(words)\n",
    "        doc_sents.append(sents)\n",
    "    return doc_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Word and Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1120 11:47:40.984783 140691828995904 SentenceTransformer.py:38] Load pretrained SentenceTransformer: distilbert-base-nli-stsb-mean-tokens\n",
      "I1120 11:47:41.008171 140691828995904 SentenceTransformer.py:42] Did not find folder distilbert-base-nli-stsb-mean-tokens\n",
      "I1120 11:47:41.008564 140691828995904 SentenceTransformer.py:48] Try to download model from server: https://sbert.net/models/distilbert-base-nli-stsb-mean-tokens.zip\n",
      "I1120 11:47:41.009045 140691828995904 SentenceTransformer.py:99] Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_distilbert-base-nli-stsb-mean-tokens\n",
      "I1120 11:47:41.851731 140691828995904 SentenceTransformer.py:123] Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "#sent_model = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "sent_tokenizer = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "#word_model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "word_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e882cec95ee74fb8826d5cdf450d9aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len word/sent emb: 3/3\n"
     ]
    }
   ],
   "source": [
    "# get docs (currently select amount)\n",
    "test_docs = corpus['text'].iloc[0:3]\n",
    "\n",
    "def get_word_embeddings(model, sentence_word_list):\n",
    "    \"\"\"Transform sentences into lists of word-level embeddings\"\"\"\n",
    "    word_embeddings = []\n",
    "    for sentence in sentence_word_list:\n",
    "        # input_ids dumps 'attention_mask part of dict'\n",
    "        word_embeddings.append(model(sentence, is_pretokenized=True)['input_ids'])\n",
    "    return word_embeddings  \n",
    "\n",
    "def word_to_sentence_embeddings(model, word_sentence_embeddings):\n",
    "    \"\"\"Transform lists of word embeddings split by sentence into np array of sentence embeddings\"\"\"\n",
    "    return model.encode(word_sentence_embeddings, is_pretokenized=True)\n",
    "\n",
    "test_word_emb = get_word_embeddings(word_tokenizer, get_sentences_as_word_lists(test_docs))\n",
    "test_sent_emb = word_to_sentence_embeddings(sent_tokenizer, test_word_emb)\n",
    "\n",
    "print(\"len word/sent emb: \" + str(len(test_word_emb)) + \"/\" + str(len(test_sent_emb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

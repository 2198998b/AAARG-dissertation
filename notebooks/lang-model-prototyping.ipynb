{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data set dependencies successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "print (\"loading data set dependencies successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET FILE META VARIABLES\n",
    "\n",
    "corpus_path = \"/nfs/trects-kba2014-filtered\" # directory of corpus of gzipped html files\n",
    "topics_path = corpus_path + \"/test-topics.xml\"\n",
    "doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "topic_tags = ['id', 'title', 'description', 'start','end','query','type'] # topic fields\n",
    "test_file_addr = corpus_path + \"/1/2012-02-22-15.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and get beautifulsoup object from markup file\n",
    "def open_markup_file(addr, gz=True, xml=False, verbose=False):\n",
    "    markup = None\n",
    "    f = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(addr)\n",
    "\n",
    "    if gz:\n",
    "        f = gzip.open(addr)\n",
    "    else:\n",
    "        f = open(addr)\n",
    "        \n",
    "    if xml == False:\n",
    "        markup = bs(f)  # open as html\n",
    "    else:\n",
    "        markup = bs(f, \"xml\")\n",
    "        \n",
    "    f.close()\n",
    "    return markup\n",
    "\n",
    "\n",
    "# parse markup and return 2D list [entry:tags]\n",
    "def parse_markup(markup, entry_list, find_tag=\"doc\", tag_list=doc_tags, topic_id=None):\n",
    "    for e in markup.find_all(find_tag):\n",
    "        entry = OrderedDict.fromkeys(tag_list)\n",
    "        if topic_id is not None:\n",
    "            entry['topic_id'] = topic_id\n",
    "        for c in e.children:  # children use direct children, descendants uses all\n",
    "            if c.name in entry:\n",
    "                entry[c.name] = c.string\n",
    "            elif c.name is None and c.string != '\\n':  # inner body of <doc> tag\n",
    "                entry['text'] = str(c.string)\n",
    "        entry_list.append(list(entry.values()))\n",
    "        \n",
    "            \n",
    "# recursively find gz html files from a directory address\n",
    "def search_dir(path):    \n",
    "    # separate the subdirectories and html files \n",
    "    # (help maintain sequential order of insertion)\n",
    "    gz_paths = []\n",
    "    for f in os.scandir(path):\n",
    "        if os.path.splitext(f.path)[-1].lower() == \".gz\":\n",
    "            gz_paths.append(f.path)\n",
    "    \n",
    "    return gz_paths\n",
    "\n",
    "\n",
    "def list_to_dataframe(markup_list, tags):\n",
    "    return pd.DataFrame(markup_list, columns=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topics into dataframe\n",
    "def load_topics(path):\n",
    "    topics_list = []\n",
    "    \n",
    "    parse_markup(open_markup_file(path, gz=False, xml=True), \n",
    "                    topics_list, find_tag=\"event\", tag_list=topic_tags)\n",
    "    \n",
    "    \n",
    "    return  list_to_dataframe(topics_list, topic_tags)\n",
    "\n",
    "topics = load_topics(topics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics loaded successfuly\n",
      "  id                                title  \\\n",
      "0  1      2012 Buenos Aires Rail Disaster   \n",
      "1  2  2012 Pakistan garment factory fires   \n",
      "2  3                 2012 Aurora shooting   \n",
      "3  4       Wisconsin Sikh temple shooting   \n",
      "\n",
      "                                         description       start         end  \\\n",
      "0  http://en.wikipedia.org/wiki/2012_Buenos_Aires...  1329910380  1330774380   \n",
      "1  http://en.wikipedia.org/wiki/2012_Pakistan_gar...  1347368400  1348232400   \n",
      "2  http://en.wikipedia.org/wiki/2012_Aurora_shooting  1342766280  1343630280   \n",
      "3  http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...  1344180300  1345044300   \n",
      "\n",
      "                      query      type  \n",
      "0  buenos aires train crash  accident  \n",
      "1     pakistan factory fire  accident  \n",
      "2         colorado shooting  shooting  \n",
      "3      sikh temple shooting  shooting  \n"
     ]
    }
   ],
   "source": [
    "print(\"Topics loaded successfuly\")\n",
    "print(topics.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:20<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# load all formatted gzipped html files into dataframe\n",
    "def load_corpus(path):\n",
    "    corpus_list = []\n",
    "    gz_paths = []\n",
    "    for topic_id in topics['id'].to_numpy():\n",
    "        id_path = corpus_path + \"/\" + topic_id + \"/\"  # every topic id correlates to subfolder named after it\n",
    "        gz_paths = search_dir(id_path)\n",
    "    for gz_path in tqdm(gz_paths, position=0, leave=True):\n",
    "        parse_markup(open_markup_file(gz_path, verbose=False),\n",
    "                        corpus_list, topic_id=topic_id)\n",
    "    return list_to_dataframe(corpus_list, doc_tags)\n",
    "\n",
    "corpus = load_corpus(corpus_path)\n",
    "#print(\"Corpus loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded succesfully: 1578 documents loaded.\n",
      "  topic_id                                     streamid  \\\n",
      "0       10  1354113657-a4417f055ea5ae84207a4edb4dad881b   \n",
      "1       10  1354112039-110cc86ea7a8a1b58306dfade5b300ec   \n",
      "2       10  1354114192-a4417f055ea5ae84207a4edb4dad881b   \n",
      "3       10  1354114426-6c8d58d994c0e3243ee8dca8f34516a4   \n",
      "\n",
      "                              docid     yyyymmddhh        kbastream  \\\n",
      "0  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "1  110cc86ea7a8a1b58306dfade5b300ec  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "2  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "3  6c8d58d994c0e3243ee8dca8f34516a4  2012-11-28-14           WEBLOG   \n",
      "\n",
      "                     zulu       epoch  \\\n",
      "0  2012-11-28T14:40:57.0Z  1354113657   \n",
      "1  2012-11-28T14:13:59.0Z  1354112039   \n",
      "2  2012-11-28T14:49:52.0Z  1354114192   \n",
      "3  2012-11-28T14:53:46.0Z  1354114426   \n",
      "\n",
      "                                               title  \\\n",
      "0  Morning Briefing: Support grows for bid by Pal...   \n",
      "1  BBC News - Egypt crisis: Appeals courts launch...   \n",
      "2  Morning Briefing: Support grows for bid by Pal...   \n",
      "3  Egypt News - Bodybuilder sets record for world...   \n",
      "\n",
      "                                                text  \\\n",
      "0  \\nMorning Briefing: Support grows for bid by P...   \n",
      "1  \\nBBC News - Egypt crisis: Appeals courts laun...   \n",
      "2  \\nMorning Briefing: Support grows for bid by P...   \n",
      "3  \\nEgypt News - Bodybuilder sets record for wor...   \n",
      "\n",
      "                                                 url  \n",
      "0  http://www.theglobeandmail.com/news/morning-br...  \n",
      "1  http://www.bbc.co.uk/news/world-middle-east-20...  \n",
      "2  http://www.theglobeandmail.com/news/morning-br...  \n",
      "3  http://www.egyptnews.net/index.php/sid/2110165...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus loaded succesfully: \" + str(len(corpus)) + \" documents loaded.\")\n",
    "print(corpus.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_file_df = list_to_dataframe(parse_markup(open_markup_file(test_file_addr)), doc_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Topic Modelling needs better preprocessing (stop words/lemmas etc.)\n",
    "    - stop words\n",
    "    - lemmatization (stemming is faster but is rule-based with more false transformations)\n",
    "    - special char removal\n",
    "- Could try removing junk at top of docs through REs/spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing dependencies import successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "import spacy\n",
    "\n",
    "print(\"preprocessing dependencies import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # try experimenting disabling parts of spacy pipeline see if .sents still works\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_as_word_lists(docs):\n",
    "    \"\"\"Tokenize sentences into lists of word characters\"\"\"\n",
    "    doc_sents = []\n",
    "    for doc in nlp.pipe(docs):\n",
    "        #sents.extend([sent.text for sent in doc.sents])\n",
    "        sents = []\n",
    "        word_count = 0\n",
    "        for sent in doc.sents:\n",
    "            words = []\n",
    "            if (len(sent) + word_count > 512): # model takes maximum 512 length sequences (need a workaround)\n",
    "                break\n",
    "            for token in sent:\n",
    "                words.append(token.text)\n",
    "                word_count += 1\n",
    "            sents.append(words)\n",
    "        doc_sents.append(sents)\n",
    "    return doc_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Word and Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "#sent_model = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "sent_tokenizer = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "#word_model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "word_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len word/sent emb: 3/3\n"
     ]
    }
   ],
   "source": [
    "# take small portion of corpus for testing currently\n",
    "test_docs = corpus['text'].iloc[0:3]\n",
    "\n",
    "def get_word_embeddings(model, sentence_word_list):\n",
    "    \"\"\"Transform sentences into lists of word-level embeddings\"\"\"\n",
    "    word_embeddings = []\n",
    "    for sentence in sentence_word_list:\n",
    "        # input_ids dumps 'attention_mask part of dict'\n",
    "        word_embeddings.append(model(sentence, is_pretokenized=True)['input_ids'])\n",
    "    return word_embeddings  \n",
    "\n",
    "def word_to_sentence_embeddings(model, word_sentence_embeddings):\n",
    "    \"\"\"Transform lists of word embeddings split by sentence into np array of sentence embeddings\"\"\"\n",
    "    return model.encode(word_sentence_embeddings, is_pretokenized=True)\n",
    "\n",
    "test_sent_word_lists = get_sentences_as_word_lists(test_docs)\n",
    "test_word_emb = get_word_embeddings(word_tokenizer, test_sent_word_lists)\n",
    "test_sent_emb = word_to_sentence_embeddings(sent_tokenizer, test_word_emb)\n",
    "\n",
    "print(\"len word/sent emb: \" + str(len(test_word_emb)) + \"/\" + str(len(test_sent_emb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "- LDA uses K-means clustering\n",
    "- HDA learns num topics automatically (Bayesian non-parametric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dependencies\n"
     ]
    }
   ],
   "source": [
    "# word level topic modelling\n",
    "# needs better preprocessing (remove stopwords/lemmitization etc)\n",
    "# maybe add REs/other preprocessing remove uninformative junk at top of docs\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "print(\"loaded dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Weighting Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of cell\n"
     ]
    }
   ],
   "source": [
    "def weigh_tokens(texts, method=\"bow\"):\n",
    "    \"\"\"Perform token weighting scheme on text and return with dict\"\"\"\n",
    "    def create_dictionary(texts):\n",
    "        \"\"\"Create a gensim dictionary of index-word mappings\"\"\"\n",
    "        return corpora.Dictionary(texts)\n",
    "    \n",
    "    flat_texts = [token for sent in texts for token in sent]  # should be fast\n",
    "    corpus_dict = create_dictionary(flat_texts)\n",
    "    weighed_tokens = []\n",
    "    \n",
    "    if method == \"bow\":\n",
    "        return [corpus_dict.doc2bow(text) for text in flat_texts], corpus_dict\n",
    "    else:\n",
    "        raise Exception(\"Incorrect method parameter\")\n",
    "\n",
    "test_topic_corpus, test_corpus_dict = weigh_tokens(test_sent_word_lists)\n",
    "\n",
    "print(\"end of cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics Found: \n",
      "(0, '0.032*\"the\" + 0.024*\"in\" + 0.023*\"a\" + 0.022*\"to\" + 0.022*\"of\" + 0.019*\",\" + 0.019*\".\" + 0.019*\"Abbas\" + 0.014*\"\\n\" + 0.012*\"President\"')\n",
      "(1, '0.020*\"to\" + 0.020*\"national\" + 0.011*\"is\" + 0.011*\"revolution\" + 0.011*\"any\" + 0.011*\"measures\" + 0.011*\"take\" + 0.011*\"The\" + 0.011*\"president\" + 0.011*\".\"')\n",
      "(2, '0.028*\"the\" + 0.027*\"and\" + 0.027*\".\" + 0.023*\"\\n\" + 0.020*\"The\" + 0.019*\"to\" + 0.018*\"News\" + 0.016*\":\" + 0.016*\"Search\" + 0.016*\"-\"')\n",
      "(3, '0.034*\",\" + 0.025*\"\\n\" + 0.023*\".\" + 0.021*\"the\" + 0.018*\"by\" + 0.018*\"for\" + 0.016*\"to\" + 0.014*\"on\" + 0.014*\"bid\" + 0.014*\"and\"')\n",
      "(4, '0.043*\"the\" + 0.043*\"\"\" + 0.034*\"to\" + 0.026*\"\\n\" + 0.020*\".\" + 0.020*\"of\" + 0.020*\"a\" + 0.020*\"that\" + 0.016*\"-\" + 0.016*\"an\"')\n",
      "end of cell\n"
     ]
    }
   ],
   "source": [
    "NUM_TOPICS = 5\n",
    "\n",
    "def model_topics(weighted_tokens, corpus_dict, method=\"lda\"):\n",
    "    if method == \"lda\":\n",
    "#         model = gensim.models.ldamodel.LdaModel(weighted_tokens, num_topics=NUM_TOPICS, \n",
    "#                                                 id2word=corpus_dict, passes=15)\n",
    "        model = gensim.models.ldamulticore.LdaMulticore(weighted_tokens, num_topics=NUM_TOPICS, \n",
    "                                                id2word=corpus_dict, passes=15)\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception(\"Incorrect method parameter\")\n",
    "        \n",
    "test_lda_model = model_topics(test_topic_corpus, test_corpus_dict)\n",
    "\n",
    "print_test_lda_topics = test_lda_model.print_topics(NUM_TOPICS)\n",
    "print(\"LDA Topics Found: \")\n",
    "for topic in print_test_lda_topics:\n",
    "    print(topic)\n",
    "\n",
    "print(\"end of cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get sentences which are nearest neighbors to the identified topics\n",
    "# https://scikit-learn.org/stable/modules/neighbors.html\n",
    "# https://stackoverflow.com/questions/60996584/bert-embedding-for-semantic-similarity\n",
    "# https://stackoverflow.com/questions/59865719/how-to-find-the-closest-word-to-a-vector-using-bert\n",
    "# https://gist.github.com/avidale/c6b19687d333655da483421880441950\n",
    "\n",
    "\n",
    "# then compare sentence results from pure extractive summariser maybe?\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "#import mxnet as mx\n",
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "# ctx = mx.gpu(0)\n",
    "# bert = BertEmbedding(ctx=ctx)\n",
    "bert_emb = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trying code from here\n",
    "https://gist.github.com/avidale/c6b19687d333655da483421880441950\n",
    "\n",
    "Preprocess embeddings in a formatted way as such can track sentences, words, embeddings\n",
    "\n",
    "do this, then pass the LDA topics into the query\n",
    "\"\"\" \n",
    "\n",
    "class EmbeddingHandler:\n",
    "    def __init__(self, sentences, model):\n",
    "        self.sentences = sentences\n",
    "        self.model = model\n",
    "        \n",
    "    def generate_embeddings(self):\n",
    "        result = self.model(self.sentences)\n",
    "#         result = list()\n",
    "#         for sent in self.sentences:\n",
    "#             result.append(self.model.encode(sent, is_pretokenized=True))\n",
    "#         #result = self.model.encode(self.sentences, is_pretokenized=True, show_progress_bar=True)\n",
    "        #print(result)\n",
    "        \n",
    "        self.sent_ids = []\n",
    "        self.token_ids = []\n",
    "        self.tokens = []\n",
    "        embeddings = []\n",
    "        for i, (toks, embs) in enumerate(tqdm(result)):\n",
    "            for j, (tok, emb) in enumerate(zip(toks, embs)):\n",
    "                self.sent_ids.append(i)\n",
    "                self.token_ids.append(j)\n",
    "                self.tokens.append(tok)\n",
    "                embeddings.append(emb)\n",
    "        embeddings = np.stack(embeddings)\n",
    "        # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
    "        self.normed_embeddings = (embeddings.T / (embeddings**2).sum(axis=1) ** 0.5).T\n",
    "        \n",
    "    def generate_sent_embeddings(self):\n",
    "        \"\"\"test sent vs word embeddings\"\"\"\n",
    "        # use sentence-transformers embeddings\n",
    "        result = self.model.encode(self.sentences)\n",
    "        self.sent_ids = []\n",
    "        self.tokens = []\n",
    "        embeddings = []\n",
    "        for i, (tok, emb) in enumerate(tqdm(zip(self.sentences,result))):\n",
    "            self.sent_ids.append(i)\n",
    "            self.tokens.append(tok)\n",
    "            embeddings.append(emb)\n",
    "        embeddings = np.stack(embeddings)\n",
    "        # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
    "        self.normed_embeddings = (embeddings.T / (embeddings**2).sum(axis=1) ** 0.5).T\n",
    "        \n",
    "    def create_comparitor(self):\n",
    "        # this takes some time\n",
    "        self.indexer = KDTree(self.normed_embeddings)\n",
    "        print(\"created KDTree\")\n",
    "    \n",
    "    def query(self, query_sent, query_word, k=10, filter_same_word=False):\n",
    "        toks, embs = self.model([query_sent])[0]\n",
    "\n",
    "        found = False\n",
    "        for tok, emb in zip(toks, embs):\n",
    "            if tok == query_word:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            raise ValueError('The query word {} is not a single token in sentence {}'.format(query_word, toks))\n",
    "        emb = emb / sum(emb**2)**0.5\n",
    "\n",
    "        if filter_same_word:\n",
    "            initial_k = max(k, 100)\n",
    "        else:\n",
    "            initial_k = k\n",
    "        di, idx = self.indexer.query(emb.reshape(1, -1), k=initial_k)  # this is returning our neighbours\n",
    "        distances = []\n",
    "        neighbors = []\n",
    "        contexts = []\n",
    "        # this is filtering for word matching\n",
    "        for i, index in enumerate(idx.ravel()):\n",
    "            token = self.tokens[index]\n",
    "            if filter_same_word and (query_word in token or token in query_word):  # take this away\n",
    "                continue\n",
    "            distances.append(di.ravel()[i])\n",
    "            neighbors.append(token)\n",
    "            contexts.append(self.sentences[self.sent_ids[index]])\n",
    "            if len(distances) == k:\n",
    "                break\n",
    "        return distances, neighbors, contexts\n",
    "    \n",
    "    def topic_neighbors(self, topic_word, k=10):\n",
    "        # get average embedding of topic word\n",
    "        # maybe instead return context sentence that is closest to averaged embedding?\n",
    "        # that way can use context to get right meaning\n",
    "        topic_emb = self.avg_embedding(self.retrieve_embeddings(topic_word))\n",
    "        \n",
    "        # get neighbors\n",
    "        # do I need reshape?\n",
    "        di, idx = self.indexer.query(topic_emb.reshape(1,-1), k=k)\n",
    "        distances = []\n",
    "        neighbors = []\n",
    "        contexts = []\n",
    "        for i, index in enumerate(idx.ravel()):\n",
    "            token = self.tokens[index]\n",
    "            distances.append(di.ravel()[i])\n",
    "            neighbors.append(token)\n",
    "            contexts.append(self.sentences[self.sent_ids[index]])\n",
    "        return distances, neighbors, contexts\n",
    "        \n",
    "        \n",
    "    def retrieve_embeddings(self, token):\n",
    "        idxs = []\n",
    "        for i, t in enumerate(self.tokens):\n",
    "            if t == token:\n",
    "                idxs.append(i)\n",
    "            elif token in t:  # sent-embeddings temp workaround\n",
    "                idxs.append(i)\n",
    "        embs = []\n",
    "        for i in idxs:\n",
    "            embs.append(self.normed_embeddings[i])\n",
    "        return embs\n",
    "    \n",
    "    def avg_embedding(self, emb_list):\n",
    "        return np.mean(emb_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test just get sentences\n",
    "# richard said sents are actually split by \\n\n",
    "def split_sentences(docs):\n",
    "    \"\"\"Tokenize sentences into lists of word characters\"\"\"\n",
    "    doc_sents = []\n",
    "    for doc in nlp.pipe(docs):\n",
    "        #sents.extend([sent.text for sent in doc.sents])\n",
    "        for sent in doc.sents:\n",
    "            doc_sents.append(sent.text)\n",
    "    return doc_sents\n",
    "\n",
    "def split_newline(docs):\n",
    "    sents = []\n",
    "    for doc in docs:\n",
    "        sents.extend(doc.splitlines())\n",
    "    return sents\n",
    "\n",
    "emb_handler_corp = split_newline(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:00<00:00, 130654.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created KDTree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb_handler = EmbeddingHandler(emb_handler_corp, bert_emb)  # [0] index taking first doco\n",
    "emb_handler.generate_embeddings()\n",
    "emb_handler.create_comparitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:00, 535891.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created KDTree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sent_emb_h = EmbeddingHandler(emb_handler_corp, sent_tokenizer)\n",
    "sent_emb_h.generate_sent_embeddings()\n",
    "sent_emb_h.create_comparitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 0.634126151881698  The Swiss and Danes join growing list of European countries supporting the bid, including France , Norway and Spain .\n",
      "\n",
      "and 0.634126151881698  The Swiss and Danes join growing list of European countries supporting the bid, including France , Norway and Spain .\n",
      "\n",
      "and 0.63657503617046  The Swiss and Danes join growing list of European countries supporting the bid, including France , Norway and Spain .\n",
      "\n",
      "and 0.63657503617046  The Swiss and Danes join growing list of European countries supporting the bid, including France , Norway and Spain .\n",
      "\n",
      "and 0.6419069099513167  The Muslim Brotherhood and other Islamist groups have called a rally for Saturday in support of Mr Mursi .\n",
      "\n",
      "and 0.6535914160403863  UK and Ireland Egypt protests continue in crisis over 44 mins ago International Business Times UK Cairo Protesters Condemn Mursi Power Grab [VIDEO + PHOTOS] 2 hrs ago Sky News Egypt: Major Protest Against President Morsi 13 hrs ago BusinessWeek Egypt Anti-Mursi Protesters Test President and Opposition Unity 14 hrs ago Lebanon Daily Star Tahrir rises to challenge Mursi 14 hrs ago About these results Share this page Delicious Digg Facebook reddit StumbleUpon Twitter Email Print More Middle East stories RSS Twin blasts shake Damascus suburb At least 34 people have been killed by two car bomb explosions in a south-eastern district of Syria 's capital, Damascus , state media report.\n",
      "\n",
      "and 0.6683205861937945  Anti- Mursi protests were held in Cairo , Alexandria, Suez , Minya and other Nile Delta cities on Tuesday.\n",
      "\n",
      "and 0.6684830851032422  Appeals courts and the Court of Cassation will halt work until the decree is revoked, the judges say.\n",
      "\n",
      "and 0.672473978069234  But I doubt God talks to sitcom stars Life Ian Brown and Andrew Solomon discuss the mysteries of childhood identity Report on Business ‘ Mis-hires ’ and how to avoid them Most popular videos » News Meet the man with world's largest upper arms Arts Halle Berry 's ex gets restraining order against Olivier Martinez News Edmonton man's Bohemian Rhapsody goes viral News Mexican beauty queen gunned down in drug violence Life It's sweet and easy to make dulce de leche at home Rob Ford respects court decision, but wants to finish his job Technology Showrooming: The most feared trend in retailing ◀ ● ● ▶ Most Popular Stories Car prices to soar as Ottawa takes aim at emissions Drunk ‘ Bohemian Rhapsody ’ singer convicted, wears Viking horns to court Two and a Half Men actor apologizes for calling his show ‘ filth ’ Ford loyalists silently weigh their options November 28: Your daily horoscope More Top Stories World The Globe's Omar el Akkad takes your questions on Egypt unrest Editorials The PQ’ s perennial flag-removal act is tired political theatre Golf News Golf ’s rule makers ban anchored stroke in 2016 Holiday Survival Guide 7 holiday movies you should watch this year The Challenge He ’s 26, with $60-million in revenue, but could still use advice Inside the Market At the open: TSX down as commodities take a tumble Video Green Party leader positive about byelection losses Sound Around You project builds sound map of the world Russia's MegaFon IPO scores big UK papers await judge's landmark report Photos Celebrity Photos of the Week: Bieber dressed down!\n",
      "\n",
      "and 0.672473978069234  But I doubt God talks to sitcom stars Life Ian Brown and Andrew Solomon discuss the mysteries of childhood identity Report on Business ‘ Mis-hires ’ and how to avoid them Most popular videos » News Meet the man with world's largest upper arms Arts Halle Berry 's ex gets restraining order against Olivier Martinez News Edmonton man's Bohemian Rhapsody goes viral News Mexican beauty queen gunned down in drug violence Life It's sweet and easy to make dulce de leche at home Rob Ford respects court decision, but wants to finish his job Technology Showrooming: The most feared trend in retailing ◀ ● ● ▶ Most Popular Stories Car prices to soar as Ottawa takes aim at emissions Two and a Half Men actor apologizes for calling his show ‘ filth’ Drunk ‘ Bohemian Rhapsody ’ singer convicted, wears Viking horns to court Ford loyalists silently weigh their options November 28: Your daily horoscope More Top Stories Soccer Platini says UEFA could scrap Europa League Editorials The PQ ’s perennial flag-removal act is tired political theatre Basketball Miami ’s LeBron James tops latest NBA jersey-sale rankings Holiday Survival Guide 7 holiday movies you should watch this year The Challenge He ’s 26, with $60-million in revenue, but could still use advice Carrick On Money Carrick on money: No economy for young men and women Video Man falls to his death after clinging to 10th floor window of burning building Sound Around You project builds sound map of the world Russia's MegaFon IPO scores big UK papers await judge's landmark report Photos Celebrity Photos of the Week: Bieber dressed down!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dist, neigh, cont = emb_handler.topic_neighbors(\"and\")\n",
    "for d, w, c in zip(dist, neigh, cont):\n",
    "    print('{} {}  {}'.format(w, d, c.strip()))\n",
    "    print(\"\")\n",
    "#avg_emb = emb_handler.avg_embedding(ret_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC News - Egypt crisis: Appeals courts launch anti-Mursi strike action Accessibility links Skip to content Skip to local navigation Accessibility Help bbc.co. uk navigation News Sport Weather Travel Future TV Radio More … Search term: Middle East Home US &amp ; Canada Latin America UK Africa Asia Europe Mid-East Business Health Sci / Environment Tech Entertainment Video 28 November 2012 Last updated at 09:04 ET Share this page Delicious Digg Facebook reddit StumbleUpon Twitter Email Print Egypt crisis: Appeals courts launch anti-Mursi strike action Protests and clashes have continued Continue reading the main story Egypt changing Mursi's gamble Who holds the power? 0.7788612088764301\n",
      "\n",
      "( MARKO DJURICA /REUTERS ) Morning Briefing: Support grows for bid by Palestinians for UN recognition Add to ... Stephen Northfield And Jill Mahoney The Globe and Mail Published Wednesday, Nov. 28 2012, 8:26 AM EST Last updated Wednesday, Nov. 28 2012, 9:28 AM EST Comments closed Print / License A A A summary of what you need to know today, compiled by The Globe’s news desk on Nov. 28, 2012 Support grows for bid by Palestinians for UN recognition More and more countries are lining up behind a bid by the Palestinians to get enhanced status at the United Nations . 0.7878124964370655\n",
      "\n",
      "( MARKO DJURICA /REUTERS ) Morning Briefing: Support grows for bid by Palestinians for UN recognition Add to ... Stephen Northfield And Jill Mahoney The Globe and Mail Published Wednesday, Nov. 28 2012, 8:26 AM EST Last updated Wednesday, Nov. 28 2012, 9:28 AM EST Comments closed Print / License A A A summary of what you need to know today, compiled by The Globe’s news desk on Nov. 28, 2012 Support grows for bid by Palestinians for UN recognition More and more countries are lining up behind a bid by the Palestinians to get enhanced status at the United Nations . 0.7878124964370655\n",
      "\n",
      "Morning Briefing: Support grows for bid by Palestinians for UN recognition - The Globe and Mail The Globe and Mail Jump to main navigation Jump to main content Search: News & Quotes Jobs News Search News by eluta. ca Search Jobs at eluta. ca Jobs Login Register 18 Toronto Subscribe Help Home News Commentary Business Investing Sports Life Arts Technology Drive Site Map National Politics Toronto British Columbia World Video Home » News Palestinians hold placards depicting President Mahmoud Abbas during a rally in support of Abbas ' efforts to secure a diplomatic upgrade at the United Nations , in the West Bank city of Ramallah . 0.8117749867923973\n",
      "\n",
      "Morning Briefing: Support grows for bid by Palestinians for UN recognition - The Globe and Mail The Globe and Mail Jump to main navigation Jump to main content Search: News & Quotes Jobs News Search News by eluta. ca Search Jobs at eluta. ca Jobs Login Register 18 Toronto Subscribe Help Home News Commentary Business Investing Sports Life Arts Technology Drive Site Map National Politics Toronto British Columbia World Video Home » News Palestinians hold placards depicting President Mahmoud Abbas during a rally in support of Abbas ' efforts to secure a diplomatic upgrade at the United Nations , in the West Bank city of Ramallah . 0.8117749867923973\n",
      "\n",
      "The deals – which would have cut back sick days and delayed raises for new teachers – were among a number of recent agreements the province was hoping would provide a blueprint for peace with Ontario teachers, many of whom are engaging in job action in protest of Bill 115. 0.8136751566370496\n",
      "\n",
      "The deals – which would have cut back sick days and delayed raises for new teachers – were among a number of recent agreements the province was hoping would provide a blueprint for peace with Ontario teachers, many of whom are engaging in job action in protest of Bill 115. 0.8136751566370496\n",
      "\n",
      "UK and Ireland Egypt protests continue in crisis over 44 mins ago International Business Times UK Cairo Protesters Condemn Mursi Power Grab [VIDEO + PHOTOS] 2 hrs ago Sky News Egypt: Major Protest Against President Morsi 13 hrs ago BusinessWeek Egypt Anti-Mursi Protesters Test President and Opposition Unity 14 hrs ago Lebanon Daily Star Tahrir rises to challenge Mursi 14 hrs ago About these results Share this page Delicious Digg Facebook reddit StumbleUpon Twitter Email Print More Middle East stories RSS Twin blasts shake Damascus suburb At least 34 people have been killed by two car bomb explosions in a south-eastern district of Syria 's capital, Damascus , state media report. 0.8137579673083036\n",
      "\n",
      "See all Globe Products Main Sections Home News Commentary Business Investing Sports Life Arts Technology Drive Site Map More Sections Appointments Art Store Births &amp ; Announcements Careers Globe Campus Classifieds Classroom Edition Deaths Newspaper Ads Real Estate Special Reports Globe sustainability Reader Services Globe Plus Subscribe Globe Recognition Recognition card Advertise Advertise with us Newspaper Magazine Online Marketing Solutions Group Media Central Classifieds More Reader Services Online Help Contact Us Newspaper About Our Newspaper Customer Care Contact Us Staff Corrections Subscribe Vacation Stops Change Address Company Inforamtion Privacy Policy Accessibility Policy Terms & Conditions Disclaimer © Copyright 2012 The Globe and Mail Inc. All Rights Reserved. 0.8234327303752385\n",
      "\n",
      "See all Globe Products Main Sections Home News Commentary Business Investing Sports Life Arts Technology Drive Site Map More Sections Appointments Art Store Births &amp ; Announcements Careers Globe Campus Classifieds Classroom Edition Deaths Newspaper Ads Real Estate Special Reports Globe sustainability Reader Services Globe Plus Subscribe Globe Recognition Recognition card Advertise Advertise with us Newspaper Magazine Online Marketing Solutions Group Media Central Classifieds More Reader Services Online Help Contact Us Newspaper About Our Newspaper Customer Care Contact Us Staff Corrections Subscribe Vacation Stops Change Address Company Inforamtion Privacy Policy Accessibility Policy Terms & Conditions Disclaimer © Copyright 2012 The Globe and Mail Inc. All Rights Reserved. 0.8234327303752385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dist2, neigh2, cont2 = sent_emb_h.topic_neighbors(\"and\")\n",
    "for d, w in zip(dist2, neigh2):\n",
    "    print('{} {}'.format(w, d))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data set dependencies successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "print (\"loading data set dependencies successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET FILE META VARIABLES\n",
    "\n",
    "corpus_path = \"/nfs/trects-kba2014-filtered\" # directory of corpus of gzipped html files\n",
    "topics_path = corpus_path + \"/test-topics.xml\"\n",
    "doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "topic_tags = ['id', 'title', 'description', 'start','end','query','type'] # topic fields\n",
    "test_file_addr = corpus_path + \"/1/2012-02-22-15.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and get beautifulsoup object from markup file\n",
    "def open_markup_file(addr, gz=True, xml=False, verbose=False):\n",
    "    markup = None\n",
    "    f = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(addr)\n",
    "\n",
    "    if gz:\n",
    "        f = gzip.open(addr)\n",
    "    else:\n",
    "        f = open(addr)\n",
    "        \n",
    "    if xml == False:\n",
    "        markup = bs(f)  # open as html\n",
    "    else:\n",
    "        markup = bs(f, \"xml\")\n",
    "        \n",
    "    f.close()\n",
    "    return markup\n",
    "\n",
    "\n",
    "# parse markup and return 2D list [entry:tags]\n",
    "def parse_markup(markup, entry_list, find_tag=\"doc\", tag_list=doc_tags, topic_id=None):\n",
    "    for e in markup.find_all(find_tag):\n",
    "        entry = OrderedDict.fromkeys(tag_list)\n",
    "        if topic_id is not None:\n",
    "            entry['topic_id'] = topic_id\n",
    "        for c in e.children:  # children use direct children, descendants uses all\n",
    "            if c.name in entry:\n",
    "                entry[c.name] = str(c.string)\n",
    "            elif c.name is None and c.string != '\\n':  # inner body of <doc> tag\n",
    "                entry['text'] = str(c.string)\n",
    "        entry_list.append(list(entry.values()))\n",
    "        \n",
    "            \n",
    "# recursively find gz html files from a directory address\n",
    "def search_dir(path):    \n",
    "    # separate the subdirectories and html files \n",
    "    # (help maintain sequential order of insertion)\n",
    "    gz_paths = []\n",
    "    for f in os.scandir(path):\n",
    "        if os.path.splitext(f.path)[-1].lower() == \".gz\":\n",
    "            gz_paths.append(f.path)\n",
    "    \n",
    "    return gz_paths\n",
    "\n",
    "\n",
    "def list_to_dataframe(markup_list, tags):\n",
    "    return pd.DataFrame(markup_list, columns=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topics into dataframe\n",
    "def load_topics(path):\n",
    "    topics_list = []\n",
    "    \n",
    "    parse_markup(open_markup_file(path, gz=False, xml=True), \n",
    "                    topics_list, find_tag=\"event\", tag_list=topic_tags)\n",
    "    \n",
    "    df = list_to_dataframe(topics_list, topic_tags)\n",
    "    df['id'] = pd.to_numeric(df['id'])\n",
    "    return df\n",
    "\n",
    "topics = load_topics(topics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics loaded successfuly\n",
      "   id                                title  \\\n",
      "0   1      2012 Buenos Aires Rail Disaster   \n",
      "1   2  2012 Pakistan garment factory fires   \n",
      "2   3                 2012 Aurora shooting   \n",
      "3   4       Wisconsin Sikh temple shooting   \n",
      "\n",
      "                                         description       start         end  \\\n",
      "0  http://en.wikipedia.org/wiki/2012_Buenos_Aires...  1329910380  1330774380   \n",
      "1  http://en.wikipedia.org/wiki/2012_Pakistan_gar...  1347368400  1348232400   \n",
      "2  http://en.wikipedia.org/wiki/2012_Aurora_shooting  1342766280  1343630280   \n",
      "3  http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...  1344180300  1345044300   \n",
      "\n",
      "                      query      type  \n",
      "0  buenos aires train crash  accident  \n",
      "1     pakistan factory fire  accident  \n",
      "2         colorado shooting  shooting  \n",
      "3      sikh temple shooting  shooting  \n"
     ]
    }
   ],
   "source": [
    "print(\"Topics loaded successfuly\")\n",
    "print(topics.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/241 [00:00<00:15, 15.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 236/241 [00:10<00:00, 31.99it/s]\n",
      "100%|██████████| 241/241 [00:10<00:00, 23.42it/s]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [04:28<00:00,  1.11s/it]\n",
      "  0%|          | 1/241 [00:00<00:28,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [02:05<00:00,  1.92it/s]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [02:47<00:00,  1.44it/s]\n",
      "  0%|          | 1/241 [00:00<00:24,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:19<00:00,  3.04it/s]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:00<00:00,  4.01it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 7...\n",
      "Loading topic 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [00:21<00:00, 11.22it/s]\n",
      "  0%|          | 1/241 [00:00<00:41,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [00:08<00:00, 27.56it/s]\n",
      "  0%|          | 1/241 [00:00<00:33,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:20<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load all formatted gzipped html files into dataframe\n",
    "def load_corpus(path):\n",
    "    #corpus_list = []\n",
    "    df = pd.DataFrame(columns=doc_tags)\n",
    "    for topic_id in topics['id'].to_numpy():\n",
    "        print(\"Loading topic \" + str(topic_id) + \"...\")\n",
    "        topic_list = []\n",
    "        id_path = corpus_path + \"/\" + str(topic_id) + \"/\"  # every topic id correlates to subfolder named after it\n",
    "        gz_paths = search_dir(id_path)\n",
    "        for gz_path in tqdm(gz_paths, position=0, leave=True):\n",
    "            parse_markup(open_markup_file(gz_path, verbose=False),\n",
    "                            topic_list, topic_id=topic_id)\n",
    "        topic_df = list_to_dataframe(topic_list, doc_tags)\n",
    "        df = df.append(topic_df)\n",
    "    df['epoch'] = pd.to_numeric(df['epoch'])\n",
    "    return df\n",
    "\n",
    "corpus = load_corpus(corpus_path)\n",
    "print(\"Corpus loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded succesfully: 12261 documents loaded.\n",
      "  topic_id                                     streamid  \\\n",
      "0        1  1330269540-995ed81eafa60498872335da7dce1386   \n",
      "1        1  1330268520-f42a863b58b2cc53cc716953c40f6065   \n",
      "2        1  1330270020-e47e013ec518f5fdd253ce28231f509f   \n",
      "3        1  1330268700-8078290575c82c8dd0e4e99370447bd2   \n",
      "\n",
      "                              docid     yyyymmddhh kbastream  \\\n",
      "0  995ed81eafa60498872335da7dce1386  2012-02-26-15      news   \n",
      "1  f42a863b58b2cc53cc716953c40f6065  2012-02-26-15      news   \n",
      "2  e47e013ec518f5fdd253ce28231f509f  2012-02-26-15      news   \n",
      "3  8078290575c82c8dd0e4e99370447bd2  2012-02-26-15      news   \n",
      "\n",
      "                          zulu       epoch  \\\n",
      "0  2012-02-26T15:19:00.000000Z  1330269540   \n",
      "1  2012-02-26T15:02:00.000000Z  1330268520   \n",
      "2  2012-02-26T15:27:00.000000Z  1330270020   \n",
      "3  2012-02-26T15:05:00.000000Z  1330268700   \n",
      "\n",
      "                                               title  \\\n",
      "0  US says it's steadfast in rebuilding Afghanist...   \n",
      "1  Argentina Train Crash: Driver Blames Faulty Br...   \n",
      "2  The Alaska Journal of Commerce Local News Oil ...   \n",
      "3  U.S. military receives remains of last soldier...   \n",
      "\n",
      "                                                text  \\\n",
      "0  \\nUS says it's steadfast in rebuilding Afghani...   \n",
      "1  \\nArgentina Train Crash: Driver Blames Faulty ...   \n",
      "2  \\nThe Alaska Journal of Commerce Local News Oi...   \n",
      "3  \\nU.S. military receives remains of last soldi...   \n",
      "\n",
      "                                                 url  \n",
      "0    http://www.elpasotimes.com/politics/ci_20049216  \n",
      "1  http://www.thisdaylive.com/articles/argentina-...  \n",
      "2  http://ap.alaskajournal.com/pstories/20120226/...  \n",
      "3  http://www.islandpacket.com/2012/02/26/1978117...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus loaded succesfully: \" + str(len(corpus)) + \" documents loaded.\")\n",
    "print(corpus.head(4))\n",
    "# there is an error in the dataset that article at 1 is misplaced in topic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_id                                                      1\n",
      "streamid            1330268520-f42a863b58b2cc53cc716953c40f6065\n",
      "docid                          f42a863b58b2cc53cc716953c40f6065\n",
      "yyyymmddhh                                        2012-02-26-15\n",
      "kbastream                                                  news\n",
      "zulu                                2012-02-26T15:02:00.000000Z\n",
      "epoch                                                1330268520\n",
      "title         Argentina Train Crash: Driver Blames Faulty Br...\n",
      "text          \\nArgentina Train Crash: Driver Blames Faulty ...\n",
      "url           http://www.thisdaylive.com/articles/argentina-...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#test_file_df = list_to_dataframe(parse_markup(open_markup_file(test_file_addr)), doc_tags)\n",
    "#print(corpus[corpus['docid'] == \"1329910380-3afda7882974e306bc75176f5ce37f3e\"])\n",
    "#print(corpus[corpus['yyyymmddhh'] == \"2012-02-22-13\"].head(5))\n",
    "print(corpus.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    topic_id                                     streamid  \\\n",
      "116        1  1329915660-47ed792a77d798dda8697654e8fcbb43   \n",
      "\n",
      "                                docid     yyyymmddhh kbastream  \\\n",
      "116  47ed792a77d798dda8697654e8fcbb43  2012-02-22-13      news   \n",
      "\n",
      "                            zulu       epoch  \\\n",
      "116  2012-02-22T13:01:00.000000Z  1329915660   \n",
      "\n",
      "                                                 title  \\\n",
      "116  Argentine train slams into station, killing 49...   \n",
      "\n",
      "                                                  text  \\\n",
      "116  \\nArgentine train slams into station, killing ...   \n",
      "\n",
      "                                                   url  \n",
      "116  http://www.seattlepi.com/news/article/Argentin...  \n",
      "    topic_id                                     streamid  \\\n",
      "116        1  1329915660-47ed792a77d798dda8697654e8fcbb43   \n",
      "\n",
      "                                docid     yyyymmddhh kbastream  \\\n",
      "116  47ed792a77d798dda8697654e8fcbb43  2012-02-22-13      news   \n",
      "\n",
      "                            zulu       epoch  \\\n",
      "116  2012-02-22T13:01:00.000000Z  1329915660   \n",
      "\n",
      "                                                 title  \\\n",
      "116  Argentine train slams into station, killing 49...   \n",
      "\n",
      "                                                  text  \\\n",
      "116  \\nArgentine train slams into station, killing ...   \n",
      "\n",
      "                                                   url  \n",
      "116  http://www.seattlepi.com/news/article/Argentin...  \n",
      "    topic_id                                     streamid  \\\n",
      "369        1  1329915300-46c9b2db03fbaf7d2a903bbfa7ff3c93   \n",
      "\n",
      "                                docid     yyyymmddhh kbastream  \\\n",
      "369  46c9b2db03fbaf7d2a903bbfa7ff3c93  2012-02-22-12      news   \n",
      "\n",
      "                            zulu       epoch  \\\n",
      "369  2012-02-22T12:55:00.000000Z  1329915300   \n",
      "\n",
      "                                                 title  \\\n",
      "369  Argentine train slams into station, killing 49...   \n",
      "\n",
      "                                                  text  \\\n",
      "369  \\nArgentine train slams into station, killing ...   \n",
      "\n",
      "                                                   url  \n",
      "369  http://www.springfieldnewssun.com/news/nation-...  \n"
     ]
    }
   ],
   "source": [
    "# duplicates are updates to the page\n",
    "find_nug = corpus[corpus['streamid'] == \"1329915660-47ed792a77d798dda8697654e8fcbb43\"]\n",
    "# 1329915300-46c9b2db03fbaf7d2a903bbfa7ff3c93-3\n",
    "# duplicate found when -3 taken away\n",
    "dup_nug = corpus[corpus['streamid'] == \"1329915300-46c9b2db03fbaf7d2a903bbfa7ff3c93\"]\n",
    "print(corpus[corpus['docid'] == \"47ed792a77d798dda8697654e8fcbb43\"])\n",
    "print(find_nug)\n",
    "print(dup_nug)\n",
    "# print(dup_nug['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Topic Modelling needs better preprocessing (stop words/lemmas etc.)\n",
    "    - stop words\n",
    "    - lemmatization (stemming is faster but is rule-based with more false transformations)\n",
    "    - special char removal\n",
    "- Could try removing junk at top of docs through REs/spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing dependencies import successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "import spacy\n",
    "\n",
    "print(\"preprocessing dependencies import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fa3718d21c8>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # try experimenting disabling parts of spacy pipeline see if .sents still works\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell finished\n"
     ]
    }
   ],
   "source": [
    "test_docs = corpus.loc[0:3,:]  # work on just top 3 for now\n",
    "\n",
    "# in data frame, split sentences into list by the newline delimiter\n",
    "#test_docs['text'] = test_docs['text'].map(lambda x: x.splitlines())\n",
    "\n",
    "# map the non-preprocessed string list to a preprocessed string list\n",
    "\n",
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "    tokens = list()\n",
    "    doc = nlp(string)\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "    normalized_tokens = list()\n",
    "    for token in tokens:\n",
    "        normalized = token.text.lower().strip()\n",
    "        if ((token.is_alpha or token.is_digit)):\n",
    "            normalized_tokens.append(normalized)\n",
    "    return normalized_tokens\n",
    "\n",
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "    return normalize(spacy_tokenize(string))\n",
    "\n",
    "# test_prep = []\n",
    "# for doc in test_docs['text']:\n",
    "#     d = []\n",
    "#     for sent in doc:\n",
    "#         d.append(tokenize_normalize(sent))\n",
    "#     test_prep.append(d)\n",
    "        \n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_prep[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Word and Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245M/245M [00:05<00:00, 43.5MB/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68c9222f1b14981949417fdd8bd5fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=442, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c11c3bd7144c8d9e03dea9cd2e10a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "#sent_model = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "sent_tokenizer = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "#word_model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "word_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "- LDA uses K-means clustering\n",
    "- HDA learns num topics automatically (Bayesian non-parametric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dependencies\n"
     ]
    }
   ],
   "source": [
    "# word level topic modelling\n",
    "# needs better preprocessing (remove stopwords/lemmitization etc)\n",
    "# maybe add REs/other preprocessing remove uninformative junk at top of docs\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "print(\"loaded dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded cell\n"
     ]
    }
   ],
   "source": [
    "class TopicModeller: \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.corpus_dict = None\n",
    "        self.weighted_tokens = None\n",
    "        self.print_topics = None\n",
    "        \n",
    "    def weigh_tokens(self, texts, method=\"bow\"):\n",
    "        \"\"\"Perform token weighting scheme on text and return with dict\"\"\"\n",
    "        def create_dictionary(texts):\n",
    "            \"\"\"Create a gensim dictionary of index-word mappings\"\"\"\n",
    "            return corpora.Dictionary(texts)\n",
    "    \n",
    "        flat_texts = [token for sent in texts for token in sent]  # should be fast\n",
    "        self.corpus_dict = create_dictionary(flat_texts)\n",
    "        if method == \"bow\":\n",
    "            self.weighted_tokens = [self.corpus_dict.doc2bow(text) for text in flat_texts]\n",
    "        else:\n",
    "            raise Exception(\"Incorrect method parameter\")\n",
    "            \n",
    "    def model_topics(self, method=\"lda\", num_topics=10):\n",
    "        if method == \"lda\":\n",
    "    #         model = gensim.models.ldamodel.LdaModel(weighted_tokens, num_topics=NUM_TOPICS, \n",
    "    #                                                 id2word=corpus_dict, passes=15)\n",
    "            self.model = gensim.models.ldamulticore.LdaMulticore(self.weighted_tokens, num_topics=num_topics, \n",
    "                                                    id2word=self.corpus_dict, passes=15)\n",
    "        else:\n",
    "            raise Exception(\"Incorrect method parameter\")\n",
    "            \n",
    "        self.print_topics = self.model.print_topics()\n",
    "        return self.print_topics\n",
    "print(\"loaded cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model = TopicModeller()\n",
    "# topic_model.weigh_tokens(test_prep)\n",
    "# print(topic_model.model_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /root/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
      "Downloading /root/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
     ]
    }
   ],
   "source": [
    "# first get sentences which are nearest neighbors to the identified topics\n",
    "# https://scikit-learn.org/stable/modules/neighbors.html\n",
    "# https://stackoverflow.com/questions/60996584/bert-embedding-for-semantic-similarity\n",
    "# https://stackoverflow.com/questions/59865719/how-to-find-the-closest-word-to-a-vector-using-bert\n",
    "# https://gist.github.com/avidale/c6b19687d333655da483421880441950\n",
    "\n",
    "\n",
    "# then compare sentence results from pure extractive summariser maybe?\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "#import mxnet as mx\n",
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "# ctx = mx.gpu(0)\n",
    "# bert = BertEmbedding(ctx=ctx)\n",
    "bert_emb = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trying code from here\n",
    "https://gist.github.com/avidale/c6b19687d333655da483421880441950\n",
    "\n",
    "Preprocess embeddings in a formatted way as such can track sentences, words, embeddings\n",
    "\n",
    "do this, then pass the LDA topics into the query\n",
    "\"\"\" \n",
    "\n",
    "class EmbeddingHandler:\n",
    "    def __init__(self, sentences, model):\n",
    "        self.sentences = sentences\n",
    "        self.model = model\n",
    "        \n",
    "    def generate_embeddings(self):\n",
    "        result = self.model(self.sentences)\n",
    "#         result = list()\n",
    "#         for sent in self.sentences:\n",
    "#             result.append(self.model.encode(sent, is_pretokenized=True))\n",
    "#         #result = self.model.encode(self.sentences, is_pretokenized=True, show_progress_bar=True)\n",
    "        #print(result)\n",
    "        \n",
    "        self.sent_ids = []\n",
    "        self.token_ids = []\n",
    "        self.tokens = []\n",
    "        embeddings = []\n",
    "        for i, (toks, embs) in enumerate(tqdm(result)):\n",
    "            for j, (tok, emb) in enumerate(zip(toks, embs)):\n",
    "                self.sent_ids.append(i)\n",
    "                self.token_ids.append(j)\n",
    "                self.tokens.append(tok)\n",
    "                embeddings.append(emb)\n",
    "        embeddings = np.stack(embeddings)\n",
    "        # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
    "        self.normed_embeddings = (embeddings.T / (embeddings**2).sum(axis=1) ** 0.5).T\n",
    "        \n",
    "    def generate_sent_embeddings(self):\n",
    "        \"\"\"test sent vs word embeddings\"\"\"\n",
    "        # use sentence-transformers embeddings\n",
    "        result = self.model.encode(self.sentences)\n",
    "        self.sent_ids = []\n",
    "        self.tokens = []\n",
    "        embeddings = []\n",
    "        for i, (tok, emb) in enumerate(tqdm(zip(self.sentences,result))):\n",
    "            self.sent_ids.append(i)\n",
    "            self.tokens.append(tok)\n",
    "            embeddings.append(emb)\n",
    "        embeddings = np.stack(embeddings)\n",
    "        # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
    "        self.normed_embeddings = (embeddings.T / (embeddings**2).sum(axis=1) ** 0.5).T\n",
    "        \n",
    "    def create_comparitor(self):\n",
    "        # this takes some time\n",
    "        self.indexer = KDTree(self.normed_embeddings)\n",
    "        print(\"created KDTree\")\n",
    "    \n",
    "    def query(self, query_sent, query_word, k=10, filter_same_word=False):\n",
    "        toks, embs = self.model([query_sent])[0]\n",
    "\n",
    "        found = False\n",
    "        for tok, emb in zip(toks, embs):\n",
    "            if tok == query_word:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            raise ValueError('The query word {} is not a single token in sentence {}'.format(query_word, toks))\n",
    "        emb = emb / sum(emb**2)**0.5\n",
    "\n",
    "        if filter_same_word:\n",
    "            initial_k = max(k, 100)\n",
    "        else:\n",
    "            initial_k = k\n",
    "        di, idx = self.indexer.query(emb.reshape(1, -1), k=initial_k)  # this is returning our neighbours\n",
    "        distances = []\n",
    "        neighbors = []\n",
    "        contexts = []\n",
    "        # this is filtering for word matching\n",
    "        for i, index in enumerate(idx.ravel()):\n",
    "            token = self.tokens[index]\n",
    "            if filter_same_word and (query_word in token or token in query_word):  # take this away\n",
    "                continue\n",
    "            distances.append(di.ravel()[i])\n",
    "            neighbors.append(token)\n",
    "            contexts.append(self.sentences[self.sent_ids[index]])\n",
    "            if len(distances) == k:\n",
    "                break\n",
    "        return distances, neighbors, contexts\n",
    "    \n",
    "    def topic_neighbors(self, topic_word, k=10):\n",
    "        # get average embedding of topic word\n",
    "        # maybe instead return context sentence that is closest to averaged embedding?\n",
    "        # that way can use context to get right meaning\n",
    "        topic_emb = self.avg_embedding(self.retrieve_embeddings(topic_word))\n",
    "        \n",
    "        # get neighbors\n",
    "        # do I need reshape?\n",
    "        di, idx = self.indexer.query(topic_emb.reshape(1,-1), k=k)\n",
    "        distances = []\n",
    "        neighbors = []\n",
    "        contexts = []\n",
    "        for i, index in enumerate(idx.ravel()):\n",
    "            token = self.tokens[index]\n",
    "            distances.append(di.ravel()[i])\n",
    "            neighbors.append(token)\n",
    "            contexts.append(self.sentences[self.sent_ids[index]])\n",
    "        return distances, neighbors, contexts\n",
    "        \n",
    "        \n",
    "    def retrieve_embeddings(self, token):\n",
    "        idxs = []\n",
    "        for i, t in enumerate(self.tokens):\n",
    "            if t == token:\n",
    "                idxs.append(i)\n",
    "            elif token in t:  # sent-embeddings temp workaround\n",
    "                idxs.append(i)\n",
    "        embs = []\n",
    "        for i in idxs:\n",
    "            embs.append(self.normed_embeddings[i])\n",
    "        return embs\n",
    "    \n",
    "    def avg_embedding(self, emb_list):\n",
    "        return np.mean(emb_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test just get sentences\n",
    "# # richard said sents are actually split by \\n\n",
    "# def split_sentences(docs):\n",
    "#     \"\"\"Tokenize sentences into lists of word characters\"\"\"\n",
    "#     doc_sents = []\n",
    "#     for doc in nlp.pipe(docs):\n",
    "#         #sents.extend([sent.text for sent in doc.sents])\n",
    "#         for sent in doc.sents:\n",
    "#             doc_sents.append(sent.text)\n",
    "#     return doc_sents\n",
    "\n",
    "# def split_newline(docs):\n",
    "#     sents = []\n",
    "#     for doc in docs:\n",
    "#         sents.extend(doc.splitlines())\n",
    "#     return sents\n",
    "\n",
    "# emb_handler_corp = split_newline(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emb_handler_corp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4ba2b241689f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_handler_corp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_emb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [0] index taking first doco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0memb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0memb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_comparitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'emb_handler_corp' is not defined"
     ]
    }
   ],
   "source": [
    "emb_handler = EmbeddingHandler(emb_handler_corp, bert_emb)  # [0] index taking first doco\n",
    "emb_handler.generate_embeddings()\n",
    "emb_handler.create_comparitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_emb_h = EmbeddingHandler(emb_handler_corp, sent_tokenizer)\n",
    "sent_emb_h.generate_sent_embeddings()\n",
    "sent_emb_h.create_comparitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist, neigh, cont = emb_handler.topic_neighbors(\"and\")\n",
    "for d, w, c in zip(dist, neigh, cont):\n",
    "    print('{} {}  {}'.format(w, d, c.strip()))\n",
    "    print(\"\")\n",
    "#avg_emb = emb_handler.avg_embedding(ret_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2, neigh2, cont2 = sent_emb_h.topic_neighbors(\"and\")\n",
    "for d, w in zip(dist2, neigh2):\n",
    "    print('{} {}'.format(w, d))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63af78be9b874f4797aab7108e3edd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded summarisation model\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "#from summarizer.coreference_handler import CoreferenceHandler\n",
    "#co_handler = CoreferenceHandler(greedyness=0.4)\n",
    "#sum_model = Summarizer(sentence_handler=co_handler)\n",
    "sum_model = Summarizer()\n",
    "print(\"loaded summarisation model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for processing massive strings\n",
    "#!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ValueError: [E088] Text of length 3496277 exceeds maximum of 1000000. \n",
    "The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. \n",
    "This means long texts may cause memory allocation errors. \n",
    "If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. \n",
    "The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n",
    "\"\"\"\n",
    "\n",
    "class SummarizationHandler:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def summarize(self, texts, max_len=500000):\n",
    "        def sum_texts(texts_list, ratio=None):\n",
    "            sums = []\n",
    "            print(\"Summarising text in \" + str(len(texts_list)) + \" pieces.\")\n",
    "            for text in tqdm(texts_list):\n",
    "                if ratio is None:\n",
    "                    sums.append(self.model(text))\n",
    "                else:\n",
    "                    sums.append(self.model(text, ratio=ratio))\n",
    "            return \". \".join(sums)\n",
    "        \n",
    "        total_len = self.total_length(texts)\n",
    "        split_texts, size_split = self.split_texts(texts, total_len, max_len)\n",
    "        split_ratio = size_split / total_len\n",
    "        \n",
    "        if split_ratio >= 1:\n",
    "            cur_sum = sum_texts(split_texts)\n",
    "        else:\n",
    "            sums = sum_texts(split_texts, split_ratio)\n",
    "            cur_sum = \". \".join(sums)\n",
    "            cur_sum = sum_texts([cur_sum])\n",
    "        return cur_sum\n",
    "        \n",
    "        \n",
    "    def split_texts(self, texts, total_len, max_len):\n",
    "        \"\"\"Split texts into list of strings under max_len\"\"\"\n",
    "        num_split, size_split = self.optimal_split(total_len, max_len)\n",
    "        splits = []\n",
    "        cur_split = \"\"\n",
    "        i = 1\n",
    "        for text in texts:\n",
    "            if i == num_split:\n",
    "                # just add to last no check\n",
    "                cur_split += text\n",
    "            else:\n",
    "                if (len(cur_split) + len(text) > size_split):\n",
    "                    splits.append(cur_split)\n",
    "                    cur_split = text\n",
    "                    i += 1\n",
    "                else:\n",
    "                    cur_split += text\n",
    "        splits.append(cur_split)\n",
    "        return splits, size_split\n",
    "            \n",
    "    def optimal_split(self, total_len, max_len):\n",
    "        \"\"\"Find even split of text under max_len\"\"\"\n",
    "        under_len = int(max_len * 0.95)  # use slightly under max_len for safety\n",
    "        cur_div = 1\n",
    "        cur_size = total_len / cur_div\n",
    "        while (cur_size > under_len):\n",
    "            cur_div += 1\n",
    "            cur_size = total_len / cur_div\n",
    "        return cur_div, cur_size\n",
    "        \n",
    "    \n",
    "    def total_length(self,texts):\n",
    "        total = 0\n",
    "        for t in texts:\n",
    "            total += len(t)\n",
    "        return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:40<00:00, 40.62s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US says it's steadfast in rebuilding Afghanistan - El Paso Times Mobile e-Edition Today’s Print Ads Newsletters Customer Service Subscribe This Site Web Search powered by YAHOO ! We've got to create a situation in which al-Qaida is not coming back.\" The Pentagon on Sunday identified Air Force Lt. John D.\n",
      "Loftis as one of the service members killed in the ministry incident. \"It's an extraordinary admission of failure for us to establish the relationships that you'd have to have for a successful transition to the Afghan military and Afghan security leadership,\" Romney said. \"So they are very much in this fight trying to protect us.\" This material may not be published, broadcast, rewritten or redistributed. Investigators said they would check the control room recordings. Driver, Marcos Cordoba is being investigated on suspicion of what Argentine law calls \"guilty damage without an attempt to cause harm\". According to Noticias Argentinas news agency, Cordoba told investigators he had alerted the control room to the braking problem. Blood tests showed Cordoba had not been drinking or using drugs when the accident happened. Got ’ Read more... 14 Apr 2012 In Praise of a Good Read Read more... 14 Apr 2012 Add your comment Please leave your comment below. Your name will appear next to your comment. Read more... 12 Apr 2012 July 14: Royal Father Prays for Oshiomhole ’s Success Read more... 12 Apr 2012 Trending on ThisDayLive Featured Jonathan Okonjo -Iweala Appointments Godfather World Bank MAJEKODUNMI Thisday Tweets ThisDay Poll Should Government Negotiate with Bko Haram ? Please wait... Yes No Useful Links About Us Advertise RSS Alerts Archives Contact Us Twitter Facebook Copyright © 2012 Leaders & Company Limited, Co. Regn . International advisers working at Afghan ministries were recalled out of fears of another attack. Another demonstrator was killed by Afghan police, he added. He declined to comment on whether there were any wounded. In 2006, gunmen abducted Altaei , an Iraqi -born reservist who was 41 at the time, after he d sneaked out of the Green Zone in Baghdad to visit his new Iraqi wife. All I heard was a woman screaming on the phone, he said, recalling the phone call from Altaeis wife seconds after his capture from a busy Baghdad street in daylight. He left Iraq when he was 12 and came back to help. He was abducted soon after the couple returned to Iraq , Qanbar said, when he was en route to bring lamb to his wifes home for a religious feast. The group claiming to hold him was a previously unknown band of militants. Afghanistan 's president renewed his calls for calm Sunday in a televised address to the nation after the burning of Qurans at a U.S. base sparked five days of deadly protests and prompted the international military coalition to pull its advisers from Afghan ministries out of fear that they would become the next targets. More than 30 people have been killed, including four U.S. troops, in six days of unrest. \"Tensions are running very high here, and I think we need to let things calm down, return to a more normal atmosphere, and then get on with business,\" Ambassador Ryan Crocker told CNN 's \"State of the Union .\" \" This is not the time to decide that we're done here,\" he said. The building is one of the city's most heavily guarded buildings, and the slayings raised doubts about safety as coalition troops continue their withdrawal. The advisers are key to helping improve governance and preparing the country's security forces to take on more responsibility. He did not provide further details about the suspect or his possible motive. Afghanistan 's defence and interior ministers were to visit Washington this week, but they called off the trip to consult with other Afghan officials and religious leaders on how to stop the violence, Pentagon press secretary George Little said. The Afghan officials had planned to meet with Defence Secretary Leon Panetta and Joint Chiefs Chairman Gen . Last week, during a protest in Nangarhar province in the east , two other U.S. troops were killed when an Afghan soldier turned his gun on them. Armed individuals in the crowd fired on police and threw grenades at the U.S. base on the city outskirts, said Amanuddin Quriashi , administrator in Imam Sahib. A NATO spokesman said an explosion occurred outside the base, but that the grenades did not breach its defences. Karzai did not mention the killings at the ministry in his opening remarks but when a reporter asked, he said he was \"saddened\" by their deaths. \"We continue to move forward and stand by our Afghan partners in this campaign. Jimmie Cummings , a spokesman for the international force. Germany has withdrawn troops early from an outpost in northern Afghanistan because of the Quran protests. But the regional commander decided to pull the remaining 50 German troops back to a large base in Kunduz because of nearby demonstrations. ___ Associated Press Writer Heidi Vogt in Kabul contributed to this report. Tip someone you know about this article: To: From : Last changed: February 26. The contents of this website are protected by copyright and may be used only for your personal non-commercial purposes. All other rights are reserved and commercial use is prohibited. He added that the letter was delivered by Ryan Crocker , the U.S. ambassador to Afghanistan , Thursday afternoon. Karzai 's office said Obama called the Quran burnings \"inadvertent,\" adding that the U.S. \"will take the appropriate steps to avoid any recurrence, to include holding accountable those responsible.\" Seven protesters were killed in clashes on Wednesday. continued on next page » Filed under: afghanistan | president barack obama | quran | COMMENTING RULES: To encourage an open exchange of ideas in The Hour community, we are pleased to allow readers to post comments following stories appearing on this website. You must be registered and logged in to comment on stories . Man who fled accident scene arrested for lying to police about vehicle theft 2. Norwalk man robbed at King Kennedy housing complex 3. Official: âNEON went ahead with CTEâ merger against its own legal advice; Mann may have 'misled' board 2. Councilman 's rap video has Moccia fuming 4. Westport Avenue gas station robbed at gunpoint 5. The Afghan Interior Ministry identified one of its employees as a suspect in the fatal shooting of two U.S. officers in its headquarters a day earlier, an attack that prompted NATO to recall its staff from ministries . The Koran burnings could make it far harder for NATO forces to win the trust of the Afghan public as they try to stabilise the country ahead of the withdrawal of foreign combat troops at the end of 2014. The Interior Ministry is trying to arrest the suspected individual.\" CCTV footage showed that Saboor had access to the Command and Control Centre , tucked deep inside the ministry, where the slain Americans were found, security officials told Reuters on condition of anonymity . Posted by Marisol on February 26, 2012 8:18 AM | 37 Comments del. They have only been trying for a decade...I wonder how much longer till these fools realize they are shoveling stuff against the tide and the stuff keeps attacking them? Maybe it's time to get the stuff out of there... Maxpublius | February 26, 2012 8:48 AM | Reply Blame the Saudi -handholder Bush for not recognizing evil and airing the most perfidious, destructive meme of the 21st century, \"Islam is a religion of peace\". Maxpublius | February 26, 2012 8:58 AM | Reply Then finally blame the wealthy, career-obsessed generals for carrying out illegal orders to stand up Islamic sharia states which are inhospitable, nay, deadly, to non-Muslims, without making a wimper of protest, much less quit on principal. There's nothing good in it which can't be found elsewhere and it has rot in it which is hard to find anywhere else. I have seen Muslim burn American flag â ¦ manny times â ¦. And as for the effort at \"winning hearts and minds\"? Alert | February 26, 2012 11:31 AM | Reply Maxpublius makes an excellent point! Saudis knew that and launched 9/11 on Americans from Saudi Arabia (yes, most of the hijackers were Saudis and those who were not, got their visas stamped in Riyadh . com/watch?v =PuAg - tOsrWU ) thereby, covering-up Saudi Arabia from 9/11 terrorist attacks. Readers will recall that AFTER 9/11, Bush DOUBLED, yes. freerepublic.com/focus/f-news/1999287/posts ). This would FINALLY make them see what they have been missing for centuries that we mean well and we are nice people. gail duituturaga replied to comment from fineliving56 | February 26, 2012 11:41 AM | Reply You know that at the book makers they do not wear white gloves when putting this koran book together nor do the sellers of the book by this title. Nor do the loggers that cut down the trees to make the paper. Could be that dogs pissed on the tree or rats ran amoke in the branches. I would watch as long as it is factual and brave. They'll remember in November -- and thereafter. My sincere sorrow for the families of the dead. It is clear that Newt and Rick at least understand much of the criminal gang bangers, no matter any other flaws. No one is living without flaws, so I still say, PICK YOUR POISON , AND PICK IT WELL! Whats Up Doc | February 26, 2012 1:22 PM | Reply I would take bets that this killer will never face justice and US will not insist upon it either they don't want to get Muslims more upset. No system which encourages hate and domination can lead to peace and true democracy. Makes one wonder if moderates even exist at all ! Not fully sure of what is going on, but I know evil of the gang when I see it, at least, and of grievous behavior of the regime, on multiple levels. We've spent ten years there trying to civilize these barbarians, and they are as bad as when we first went in . Some protesters hoisted the white Taliban flag. ................................... And there's another \"Afghan soldier\" who all at once exhibits \"Sudden Jihad Syndrome\". fineliving56 replied to comment from TJFREEDOMJIHAD | February 26, 2012 2:08 PM | Reply Thank you for your worm post As Arab Ex Muslimah emigrant, I am still and will ever be surprised that America has not YET ROUNDED UP ALL Muslims and told them simply ... GO BACK WHERE EVER YOU CAME FROM. I would not blame them ..... in fact I expected ..because where I came from not only will they kick people they do not like out, they will try to kill them. Kill anything in your path, bomb as much infrastructure as you can and get the hell out of that crap hole once and for all and come home. To think otherwise is sheer stupidity and after a decade of trying, don't you think any moron would have realized by now that there is no hope, that you have to understand who you are fighting. Charles Griffith replied to comment from ImNoDhimmi | February 27, 2012 5:58 AM | Reply Thank you for reminding me....I couldn't agree more. The horrific contrast in every possible aspect between these two Democrat Party Presidents is impossible to catalogue. This was something that happened as a mistake. Romney ’s references to wealth put him in Dan Quayle territory ” – WashPost A2 , Melinda Henneberger column, “ She the People: The world as women see it” : “Barack Obama never tried to come off as just your average dude — he couldn’t have, but the change sure was nice — and Romney needn ’t, either. The President and his wife come; they do a nice little photo thing. You walk in and the tables are all out of the place. That person is Greg Brock , a senior editor who has been overseeing corrections since 2006. Mr.\n",
      "Brock estimated, conservatively, that The Times receives 14,000 reports of errors in a year. A] trend emerged a couple of years ago: too many errors in obituaries. http://nyti.ms/AuqDue IF YOU READ ONLY ONE THING: Ann Romney , campaigning alongside her husband at an Americans for Prosperity Forum in Troy , Mich. , told the crowd that she 's decided her husband is done debating, per Reid Epstein and Maggie Haberman : “Maybe I should just do all the talking and let him just stand here and watch me … I 've also decided no more debates. PUNDIT PREP – L.A.\n",
      "Times A1 , “ Mitt Romney grew up on politics : He is running as the anti-politician, but was immersed in the life by his parents’ bids,” by Maeve Reston in Detroit : “[A] t the [1962] tulip festival in Holland , Mich. [when his father was running for governor], … 15-year-old Mitt Romney suited up in Dutch trousers, a hat and wooden shoes. … ‘ These jobs are needed now to help America get back to work! We are heading deeper into unsustainable debt. Fact: The debt we have ‘is’ currently unsustainable. I have heard even conservatives saying we’ll be just like Greece soon. Fact: Greece never had the burden of defense of the free world that we have. Reply Quote Report Abuse 66gardeners Party: NA Reply #2 Feb. 26, 2012 - 11:36 AM EST google Asperger 's syndrome. Romney is incapable of empathy towards others or communicating effectively. This is not his fault, but it does make him ineligible for POTUS. Reply Quote Report Abuse Calirangr Party: Republican Reply #3 Feb. 26, 2012 - 1:10 PM EST 66...where in the constitution does it say that the Prez needs to be Empathic? What's more important, I just don't see how you 'do ' see it. Inspiring others to do what they otherwise would not. I'm the case of Barack Obama , how hard it is to inspire folks to take more from those who have it (...especially when you have majorities in both teh house and senate . Now, give me an example of where taking more from others has worked. It's too funny that the left derides Romney for being too successfull. Maybe it's working out well for you....but it's not for the rest of us. Reply Quote Report Abuse skint Party: Independent Reply #5 Feb. 26, 2012 - 11:21 PM EST the biggest misconception would be the media being \"FAIR\" All the 20 and 30 someting so-called \"JournaLIST are the product of the radical 60 professors. Browse all e-mail newsletters Related To Story Outrage Spreading Over Quran Burning Embed this Video x Email Facebook Digg Twitter Reddit Delicious Link More Deaths In Quran -Burning Protests From Nick Paton Walsh and Masoud Popalzai POSTED: 5:07 am MST February 24, 2012 UPDATED: 8:21 am MST February 24, 2012 KABUL , Afghanistan ( CNN ) -- A large crowd swarmed a military base and numerous demonstrations turned deadly Friday in Afghanistan , the fourth day of fallout after NATO troops burned Qurans at a military base, officials said. We'll come through this together as a unit.\" We stand for something greater than that.\" Gen.\n",
      "Sher Mohammad Karimi , the Afghan National Army 's chief of staff, joined Allen on the trip and thanked the troops for their \"sacrifices for humanity, not just the Afghan people.\" NATO posted a video of the visit on its official YouTube channel, without naming the base. One civilian was killed and 11 others were injured, the official said. NATO troops incinerated Islamic religious material at Bagram Airfield earlier this week. CNN 's Josh Levs contributed to this report. Copyright CNN 2012 The following are comments from our users. By posting a comment you agree to accept our Terms of Use . To report an offensive or otherwise inappropriate comment, click the \"Flag\" link that appears beneath that comment. Comments that are flagged by a set number of users will be automatically removed. Recommend MORE Leave Your Reply Below Click here to cancel reply. Name (required) Email Location Web Your comments may appear in The Express Tribune paper. Notify me of followup comments via e-mail Comments are moderated and generally will be posted if they are on-topic and not abusive. Run for your life Maestro96 @ Bluejay : It was sarcasm .\n",
      "complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sum_handler = SummarizationHandler(sum_model)\n",
    "\n",
    "test_topic = corpus.loc[corpus['topic_id'] == 1]['text'][0:10]\n",
    "test_topic_sum = sum_handler.summarize(test_topic)\n",
    "\n",
    "print(test_topic_sum)\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading 'Nugget' Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "nugget_dir = \"/nfs/TemporalSummarization/ts13/results\"\n",
    "updates_sampled_path = nugget_dir + \"/updates_sampled.tsv\"\n",
    "nuggets_path = nugget_dir + \"/nuggets.tsv\"\n",
    "nug_matches_path = nugget_dir + \"/matches.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_duplicates(df):\n",
    "    seen = set()\n",
    "    seen_twice = set()\n",
    "    for docid in df['docid']:\n",
    "        if docid not in seen:\n",
    "            seen.add(docid)\n",
    "        else:\n",
    "            seen_twice.add(docid)\n",
    "    return seen_twice\n",
    "\n",
    "def create_update_df():\n",
    "    \"\"\"Data Frame containing information about docs which have updates/multiple instances in corpus\"\"\"\n",
    "    def create_entry(row, col_tags):\n",
    "        entry = {}\n",
    "        for col in col_tags:\n",
    "            entry[col] = row[col]\n",
    "        return entry\n",
    "    \n",
    "    col_tags = ['docid', 'streamid', 'epoch', 'yyyymmddhh', 'zulu']\n",
    "    entry_list = []\n",
    "    dups = find_duplicates(corpus)\n",
    "    for docid in tqdm(dups, position=0, leave=True):\n",
    "        d = corpus[corpus['docid'] == docid]\n",
    "        for index, row in d.iterrows():\n",
    "            entry = create_entry(row, col_tags)\n",
    "            entry_list.append(entry)\n",
    "             \n",
    "    update_df = pd.DataFrame(entry_list)\n",
    "    update_df = update_df.set_index(col_tags)\n",
    "    return update_df\n",
    "                    \n",
    "                \n",
    "def create_nugget_df():\n",
    "    \"\"\"Dataframe containing nugget data and its appearances in corpus\"\"\"\n",
    "    def create_entry(row, reg_cols, multi_col_vals=None):\n",
    "        entry_dict = {}\n",
    "        for col in reg_cols:\n",
    "            entry_dict[col] = row[col]\n",
    "        if multi_cols is not None:\n",
    "            for k,v in multi_col_vals.items():\n",
    "                entry_dict[k] = v\n",
    "        return entry_dict\n",
    "        \n",
    "    nuggets_tsv = pd.read_csv(nuggets_path, \"\\t\")\n",
    "    entry_list = []\n",
    "    reg_cols = ['query_id', 'nugget_id', 'importance', 'nugget_len', 'nugget_text']\n",
    "    multi_cols = ['docid', 'streamid', 'epoch', 'yyyymmddhh']  # multiindex cols\n",
    "    num_cols = ['query_id', 'importance', 'nugget_len', 'epoch']\n",
    "    \n",
    "    pbar = tqdm(total=len(nuggets_tsv), position=0, leave=True)\n",
    "    for index, row in nuggets_tsv.iterrows():\n",
    "        # find where nugget appears in text\n",
    "        nug_text = row['nugget_text']\n",
    "        topic_id = 0\n",
    "        try:\n",
    "            topic_id = int(row['query_id'])  # make sure pattern match in correct topic\n",
    "        except ValueError:\n",
    "            pbar.update()\n",
    "            continue  # topic_id is unknown string in tsv file, e.g. \"TS13.07\"\n",
    "        appears = corpus[corpus['topic_id'] == topic_id]\n",
    "        appears = appears[appears['text'].str.contains(re.escape(nug_text))]  # make sure no accidental regex pattern\n",
    "        \n",
    "        # gather information on docs it appears in\n",
    "        dups = find_duplicates(appears)  # get docids where nugget appears\n",
    "        for docid in dups:\n",
    "            upd = appears[appears['docid'] == docid]  # get docs with this docid\n",
    "            for i, r in upd.iterrows():  # gather info on each doc with this docid (e.g. streamid, epoch etc.)\n",
    "                multi_col_vals = {}\n",
    "                for multi_col in multi_cols:\n",
    "                    multi_col_vals[multi_col] = r[multi_col]\n",
    "                entry = create_entry(row, reg_cols, multi_col_vals=multi_col_vals)\n",
    "                entry_list.append(entry)\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "    \n",
    "    # form multi-index nugget dataframe\n",
    "    reg_cols.extend(multi_cols)  # get new multiindex order\n",
    "#     index = pd.MultiIndex.from_frame(pd.DataFrame(entry_list))\n",
    "#     nugget_df = pd.DataFrame(entry_list, index=index)\n",
    "    nugget_df = pd.DataFrame(entry_list)\n",
    "    #nugget_df = pd.DataFrame(entry_list)\n",
    "#     reg_cols.extend(multi_cols)  # get new multiindex order\n",
    "    #nugget_df = nugget_df.set_index(reg_cols)\n",
    "#     for col in reg_cols:\n",
    "#         if col == \"query_id\":\n",
    "#             col = \"topic_id\"\n",
    "#     nugget_df.rename(reg_cols, inplace=True)\n",
    "    nugget_df[num_cols] = nugget_df[num_cols].apply(pd.to_numeric, errors='coerce', axis=1)  # convert appropriate cols to numerical values\n",
    "    nugget_df.rename(columns={'query_id':'topic_id'}, inplace=True)  # topic_id matches other dataframes\n",
    "    return nugget_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1366/1366 [00:39<00:00, 34.96it/s] \n"
     ]
    }
   ],
   "source": [
    "nugget_df = create_nugget_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>nugget_id</th>\n",
       "      <th>importance</th>\n",
       "      <th>nugget_len</th>\n",
       "      <th>nugget_text</th>\n",
       "      <th>docid</th>\n",
       "      <th>streamid</th>\n",
       "      <th>epoch</th>\n",
       "      <th>yyyymmddhh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.052</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Hundreds injured</td>\n",
       "      <td>dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420</td>\n",
       "      <td>2012-02-23-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.052</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Hundreds injured</td>\n",
       "      <td>dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420</td>\n",
       "      <td>2012-02-23-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.054</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>February 22, 2012</td>\n",
       "      <td>ddd856e0a350c52b7c078c9bcdd609d9</td>\n",
       "      <td>1329930660-ddd856e0a350c52b7c078c9bcdd609d9</td>\n",
       "      <td>1329930660</td>\n",
       "      <td>2012-02-22-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.054</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>February 22, 2012</td>\n",
       "      <td>ddd856e0a350c52b7c078c9bcdd609d9</td>\n",
       "      <td>1329930660-ddd856e0a350c52b7c078c9bcdd609d9</td>\n",
       "      <td>1329930660</td>\n",
       "      <td>2012-02-22-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.054</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>February 22, 2012</td>\n",
       "      <td>ecda22bcfc10da137b49f0089bd5d7f5</td>\n",
       "      <td>1329916140-ecda22bcfc10da137b49f0089bd5d7f5</td>\n",
       "      <td>1329916140</td>\n",
       "      <td>2012-02-22-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id      nugget_id  importance  nugget_len        nugget_text  \\\n",
       "0         1  VMTS13.01.052           3           2   Hundreds injured   \n",
       "1         1  VMTS13.01.052           3           2   Hundreds injured   \n",
       "2         1  VMTS13.01.054           1           3  February 22, 2012   \n",
       "3         1  VMTS13.01.054           1           3  February 22, 2012   \n",
       "4         1  VMTS13.01.054           1           3  February 22, 2012   \n",
       "\n",
       "                              docid  \\\n",
       "0  dd95d5dbbff443c3ddae4e34a5d2e9c1   \n",
       "1  dd95d5dbbff443c3ddae4e34a5d2e9c1   \n",
       "2  ddd856e0a350c52b7c078c9bcdd609d9   \n",
       "3  ddd856e0a350c52b7c078c9bcdd609d9   \n",
       "4  ecda22bcfc10da137b49f0089bd5d7f5   \n",
       "\n",
       "                                      streamid       epoch     yyyymmddhh  \n",
       "0  1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1  1330041420  2012-02-23-23  \n",
       "1  1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1  1330041420  2012-02-23-23  \n",
       "2  1329930660-ddd856e0a350c52b7c078c9bcdd609d9  1329930660  2012-02-22-17  \n",
       "3  1329930660-ddd856e0a350c52b7c078c9bcdd609d9  1329930660  2012-02-22-17  \n",
       "4  1329916140-ecda22bcfc10da137b49f0089bd5d7f5  1329916140  2012-02-22-13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(nugget_df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1245/1245 [00:02<00:00, 586.81it/s]\n"
     ]
    }
   ],
   "source": [
    "update_df = create_update_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th>streamid</th>\n",
       "      <th>epoch</th>\n",
       "      <th>yyyymmddhh</th>\n",
       "      <th>zulu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1ac1331e640f51ce5ec082c2d4645c1c</th>\n",
       "      <th>1347511393-1ac1331e640f51ce5ec082c2d4645c1c</th>\n",
       "      <th>1347511393</th>\n",
       "      <th>2012-09-13-04</th>\n",
       "      <th>2012-09-13T04:43:13.0Z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347511394-1ac1331e640f51ce5ec082c2d4645c1c</th>\n",
       "      <th>1347511394</th>\n",
       "      <th>2012-09-13-04</th>\n",
       "      <th>2012-09-13T04:43:14.0Z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cc4bfe4c92b325f18a38601c9883dfd1</th>\n",
       "      <th>1353877843-cc4bfe4c92b325f18a38601c9883dfd1</th>\n",
       "      <th>1353877843</th>\n",
       "      <th>2012-11-25-21</th>\n",
       "      <th>2012-11-25T21:10:43.0Z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353877262-cc4bfe4c92b325f18a38601c9883dfd1</th>\n",
       "      <th>1353877262</th>\n",
       "      <th>2012-11-25-21</th>\n",
       "      <th>2012-11-25T21:01:02.0Z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353877279-cc4bfe4c92b325f18a38601c9883dfd1</th>\n",
       "      <th>1353877279</th>\n",
       "      <th>2012-11-25-21</th>\n",
       "      <th>2012-11-25T21:01:19.0Z</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(1ac1331e640f51ce5ec082c2d4645c1c, 1347511393-1ac1331e640f51ce5ec082c2d4645c1c, 1347511393, 2012-09-13-04, 2012-09-13T04:43:13.0Z), (1ac1331e640f51ce5ec082c2d4645c1c, 1347511394-1ac1331e640f51ce5ec082c2d4645c1c, 1347511394, 2012-09-13-04, 2012-09-13T04:43:14.0Z), (cc4bfe4c92b325f18a38601c9883dfd1, 1353877843-cc4bfe4c92b325f18a38601c9883dfd1, 1353877843, 2012-11-25-21, 2012-11-25T21:10:43.0Z), (cc4bfe4c92b325f18a38601c9883dfd1, 1353877262-cc4bfe4c92b325f18a38601c9883dfd1, 1353877262, 2012-11-25-21, 2012-11-25T21:01:02.0Z), (cc4bfe4c92b325f18a38601c9883dfd1, 1353877279-cc4bfe4c92b325f18a38601c9883dfd1, 1353877279, 2012-11-25-21, 2012-11-25T21:01:19.0Z)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(update_df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.53s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five killed in Afghan protests over Quran burning – The Express Tribune RSS Today's Paper Subscribe beta 2.0 High: 28 ° C Low : 19 ° C Home Pakistan Business World Sports Life &amp ; Style Multimedia Opinion Magazine Blogs Jobs Classifieds Alerts Five killed in Afghan protests over Quran burning At least five Afghan ­s were shot dead and dozens wounde ­ d in clashe­s betwee ­ n police and demons­trator­s. By AFP Published: February 22, 2012 Afghan policemen march towards protesters during a protest near a U.S. military base in Kabul February 22, 2012. PHOTO: REUTERS KABUL: At least five Afghans were shot dead and dozens wounded Wednesday in clashes between police and demonstrators protesting over the burning of the Holy Quran at a US -run military base, officials said. In Kabul , hundreds of people poured onto the Jalalabad road, throwing stones at US military base Camp Phoenix , where troops guarding the base fired into the air and black smoke from burning tyres rose, an AFP photographer said. Afghanistan is a deeply religious country where slights against Islam have frequently provoked violent protests and Afghans were incensed that any Western troops could be so insensitive, 10 years after the 2001 US -led invasion. The US commander in Afghanistan , General John Allen , apologised and ordered an investigation into the incident, admitting that religious materials, including the Holy Quran “were inadvertently taken to an incineration facility”. Doctor Ahmad Ali said one person was killed and 10 others had been admitted to Jalalabad hospital with gunshot wounds. He is a young man from the protesters,” Ali said. A spokesman for the US -led NATO force in Afghanistan , Lieutenant Colonel Jimmie Cummings , told AFP he could not confirm that the Holy Quran had been burnt by Americans at the base, saying it was still under investigation. Comments (7) Share this article Print this page Email a friend Related Stories 21 Feb 2012 US commander apologises over Quran mistreatment 22 Feb 2012 Violent Afghan protests spread over Quran burning Tweet Share this article Reader Comments (7) All Comments Reader's Recommendations sars Feb 22, 2012 - 7:55PM Reply doesnt anyone think that this is ridiculous??. Wah Jee Wah ..Infidel Oppression at its peak! Recommend Rajendra Kalkhande Feb 23, 2012 - 4:34AM Reply Sorry if I hurt Muslim sentiments. There are millions of such books, but message remains same. Is message more important or the package of papers which carry it? Like “Atma” can never be destroyed, same holds true of Quran. @someone, Even if I burn bible before those arrogant US / NATO marines they will not feel anything . Burning Bible before them even though they might be christians will not affect them in any way ..They are burning your Holy Book to provoke you ..So use common sense and ignore and they lose their power to hurt you ..This is a childhood technique mothers teach their children when they are taunted ..\n",
      "IF you still think you want to attack them then see the results .Do you want to be 1 among the five who have been killed? Now we download some verses and delete them after composing. Where this country is going no body knows. Recommend MORE Leave Your Reply Below Click here to cancel reply. Name (required) Email Location Web Your comments may appear in The Express Tribune paper. For this reason we encourage you to provide your city. Notify me of followup comments via e-mail Comments are moderated and generally will be posted if they are on-topic and not abusive. For more information, please see our Comments FAQ . AFP Prince Charles may work on TV show Britai­n 's Prince Charle­s is being lined up for a role on a childr­ en 's TV show. Our Correspondent Comment & Debate Pakistan test fires short-range ballistic missile Proud Pakistani That should keep the Indians on their feet! We can defend ourself better than you think. Mysterious death: Scottish lawyer found dead in Lahore hotel Proud Pakistani That should keep the indians on their feet. Cricket: Latif criticises Sami snub popsaeed @ shadi : Its not punjab cricket board its Pakistan cricket board dont try to prevail \"tasub\" ‘Forced conversion’ : Hindu community demands release of ‘kidnapped’ girl VINOD @ Zeta : I fully agree with you papers like ET and Dawn must be banned for talking the truth and bringing ... Appeal process: Another attempt to repatriate Aafia Siddqui Truth Seeker @ Harry Stone : \"Actually Raymond Davis was a diplomat. What is happening in Xinjiang and Baluchistan are very simple ... Shiraz Uppal: A new beginning MAD those talking about singers quitting and finding religion when their careers are going down should keep in mind Junaid Jamshed . ... Appeal process: Another attempt to repatriate Aafia Siddqui Truth Seeker @ Samir : \"Did you make this up? The tragedy comes after a series of train accidents in Argentina and will likely bring about a prolonged legal battle. Argentina 's government said they would join plaintiffs against Trenes de Buenos Aires ( TBA ), the company that holds the concession on the Sarmiento line where the accident occurred. Thomson Reuters journalists are subject to an Editorial Handbook which requires fair presentation and disclosure of relevant interests. For a complete list of exchanges and delays, please click here . Mr.\n",
      "Schiavi said brake failure was the suspected cause. He said one car pierced into another by nearly 20 feet. Passengers told the local news media that the train, which is operated by the private company Trenes de Buenos Aires , was traveling faster than normal and had struggled to slow down when braking at stations ahead of Once Station . People were pulling others out of the wreckage. The bus had crossed the tracks when the barrier was down. In February 2011, four people were killed in a collision of two trains. The newspaper La Nación said the accident was the third-deadliest in Argentina ’s history, surpassed only by a 1972 collision that killed 142 people and a 1978 accident involving trains and other vehicles that left 55 dead. Get 50% Off The New York Times &amp ; Free All Digital Access. Get Free E -mail Alerts on These Topics Railroad Accidents and Safety Argentina Buenos Aires ( Argentina ) Getting fat but staying fit? With Your Friends Explore news, videos and much more based on what your friends are reading and watching. Publish your own activity and retain full control. YOUR FRIENDS' ACTIVITY prev next YOU ON YAHOO! SPORTS Your Activity |Social : OFFON Turn Social ONRemind me when I share |Options What is this?Not you? And now Umbro have posted more pictures on their blog: The new England shirt ( blog.umbro.com ) The new red and white design has already courted plenty of controversy — the FA 's latest contract with Umbro stipulates that the national kit will be changed roughly every 18 months, rather than the previous two-year cycle. and the last photo caption was brilliant too! 37 mins ago Reply Retweet Favorite DirtyTackleDirty Tackle Joey Barton first to reveal Umbro's new England shirt http ://t. co/uRD0Z6Ta 1 hr ago Reply Retweet Favorite DirtyTackleDirty Tackle @ JCheese23 Yahoo 's basketball blog covered it the other day. 1 hr ago Reply Retweet Favorite More tweets » Yahoo ! Sports Bloggers Brooks Peck Brooks Peck is a Soccer blogger for Yahoo! PostsWebsiteEmailRSS Ryan Bailey Ryan Bailey is a Soccer blogger for Yahoo! Back in 2008 \"the situation was disastrous and the braking system was terrible,\" said Despouy . State authorities have \"not taken measures since then, nor applied serious sanctions\" against the company. An angry crowd shouted \"Murderers, murderers!\" Planning Minister Julio de Vido told reporters Thursday that the government would sue the company. Most bodies were found in the first and second carriages. \"My son must be somewhere, alive or dead!\" The death toll reached 50, with 11 still unidentified, authorities said. Meanwhile passengers returned to the station, with less people occupying the front carriages of trains. I didn't sleep last night,\" said a 19-year-old who gave his name as Daniel . The Sarmiento rail line, owned by TBA , links the center of Buenos Aires to a densely populated suburb 70 kilometers (44 miles) to the west of the city. The rail network was privatized in the 1990s. The region's transit system has been plagued with serious accidents in recent years. Press Press kit Jobs Help Contact us Site map Terms of use / Privacy policy © 2006 - 2012 Copyright FRANCE 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# select random selection of streams with nugs to summarise\n",
    "test_nug = nugget_df[nugget_df['topic_id'] == 1].sample(n=10)['streamid']\n",
    "test_nug = corpus[corpus['streamid'].isin(test_nug)]\n",
    "test_nug_sum = sum_handler.summarize(test_nug['text'])\n",
    "print(test_nug_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2 out of 12\n",
      "VMTS13.01.054(1): February 22, 2012\n",
      "VMTS13.01.080(1): 55 dead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test compare the topic 1, 0:10 slice\n",
    "\n",
    "def nugget_rows(streamids):\n",
    "    nug_rows = nugget_df[nugget_df['streamid'].isin(streamids)]\n",
    "    nug_rows = nug_rows.drop_duplicates('nugget_id')\n",
    "    return nug_rows\n",
    "\n",
    "def nugget_score(nugget_rows, summary):\n",
    "    contains = {}\n",
    "    for index, row in nugget_rows.iterrows():\n",
    "        if row['nugget_text'] in summary:\n",
    "            d = {}\n",
    "            d['importance'] = row['importance']\n",
    "            d['nugget_text'] = row['nugget_text']\n",
    "            contains[row['nugget_id']] = d\n",
    "    score = 0\n",
    "    print_str = \"\"\n",
    "    for k,v in contains.items():\n",
    "        print_str += k\n",
    "        print_str += \"(\" + str(v['importance']) + \"): \" + str(v['nugget_text'])\n",
    "        print_str += \"\\n\"\n",
    "        score += v['importance']\n",
    "    total = sum(list(nugget_rows['importance']))\n",
    "    print(\"Score: \" + str(score) + \" out of \" + str(total))\n",
    "    print(print_str)\n",
    "\n",
    "nugget_score(nugget_rows(test_nug['streamid']), test_nug_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuggetReport:\n",
    "    \"\"\"Class that generates text of nuggets to be used for comparison within given bounds\"\"\"\n",
    "    def get_report():\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "\n",
    "# db_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "# db_name = 'sumresults.db'\n",
    "# db_path = db_dir + '/' + db_name\n",
    "\n",
    "# conn = sqlite3.connect(db_path)  # creates db if doesn't exist\n",
    "# c = conn.cursor()  # allows send commands to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.execute(\"\"\"CREATE TABLE results (\n",
    "#     topic_id integer,\n",
    "#     summary text\n",
    "# )\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data set dependencies successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import sqlite3\n",
    "import ipynb.fs\n",
    "print (\"loading data set dependencies successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET FILE META VARIABLES\n",
    "\n",
    "corpus_path = \"/nfs/trects-kba2014-filtered\" # directory of corpus of gzipped html files\n",
    "topics_path = corpus_path + \"/test-topics.xml\"\n",
    "doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "topic_tags = ['id', 'title', 'description', 'start','end','query','type'] # topic fields\n",
    "test_file_addr = corpus_path + \"/1/2012-02-22-15.gz\"\n",
    "# database address variables\n",
    "db_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "db_name = 'sumresults.db'\n",
    "db_path = db_dir + '/' + db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and get beautifulsoup object from markup file\n",
    "def open_markup_file(addr, gz=True, xml=False, verbose=False):\n",
    "    markup = None\n",
    "    f = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(addr)\n",
    "\n",
    "    if gz:\n",
    "        f = gzip.open(addr)\n",
    "    else:\n",
    "        f = open(addr)\n",
    "        \n",
    "    if xml == False:\n",
    "        markup = bs(f)  # open as html\n",
    "    else:\n",
    "        markup = bs(f, \"xml\")\n",
    "        \n",
    "    f.close()\n",
    "    return markup\n",
    "\n",
    "\n",
    "# parse markup and return 2D list [entry:tags]\n",
    "def parse_markup(markup, entry_list, find_tag=\"doc\", tag_list=doc_tags, topic_id=None):\n",
    "    for e in markup.find_all(find_tag):\n",
    "        entry = OrderedDict.fromkeys(tag_list)\n",
    "        if topic_id is not None:\n",
    "            entry['topic_id'] = topic_id\n",
    "        for c in e.children:  # children use direct children, descendants uses all\n",
    "            if c.name in entry:\n",
    "                entry[c.name] = str(c.string)\n",
    "            elif c.name is None and c.string != '\\n':  # inner body of <doc> tag\n",
    "                entry['text'] = str(c.string)\n",
    "        entry_list.append(list(entry.values()))\n",
    "        \n",
    "            \n",
    "# recursively find gz html files from a directory address\n",
    "def search_dir(path):    \n",
    "    # separate the subdirectories and html files \n",
    "    # (help maintain sequential order of insertion)\n",
    "    gz_paths = []\n",
    "    for f in os.scandir(path):\n",
    "        if os.path.splitext(f.path)[-1].lower() == \".gz\":\n",
    "            gz_paths.append(f.path)\n",
    "    \n",
    "    return gz_paths\n",
    "\n",
    "\n",
    "def list_to_dataframe(markup_list, tags):\n",
    "    return pd.DataFrame(markup_list, columns=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topics into dataframe\n",
    "def load_topics(path):\n",
    "    topics_list = []\n",
    "    \n",
    "    parse_markup(open_markup_file(path, gz=False, xml=True), \n",
    "                    topics_list, find_tag=\"event\", tag_list=topic_tags)\n",
    "    \n",
    "    df = list_to_dataframe(topics_list, topic_tags)\n",
    "    df['id'] = pd.to_numeric(df['id'])\n",
    "    return df\n",
    "\n",
    "topics = load_topics(topics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics loaded successfuly\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>query</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012 Buenos Aires Rail Disaster</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2012_Buenos_Aires...</td>\n",
       "      <td>1329910380</td>\n",
       "      <td>1330774380</td>\n",
       "      <td>buenos aires train crash</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2012 Pakistan garment factory fires</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2012_Pakistan_gar...</td>\n",
       "      <td>1347368400</td>\n",
       "      <td>1348232400</td>\n",
       "      <td>pakistan factory fire</td>\n",
       "      <td>accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2012 Aurora shooting</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2012_Aurora_shooting</td>\n",
       "      <td>1342766280</td>\n",
       "      <td>1343630280</td>\n",
       "      <td>colorado shooting</td>\n",
       "      <td>shooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Wisconsin Sikh temple shooting</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...</td>\n",
       "      <td>1344180300</td>\n",
       "      <td>1345044300</td>\n",
       "      <td>sikh temple shooting</td>\n",
       "      <td>shooting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                title  \\\n",
       "0   1      2012 Buenos Aires Rail Disaster   \n",
       "1   2  2012 Pakistan garment factory fires   \n",
       "2   3                 2012 Aurora shooting   \n",
       "3   4       Wisconsin Sikh temple shooting   \n",
       "\n",
       "                                         description       start         end  \\\n",
       "0  http://en.wikipedia.org/wiki/2012_Buenos_Aires...  1329910380  1330774380   \n",
       "1  http://en.wikipedia.org/wiki/2012_Pakistan_gar...  1347368400  1348232400   \n",
       "2  http://en.wikipedia.org/wiki/2012_Aurora_shooting  1342766280  1343630280   \n",
       "3  http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...  1344180300  1345044300   \n",
       "\n",
       "                      query      type  \n",
       "0  buenos aires train crash  accident  \n",
       "1     pakistan factory fire  accident  \n",
       "2         colorado shooting  shooting  \n",
       "3      sikh temple shooting  shooting  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics loaded successfuly\")\n",
    "print(display(topics[0:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Topics Into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating topic table...\n",
      "creating techniques table...\n",
      "creating instances table...\n",
      "creating meta table...\n",
      "creating updates table...\n",
      "creating nuggets table...\n",
      "creating linking nuggets table\n",
      "committing changes\n",
      "committed\n",
      "added topics_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipynb.fs.defs.database_management:134: UserWarning: The topics table already has entries\n"
     ]
    }
   ],
   "source": [
    "from .defs.database_management import create_tables, populate_topics  # import database_management functions\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "create_tables(conn, cursor,path=db_path)\n",
    "populate_topics(conn, cursor, db_path, topics)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [00:10<00:00, 23.29it/s]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [04:24<00:00,  1.10s/it]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [02:04<00:00,  1.94it/s]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [02:46<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:19<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:00<00:00,  3.97it/s]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 7...\n",
      "Loading topic 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [00:21<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [00:08<00:00, 27.32it/s]\n",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:21<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load all formatted gzipped html files into dataframe\n",
    "def load_corpus(path):\n",
    "    #corpus_list = []\n",
    "    df = pd.DataFrame(columns=doc_tags)\n",
    "    for topic_id in topics['id'].to_numpy():\n",
    "        print(\"Loading topic \" + str(topic_id) + \"...\")\n",
    "        topic_list = []\n",
    "        id_path = corpus_path + \"/\" + str(topic_id) + \"/\"  # every topic id correlates to subfolder named after it\n",
    "        gz_paths = search_dir(id_path)\n",
    "        for gz_path in tqdm(gz_paths, position=0, leave=True):\n",
    "            parse_markup(open_markup_file(gz_path, verbose=False),\n",
    "                            topic_list, topic_id=topic_id)\n",
    "        topic_df = list_to_dataframe(topic_list, doc_tags)\n",
    "        df = df.append(topic_df)\n",
    "    df['epoch'] = pd.to_numeric(df['epoch'])\n",
    "    return df\n",
    "\n",
    "corpus = load_corpus(corpus_path)\n",
    "print(\"Corpus loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded succesfully: 12261 documents loaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>streamid</th>\n",
       "      <th>docid</th>\n",
       "      <th>yyyymmddhh</th>\n",
       "      <th>kbastream</th>\n",
       "      <th>zulu</th>\n",
       "      <th>epoch</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1330269540-995ed81eafa60498872335da7dce1386</td>\n",
       "      <td>995ed81eafa60498872335da7dce1386</td>\n",
       "      <td>2012-02-26-15</td>\n",
       "      <td>news</td>\n",
       "      <td>2012-02-26T15:19:00.000000Z</td>\n",
       "      <td>1330269540</td>\n",
       "      <td>US says it's steadfast in rebuilding Afghanist...</td>\n",
       "      <td>\\nUS says it's steadfast in rebuilding Afghani...</td>\n",
       "      <td>http://www.elpasotimes.com/politics/ci_20049216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1330268520-f42a863b58b2cc53cc716953c40f6065</td>\n",
       "      <td>f42a863b58b2cc53cc716953c40f6065</td>\n",
       "      <td>2012-02-26-15</td>\n",
       "      <td>news</td>\n",
       "      <td>2012-02-26T15:02:00.000000Z</td>\n",
       "      <td>1330268520</td>\n",
       "      <td>Argentina Train Crash: Driver Blames Faulty Br...</td>\n",
       "      <td>\\nArgentina Train Crash: Driver Blames Faulty ...</td>\n",
       "      <td>http://www.thisdaylive.com/articles/argentina-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1330270020-e47e013ec518f5fdd253ce28231f509f</td>\n",
       "      <td>e47e013ec518f5fdd253ce28231f509f</td>\n",
       "      <td>2012-02-26-15</td>\n",
       "      <td>news</td>\n",
       "      <td>2012-02-26T15:27:00.000000Z</td>\n",
       "      <td>1330270020</td>\n",
       "      <td>The Alaska Journal of Commerce Local News Oil ...</td>\n",
       "      <td>\\nThe Alaska Journal of Commerce Local News Oi...</td>\n",
       "      <td>http://ap.alaskajournal.com/pstories/20120226/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1330268700-8078290575c82c8dd0e4e99370447bd2</td>\n",
       "      <td>8078290575c82c8dd0e4e99370447bd2</td>\n",
       "      <td>2012-02-26-15</td>\n",
       "      <td>news</td>\n",
       "      <td>2012-02-26T15:05:00.000000Z</td>\n",
       "      <td>1330268700</td>\n",
       "      <td>U.S. military receives remains of last soldier...</td>\n",
       "      <td>\\nU.S. military receives remains of last soldi...</td>\n",
       "      <td>http://www.islandpacket.com/2012/02/26/1978117...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic_id                                     streamid  \\\n",
       "0        1  1330269540-995ed81eafa60498872335da7dce1386   \n",
       "1        1  1330268520-f42a863b58b2cc53cc716953c40f6065   \n",
       "2        1  1330270020-e47e013ec518f5fdd253ce28231f509f   \n",
       "3        1  1330268700-8078290575c82c8dd0e4e99370447bd2   \n",
       "\n",
       "                              docid     yyyymmddhh kbastream  \\\n",
       "0  995ed81eafa60498872335da7dce1386  2012-02-26-15      news   \n",
       "1  f42a863b58b2cc53cc716953c40f6065  2012-02-26-15      news   \n",
       "2  e47e013ec518f5fdd253ce28231f509f  2012-02-26-15      news   \n",
       "3  8078290575c82c8dd0e4e99370447bd2  2012-02-26-15      news   \n",
       "\n",
       "                          zulu       epoch  \\\n",
       "0  2012-02-26T15:19:00.000000Z  1330269540   \n",
       "1  2012-02-26T15:02:00.000000Z  1330268520   \n",
       "2  2012-02-26T15:27:00.000000Z  1330270020   \n",
       "3  2012-02-26T15:05:00.000000Z  1330268700   \n",
       "\n",
       "                                               title  \\\n",
       "0  US says it's steadfast in rebuilding Afghanist...   \n",
       "1  Argentina Train Crash: Driver Blames Faulty Br...   \n",
       "2  The Alaska Journal of Commerce Local News Oil ...   \n",
       "3  U.S. military receives remains of last soldier...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nUS says it's steadfast in rebuilding Afghani...   \n",
       "1  \\nArgentina Train Crash: Driver Blames Faulty ...   \n",
       "2  \\nThe Alaska Journal of Commerce Local News Oi...   \n",
       "3  \\nU.S. military receives remains of last soldi...   \n",
       "\n",
       "                                                 url  \n",
       "0    http://www.elpasotimes.com/politics/ci_20049216  \n",
       "1  http://www.thisdaylive.com/articles/argentina-...  \n",
       "2  http://ap.alaskajournal.com/pstories/20120226/...  \n",
       "3  http://www.islandpacket.com/2012/02/26/1978117...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus loaded succesfully: \" + str(len(corpus)) + \" documents loaded.\")\n",
    "print(display(corpus[0:4]))\n",
    "# there is an error in the dataset that article at 1 is misplaced in topic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # duplicates are updates to the page\n",
    "# find_nug = corpus[corpus['streamid'] == \"1329915660-47ed792a77d798dda8697654e8fcbb43\"]\n",
    "# # 1329915300-46c9b2db03fbaf7d2a903bbfa7ff3c93-3\n",
    "# # duplicate found when -3 taken away\n",
    "# dup_nug = corpus[corpus['streamid'] == \"1329915300-46c9b2db03fbaf7d2a903bbfa7ff3c93\"]\n",
    "# print(corpus[corpus['docid'] == \"47ed792a77d798dda8697654e8fcbb43\"])\n",
    "# print(find_nug)\n",
    "# print(dup_nug)\n",
    "# # print(dup_nug['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Topic Modelling needs better preprocessing (stop words/lemmas etc.)\n",
    "    - stop words\n",
    "    - lemmatization (stemming is faster but is rule-based with more false transformations)\n",
    "    - special char removal\n",
    "- Could try removing junk at top of docs through REs/spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing dependencies import successful\n"
     ]
    }
   ],
   "source": [
    "# ## IMPORT DEPENDENCIES\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# print(\"preprocessing dependencies import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f410b5435e8>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")  # try experimenting disabling parts of spacy pipeline see if .sents still works\n",
    "\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\n",
    "# nlp.remove_pipe('tagger')\n",
    "# nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_docs = corpus.loc[0:3,:]  # work on just top 3 for now\n",
    "\n",
    "# # in data frame, split sentences into list by the newline delimiter\n",
    "# #test_docs['text'] = test_docs['text'].map(lambda x: x.splitlines())\n",
    "\n",
    "# # map the non-preprocessed string list to a preprocessed string list\n",
    "\n",
    "# #@Tokenize\n",
    "# def spacy_tokenize(string):\n",
    "#     tokens = list()\n",
    "#     doc = nlp(string)\n",
    "#     for token in doc:\n",
    "#         if not token.is_stop:\n",
    "#             tokens.append(token)\n",
    "#     return tokens\n",
    "\n",
    "# #@Normalize\n",
    "# def normalize(tokens):\n",
    "#     normalized_tokens = list()\n",
    "#     for token in tokens:\n",
    "#         normalized = token.text.lower().strip()\n",
    "#         if ((token.is_alpha or token.is_digit)):\n",
    "#             normalized_tokens.append(normalized)\n",
    "#     return normalized_tokens\n",
    "\n",
    "# #@Tokenize and normalize\n",
    "# def tokenize_normalize(string):\n",
    "#     return normalize(spacy_tokenize(string))\n",
    "\n",
    "# # test_prep = []\n",
    "# # for doc in test_docs['text']:\n",
    "# #     d = []\n",
    "# #     for sent in doc:\n",
    "# #         d.append(tokenize_normalize(sent))\n",
    "# #     test_prep.append(d)\n",
    "        \n",
    "# print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_prep[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Word and Sentence Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# #sent_model = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "# sent_tokenizer = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# #word_model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "# word_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "- LDA uses K-means clustering\n",
    "- HDA learns num topics automatically (Bayesian non-parametric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dependencies\n"
     ]
    }
   ],
   "source": [
    "# # word level topic modelling\n",
    "# # needs better preprocessing (remove stopwords/lemmitization etc)\n",
    "# # maybe add REs/other preprocessing remove uninformative junk at top of docs\n",
    "\n",
    "# import gensim\n",
    "# from gensim import corpora\n",
    "\n",
    "# print(\"loaded dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded cell\n"
     ]
    }
   ],
   "source": [
    "# class TopicModeller: \n",
    "#     def __init__(self):\n",
    "#         self.model = None\n",
    "#         self.corpus_dict = None\n",
    "#         self.weighted_tokens = None\n",
    "#         self.print_topics = None\n",
    "        \n",
    "#     def weigh_tokens(self, texts, method=\"bow\"):\n",
    "#         \"\"\"Perform token weighting scheme on text and return with dict\"\"\"\n",
    "#         def create_dictionary(texts):\n",
    "#             \"\"\"Create a gensim dictionary of index-word mappings\"\"\"\n",
    "#             return corpora.Dictionary(texts)\n",
    "    \n",
    "#         flat_texts = [token for sent in texts for token in sent]  # should be fast\n",
    "#         self.corpus_dict = create_dictionary(flat_texts)\n",
    "#         if method == \"bow\":\n",
    "#             self.weighted_tokens = [self.corpus_dict.doc2bow(text) for text in flat_texts]\n",
    "#         else:\n",
    "#             raise Exception(\"Incorrect method parameter\")\n",
    "            \n",
    "#     def model_topics(self, method=\"lda\", num_topics=10):\n",
    "#         if method == \"lda\":\n",
    "#     #         model = gensim.models.ldamodel.LdaModel(weighted_tokens, num_topics=NUM_TOPICS, \n",
    "#     #                                                 id2word=corpus_dict, passes=15)\n",
    "#             self.model = gensim.models.ldamulticore.LdaMulticore(self.weighted_tokens, num_topics=num_topics, \n",
    "#                                                     id2word=self.corpus_dict, passes=15)\n",
    "#         else:\n",
    "#             raise Exception(\"Incorrect method parameter\")\n",
    "            \n",
    "#         self.print_topics = self.model.print_topics()\n",
    "#         return self.print_topics\n",
    "# print(\"loaded cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model = TopicModeller()\n",
    "# topic_model.weigh_tokens(test_prep)\n",
    "# print(topic_model.model_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bert_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ff002b4e4a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#import mxnet as mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_embedding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ctx = mx.gpu(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bert_embedding'"
     ]
    }
   ],
   "source": [
    "# # first get sentences which are nearest neighbors to the identified topics\n",
    "# # https://scikit-learn.org/stable/modules/neighbors.html\n",
    "# # https://stackoverflow.com/questions/60996584/bert-embedding-for-semantic-similarity\n",
    "# # https://stackoverflow.com/questions/59865719/how-to-find-the-closest-word-to-a-vector-using-bert\n",
    "# # https://gist.github.com/avidale/c6b19687d333655da483421880441950\n",
    "\n",
    "\n",
    "# # then compare sentence results from pure extractive summariser maybe?\n",
    "\n",
    "# from sklearn.neighbors import KDTree\n",
    "# #import mxnet as mx\n",
    "# from bert_embedding import BertEmbedding\n",
    "\n",
    "# # ctx = mx.gpu(0)\n",
    "# # bert = BertEmbedding(ctx=ctx)\n",
    "# bert_emb = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Trying code from here\n",
    "# https://gist.github.com/avidale/c6b19687d333655da483421880441950\n",
    "\n",
    "# Preprocess embeddings in a formatted way as such can track sentences, words, embeddings\n",
    "\n",
    "# do this, then pass the LDA topics into the query\n",
    "# \"\"\" \n",
    "\n",
    "# class EmbeddingHandler:\n",
    "#     def __init__(self, sentences, model):\n",
    "#         self.sentences = sentences\n",
    "#         self.model = model\n",
    "        \n",
    "#     def generate_embeddings(self):\n",
    "#         result = self.model(self.sentences)\n",
    "# #         result = list()\n",
    "# #         for sent in self.sentences:\n",
    "# #             result.append(self.model.encode(sent, is_pretokenized=True))\n",
    "# #         #result = self.model.encode(self.sentences, is_pretokenized=True, show_progress_bar=True)\n",
    "#         #print(result)\n",
    "        \n",
    "#         self.sent_ids = []\n",
    "#         self.token_ids = []\n",
    "#         self.tokens = []\n",
    "#         embeddings = []\n",
    "#         for i, (toks, embs) in enumerate(tqdm(result)):\n",
    "#             for j, (tok, emb) in enumerate(zip(toks, embs)):\n",
    "#                 self.sent_ids.append(i)\n",
    "#                 self.token_ids.append(j)\n",
    "#                 self.tokens.append(tok)\n",
    "#                 embeddings.append(emb)\n",
    "#         embeddings = np.stack(embeddings)\n",
    "#         # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
    "#         self.normed_embeddings = (embeddings.T / (embeddings**2).sum(axis=1) ** 0.5).T\n",
    "        \n",
    "#     def generate_sent_embeddings(self):\n",
    "#         \"\"\"test sent vs word embeddings\"\"\"\n",
    "#         # use sentence-transformers embeddings\n",
    "#         result = self.model.encode(self.sentences)\n",
    "#         self.sent_ids = []\n",
    "#         self.tokens = []\n",
    "#         embeddings = []\n",
    "#         for i, (tok, emb) in enumerate(tqdm(zip(self.sentences,result))):\n",
    "#             self.sent_ids.append(i)\n",
    "#             self.tokens.append(tok)\n",
    "#             embeddings.append(emb)\n",
    "#         embeddings = np.stack(embeddings)\n",
    "#         # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
    "#         self.normed_embeddings = (embeddings.T / (embeddings**2).sum(axis=1) ** 0.5).T\n",
    "        \n",
    "#     def create_comparitor(self):\n",
    "#         # this takes some time\n",
    "#         self.indexer = KDTree(self.normed_embeddings)\n",
    "#         print(\"created KDTree\")\n",
    "    \n",
    "#     def query(self, query_sent, query_word, k=10, filter_same_word=False):\n",
    "#         toks, embs = self.model([query_sent])[0]\n",
    "\n",
    "#         found = False\n",
    "#         for tok, emb in zip(toks, embs):\n",
    "#             if tok == query_word:\n",
    "#                 found = True\n",
    "#                 break\n",
    "#         if not found:\n",
    "#             raise ValueError('The query word {} is not a single token in sentence {}'.format(query_word, toks))\n",
    "#         emb = emb / sum(emb**2)**0.5\n",
    "\n",
    "#         if filter_same_word:\n",
    "#             initial_k = max(k, 100)\n",
    "#         else:\n",
    "#             initial_k = k\n",
    "#         di, idx = self.indexer.query(emb.reshape(1, -1), k=initial_k)  # this is returning our neighbours\n",
    "#         distances = []\n",
    "#         neighbors = []\n",
    "#         contexts = []\n",
    "#         # this is filtering for word matching\n",
    "#         for i, index in enumerate(idx.ravel()):\n",
    "#             token = self.tokens[index]\n",
    "#             if filter_same_word and (query_word in token or token in query_word):  # take this away\n",
    "#                 continue\n",
    "#             distances.append(di.ravel()[i])\n",
    "#             neighbors.append(token)\n",
    "#             contexts.append(self.sentences[self.sent_ids[index]])\n",
    "#             if len(distances) == k:\n",
    "#                 break\n",
    "#         return distances, neighbors, contexts\n",
    "    \n",
    "#     def topic_neighbors(self, topic_word, k=10):\n",
    "#         # get average embedding of topic word\n",
    "#         # maybe instead return context sentence that is closest to averaged embedding?\n",
    "#         # that way can use context to get right meaning\n",
    "#         topic_emb = self.avg_embedding(self.retrieve_embeddings(topic_word))\n",
    "        \n",
    "#         # get neighbors\n",
    "#         # do I need reshape?\n",
    "#         di, idx = self.indexer.query(topic_emb.reshape(1,-1), k=k)\n",
    "#         distances = []\n",
    "#         neighbors = []\n",
    "#         contexts = []\n",
    "#         for i, index in enumerate(idx.ravel()):\n",
    "#             token = self.tokens[index]\n",
    "#             distances.append(di.ravel()[i])\n",
    "#             neighbors.append(token)\n",
    "#             contexts.append(self.sentences[self.sent_ids[index]])\n",
    "#         return distances, neighbors, contexts\n",
    "        \n",
    "        \n",
    "#     def retrieve_embeddings(self, token):\n",
    "#         idxs = []\n",
    "#         for i, t in enumerate(self.tokens):\n",
    "#             if t == token:\n",
    "#                 idxs.append(i)\n",
    "#             elif token in t:  # sent-embeddings temp workaround\n",
    "#                 idxs.append(i)\n",
    "#         embs = []\n",
    "#         for i in idxs:\n",
    "#             embs.append(self.normed_embeddings[i])\n",
    "#         return embs\n",
    "    \n",
    "#     def avg_embedding(self, emb_list):\n",
    "#         return np.mean(emb_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_handler = EmbeddingHandler(emb_handler_corp, bert_emb)  # [0] index taking first doco\n",
    "# emb_handler.generate_embeddings()\n",
    "# emb_handler.create_comparitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded summarisation model\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "#from summarizer.coreference_handler import CoreferenceHandler\n",
    "#co_handler = CoreferenceHandler(greedyness=0.4)\n",
    "#sum_model = Summarizer(sentence_handler=co_handler)\n",
    "sum_model = Summarizer()\n",
    "print(\"loaded summarisation model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for processing massive strings\n",
    "#!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ValueError: [E088] Text of length 3496277 exceeds maximum of 1000000. \n",
    "The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. \n",
    "This means long texts may cause memory allocation errors. \n",
    "If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. \n",
    "The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n",
    "\"\"\"\n",
    "\n",
    "class SummarizationHandler:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def summarize(self, texts, max_len=500000, smallest_ratio=0.1):\n",
    "        def sum_texts(texts_list, ratio=None):\n",
    "            sums = []\n",
    "            print(\"Summarising text in \" + str(len(texts_list)) + \" pieces.\")\n",
    "            for text in tqdm(texts_list):\n",
    "                if ratio is None:\n",
    "                    sums.append(self.model(text))\n",
    "                else:\n",
    "                    sums.append(self.model(text, ratio=ratio))\n",
    "            return \". \".join(sums)\n",
    "        \n",
    "        def half_list(a_list):\n",
    "            half = len(a_list)//2\n",
    "            return a_list[:half], a_list[half:]\n",
    "        \n",
    "        total_len = self.total_length(texts)\n",
    "        split_texts, size_split = self.split_texts(texts, total_len, max_len)\n",
    "        split_ratio = size_split / total_len\n",
    "        \n",
    "        new_splits = [split_texts]\n",
    "        new_ratios = [split_ratio]\n",
    "        while split_ratio < smallest_ratio:\n",
    "            new_ratios = []\n",
    "            n_s = []\n",
    "            for split in new_splits:  # loop for each list:\n",
    "                t1, t2 = half_list(split)\n",
    "                n_s.append(t1)\n",
    "                n_s.append(t2)\n",
    "            new_splits = n_s\n",
    "            for s in new_splits:\n",
    "                new_ratios.append(size_split/self.total_length(s))\n",
    "            split_ratio = min(new_ratios)\n",
    "        \n",
    "        split_sums = []\n",
    "        if new_ratios[0] <= 1:\n",
    "            for i in range(len(new_splits)):\n",
    "                split_sum = sum_texts(new_splits[i], ratio=new_ratios[i])\n",
    "                split_sums.append(split_sum)\n",
    "            s = \". \".join(split_sums)\n",
    "            s = sum_texts([s])\n",
    "            return s\n",
    "        else:\n",
    "            return sum_texts(new_splits[0])\n",
    "        \n",
    "#         if split_ratio >= 1:\n",
    "#             cur_sum = sum_texts(split_texts)\n",
    "#         else:\n",
    "#             sums = sum_texts(split_texts, split_ratio)\n",
    "#             cur_sum = \". \".join(sums)\n",
    "#             cur_sum = sum_texts([cur_sum])\n",
    "#         return cur_sum\n",
    "        \n",
    "    def split_texts(self, texts, total_len, max_len):\n",
    "        \"\"\"Split texts into list of strings under max_len\"\"\"\n",
    "        num_split, size_split = self.optimal_split(total_len, max_len)\n",
    "        splits = []\n",
    "        cur_split = \"\"\n",
    "        i = 1\n",
    "        for text in texts:\n",
    "            if i == num_split:\n",
    "                # just add to last no check\n",
    "                cur_split += text\n",
    "            else:\n",
    "                if (len(cur_split) + len(text) > size_split):\n",
    "                    splits.append(cur_split)\n",
    "                    cur_split = text\n",
    "                    i += 1\n",
    "                else:\n",
    "                    cur_split += text\n",
    "        splits.append(cur_split)\n",
    "        return splits, size_split\n",
    "            \n",
    "    def optimal_split(self, total_len, max_len):\n",
    "        \"\"\"Find even split of text under max_len\"\"\"\n",
    "        under_len = int(max_len * 0.95)  # use slightly under max_len for safety\n",
    "        cur_div = 1\n",
    "        cur_size = total_len / cur_div\n",
    "        while (cur_size > under_len):\n",
    "            cur_div += 1\n",
    "            cur_size = total_len / cur_div\n",
    "        return cur_div, cur_size\n",
    "        \n",
    "    \n",
    "    def total_length(self,texts):\n",
    "        total = 0\n",
    "        for t in texts:\n",
    "            total += len(t)\n",
    "        return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (624) found smaller than n_clusters (662). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "100%|██████████| 1/1 [02:31<00:00, 151.09s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:42<00:00, 42.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US says it's steadfast in rebuilding Afghanistan - El Paso Times Mobile e-Edition Today’s Print Ads Newsletters Customer Service Subscribe This Site Web Search powered by YAHOO ! We've got to create a situation in which al-Qaida is not coming back.\" The Pentagon on Sunday identified Air Force Lt. It's an extraordinary admission of failure for us to establish the relationships that you'd have to have for a successful transition to the Afghan military and Afghan security leadership,\" Romney said. So they are very much in this fight trying to protect us.\" This material may not be published, broadcast, rewritten or redistributed. Investigators said they would check the control room recordings. Driver, Marcos Cordoba is being investigated on suspicion of what Argentine law calls \"guilty damage without an attempt to cause harm\". Blood tests showed Cordoba had not been drinking or using drugs when the accident happened. Got ’ Read more... 14 Apr 2012 In Praise of a Good Read Read more... 14 Apr 2012 Add your comment Please leave your comment below. Your name will appear next to your comment. We'll also keep you updated by email whenever someone else comments on this page. Please enter some keywords that you'd like to search for. Read more... 12 Apr 2012 July 14: Royal Father Prays for Oshiomhole ’s Success Read more... 12 Apr 2012 Trending on ThisDayLive Featured Jonathan Okonjo -Iweala Appointments Godfather World Bank MAJEKODUNMI Thisday Tweets ThisDay Poll Should Government Negotiate with Bko Haram ? The incident has swiftly spiraled out of control leaving dozens of people dead, including four U.S. troops killed by their Afghan counterparts, in a sign of the tenuous nature of the relationship between Afghanistan and the U.S.\n",
      "Afghan authorities have launched a manhunt across the country for a driver they suspect in the killing of two U.S. military advisers who were shot to death at an Afghan ministry a day earlier. International advisers working at Afghan ministries were recalled out of fears of another attack. He declined to comment on whether there were any wounded. The family was notified three days later, apparently after forensics tests at Dover Air Force Base in Delaware confirmed his identity. All I heard was a woman screaming on the phone, he said, recalling the phone call from Altaeis wife seconds after his capture from a busy Baghdad street in daylight.  Qanbar , who broke into tears while speaking by phone from Beirut , acknowledged that his nephew had made a huge mistake by sneaking off base, but he said there was no reasoning with him . He left Iraq when he was 12 and came back to help.  Qanbar said Altaei fell in love with the first girl he met upon arriving in Iraq . He was abducted soon after the couple returned to Iraq , Qanbar said, when he was en route to bring lamb to his wifes home for a religious feast. Afghanistan 's president renewed his calls for calm Sunday in a televised address to the nation after the burning of Qurans at a U.S. base sparked five days of deadly protests and prompted the international military coalition to pull its advisers from Afghan ministries out of fear that they would become the next targets. ( More than 30 people have been killed, including four U.S. troops, in six days of unrest. Still, the top U.S. diplomat in Afghanistan said the violence would not change Washington 's course . \" This is not the time to decide that we're done here,\" he said. The advisers are key to helping improve governance and preparing the country's security forces to take on more responsibility. A manhunt was under way for the main suspect in the shooting  an Afghan man who worked as a driver for an office on the same floor as the advisers who were killed, Interior Ministry spokesman Sediq Sediqi said. He did not provide further details about the suspect or his possible motive. Afghanistan 's defence and interior ministers were to visit Washington this week, but they called off the trip to consult with other Afghan officials and religious leaders on how to stop the violence, Pentagon press secretary George Little said. The protesters in Kunduz province in the north threw hand grenades to express their anger at the way some Qurans and other Islamic texts were disposed of in a burn pit last week at Bagram Air Field , north of Kabul . Armed individuals in the crowd fired on police and threw grenades at the U.S. base on the city outskirts, said Amanuddin Quriashi , administrator in Imam Sahib. Seven NATO troops were wounded by the grenade. One protester was killed by troops firing from the U.S. base, and another was killed by Afghan police, Quriashi said. A NATO spokesman said an explosion occurred outside the base, but that the grenades did not breach its defences. Karzai did not mention the killings at the ministry in his opening remarks but when a reporter asked, he said he was \"saddened\" by their deaths. Members of the international military coalition described the removal of advisers as a temporary security measure, stressing that they did not expect it to affect partnerships with the Afghans . \" We continue to move forward and stand by our Afghan partners in this campaign. Jimmie Cummings , a spokesman for the international force. Germany has withdrawn troops early from an outpost in northern Afghanistan because of the Quran protests. But the regional commander decided to pull the remaining 50 German troops back to a large base in Kunduz because of nearby demonstrations. ___ Associated Press Writer Heidi Vogt in Kabul contributed to this report. Tip someone you know about this article: To: From : Last changed: February 26. The contents of this website are protected by copyright and may be used only for your personal non-commercial purposes. All other rights are reserved and commercial use is prohibited. Seven protesters were killed in clashes on Wednesday. continued on next page » Filed under: afghanistan | president barack obama | quran | COMMENTING RULES: To encourage an open exchange of ideas in The Hour community, we are pleased to allow readers to post comments following stories appearing on this website. By posting a comment, you are agreeing to our Terms of Website Use . You must be registered and logged in to comment on stories . Norwalk Mosque hearing postponed until April 18 4. Norwalk man robbed at King Kennedy housing complex 3. Councilman 's rap video has Moccia fuming 4. Westport Avenue gas station robbed at gunpoint 5. \" Some protesters hoisted the white Taliban flag. The Koran burnings could make it far harder for NATO forces to win the trust of the Afghan public as they try to stabilise the country ahead of the withdrawal of foreign combat troops at the end of 2014. Afghan President Hamid Karzai repeated his plea for calm and restraint. \" Posted by Marisol on February 26, 2012 8:18 AM | 37 Comments del. They have only been trying for a decade...I wonder how much longer till these fools realize they are shoveling stuff against the tide and the stuff keeps attacking them? Maybe it's time to get the stuff out of there... Maxpublius | February 26, 2012 8:48 AM | Reply Blame the Saudi -handholder Bush for not recognizing evil and airing the most perfidious, destructive meme of the 21st century, \"Islam is a religion of peace\". There's nothing good in it which can't be found elsewhere and it has rot in it which is hard to find anywhere else. I have seen Muslim burn American flag â ¦ manny times â ¦. a flags that represent the pride of a nation BUT, it is still a piece of cloth â ¦ if they burn a flag of American they are not burning America If the tropes weather by mistake or not, burned the Quran, they did not burn Muslims ... .no one gets outraged and kill Muslims and cause harm for doing that â¦ I am so sorry for the loss of lives for burning of a book that worth 10 dollars â  ¦ little more or little less â ¦ but a printed 10 dollars book â ¦ I am ashamed I was ever part of this madness called Islam . And as for the effort at \"winning hearts and minds\"? Alert | February 26, 2012 11:31 AM | Reply Maxpublius makes an excellent point! Americans elected a rich, spoilt, AWOL brat who was raised on Saudi wealth. com/watch?v =PuAg - tOsrWU ) thereby, covering-up Saudi Arabia from 9/11 terrorist attacks. freerepublic.com/focus/f-news/1999287/posts ). gail duituturaga replied to comment from fineliving56 | February 26, 2012 11:41 AM | Reply You know that at the book makers they do not wear white gloves when putting this koran book together nor do the sellers of the book by this title. Could be that dogs pissed on the tree or rats ran amoke in the branches. I would watch as long as it is factual and brave. Papa Whiskey | February 26, 2012 11:42 AM | Reply There may actually be a silver lining to this bloodstained cloud: the radicalization of an awful lot of well-trained military personnel who are daily being wised up to the spuriousness of the sheepsh*t spewed by their lickspittle commanders and the odious administration that directs them. They'll remember in November -- and thereafter. The leadership of victims everywhere, display irresponsibility to criminal extent, in the face of reality. There existed much good reason for the mistake of currency of intel, which is going on in this day, and will continue. com/watch?v= qpsUdgTWITI to see Saddam 's top man, speak of WMD 's that he had, and sent to Syria , BTW. In several interviews of him , I note he holds back on intel of other OIC nations weaponry. It is clear that Newt and Rick at least understand much of the criminal gang bangers, no matter any other flaws. No one is living without flaws, so I still say, PICK YOUR POISON , AND PICK IT WELL! Whats Up Doc | February 26, 2012 1:22 PM | Reply I would take bets that this killer will never face justice and US will not insist upon it either they don't want to get Muslims more upset. Instead, it will either cause tyranny or chaos. madrussian | February 26, 2012 1:34 PM | Reply When oh when is the U.S. going to wake up to the fact that when you apologize to uncivilized people, they will still kill you! BlueRaven | February 26, 2012 1:35 PM | Reply Islam - the most irrational System known to Mankind. Exactly right, Wellington ; yes, where are they? Makes one wonder if moderates even exist at all ! More : Despite an apology from U.S.\n",
      "President Barack Obama , riots raged across the country for a sixth day on Sunday against the desecration of the Muslim holy book at a NATO air base at Bagram . Some protesters hoisted the white Taliban flag. ................................... And there's another \"Afghan soldier\" who all at once exhibits \"Sudden Jihad Syndrome\". More: The Koran burnings could make it far harder for NATO forces to win the trust of the Afghan public as they try to stabilise the country ahead of the withdrawal of foreign combat troops at the end of 2014. ................................... Amazing how many people still get this wrong, be they the Bush haters or the ones defending Bush too much. I'm betting on the latter but not by a lot. Kill anything in your path, bomb as much infrastructure as you can and get the hell out of that crap hole once and for all and come home. Gorkhali | February 27, 2012 12:25 AM | Reply When will they realize that you can never win the hearts of a people who believe in your annihilation. Charles Griffith replied to comment from ImNoDhimmi | February 27, 2012 5:58 AM | Reply Thank you for reminding me....I couldn't agree more. Get the hell out of that camel country and let them destroy themselves. This was something that happened as a mistake. Killing Americans in uniform is not a mistake … [You can] say it’s unfortunate … I think it [the apology] shows weakness.” But … if you look at my record in Massachusetts , … I’ m a solid conservative, a committed conservative with the kind of principles I think America needs. ” Romney ’s references to wealth put him in Dan Quayle territory ” – WashPost A2 , Melinda Henneberger column, “ She the People: The world as women see it” : “Barack Obama never tried to come off as just your average dude — he couldn’t have, but the change sure was nice — and Romney needn ’t, either. The President and his wife come; they do a nice little photo thing. You walk in and the tables are all out of the place. Mr.\n",
      "Brock estimated, conservatively, that The Times receives 14,000 reports of errors in a year. A] trend emerged a couple of years ago: too many errors in obituaries. http://nyti.ms/AyHVTr --PLAYBOOK FACTS OF LIFE : When we talk to journalism students or our young reporters, we point out that people trust The New York Times BECAUSE of the corrections, not DESPITE the corrections. http://nyti.ms/AuqDue IF YOU READ ONLY ONE THING: Ann Romney , campaigning alongside her husband at an Americans for Prosperity Forum in Troy , Mich. , told the crowd that she 's decided her husband is done debating, per Reid Epstein and Maggie Haberman : “Maybe I should just do all the talking and let him just stand here and watch me … I 've also decided no more debates. While the unemployment rate for construction workers rises, Congress can take immediate action to create jobs. # BIRTHWEEK : Abram Olmstead , of U.S.\n",
      "Chamber THE PRESIDENT’S WEEK AHEAD : “ On Monday, the President and the Vice President will host a meeting with the National Governors Association in the State Dining Room. These jobs are needed now to help America get back to work! Fact: Greece never had the burden of defense of the free world that we have. In 1988, our military spending as a percent of GDP was about 6%. That suggests that the military will be less of a burden in the future. This is not his fault, but it does make him ineligible for POTUS. Inspiring others to do what they otherwise would not. I'm the case of Barack Obama , how hard it is to inspire folks to take more from those who have it (...especially when you have majorities in both teh house and senate . Now, give me an example of where taking more from others has worked. Maybe it's working out well for you....but it's not for the rest of us. Reply Quote Report Abuse kweitkamp Party: NA Reply #4 Feb. 26, 2012 - 1:40 PM EST So, what's the answer to Gov.\n",
      "O'Malley 's question? Reply Quote Report Abuse skint Party: Independent Reply #5 Feb. 26, 2012 - 11:21 PM EST the biggest misconception would be the media being \"FAIR\" All the 20 and 30 someting so-called \"JournaLIST are the product of the radical 60 professors. Now is not the time for vengeance ,\" he said. \" We'll come through this together as a unit.\" Gen.\n",
      "Sher Mohammad Karimi , the Afghan National Army 's chief of staff, joined Allen on the trip and thanked the troops for their \"sacrifices for humanity, not just the Afghan people.\" NATO troops incinerated Islamic religious material at Bagram Airfield earlier this week. Copyright CNN 2012 The following are comments from our users. To report an offensive or otherwise inappropriate comment, click the \"Flag\" link that appears beneath that comment. Recognize Alzheimers Improve Your Health Deal With Pay Cuts Scripps TV Station Group © 2012 The E.W.\n",
      "Scripps Co. Click here for the privacy policy , terms of use . In the western city of Herat , a group among more than a 1,000 demonstrators broke off and tried to march towards the US consulate but were prevented by security forces, an AFP reporter said. Recommend Ameer Feb 24, 2012 - 10:38PM Reply I wish to know what these people get out of that kind of insult on our holy book My be they do not undersatnd how this book is important to us. Because any one’s faith is a connection between him and his creator. Recommend MORE Leave Your Reply Below Click here to cancel reply. Notify me of followup comments via e-mail Comments are moderated and generally will be posted if they are on-topic and not abusive. But it also brought back a painfull memory from the days bygone; 21st July,1969 ... Run for your life Maestro96 Great article. This country is riveting with sectarian and ethnic divisions. Run for your life Maestro96 @ Bluejay : It was sarcasm .\n",
      "complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sum_handler = SummarizationHandler(sum_model)\n",
    "\n",
    "test_topic = corpus.loc[corpus['topic_id'] == 1]['text'][0:10]\n",
    "test_topic_sum = sum_handler.summarize(test_topic)\n",
    "\n",
    "print(test_topic_sum)\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading 'Nugget' Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nugget_dir = \"/nfs/TemporalSummarization/ts13/results\"\n",
    "updates_sampled_path = nugget_dir + \"/updates_sampled.tsv\"\n",
    "nuggets_path = nugget_dir + \"/nuggets.tsv\"\n",
    "nug_matches_path = nugget_dir + \"/matches.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_duplicates(df):\n",
    "    seen = set()\n",
    "    seen_twice = set()\n",
    "    for docid in df['docid']:\n",
    "        if docid not in seen:\n",
    "            seen.add(docid)\n",
    "        else:\n",
    "            seen_twice.add(docid)\n",
    "    return seen_twice\n",
    "\n",
    "def create_update_df():\n",
    "    \"\"\"Data Frame containing information about docs which have updates/multiple instances in corpus\"\"\"\n",
    "    def create_entry(row, col_tags):\n",
    "        entry = {}\n",
    "        for col in col_tags:\n",
    "            entry[col] = row[col]\n",
    "        return entry\n",
    "    \n",
    "    col_tags = ['docid', 'streamid', 'epoch', 'yyyymmddhh', 'zulu']\n",
    "    entry_list = []\n",
    "    dups = find_duplicates(corpus)\n",
    "    for docid in tqdm(dups, position=0, leave=True):\n",
    "        d = corpus[corpus['docid'] == docid]\n",
    "        for index, row in d.iterrows():\n",
    "            entry = create_entry(row, col_tags)\n",
    "            entry_list.append(entry)\n",
    "             \n",
    "    update_df = pd.DataFrame(entry_list)\n",
    "    update_df = update_df.set_index(col_tags)\n",
    "    return update_df\n",
    "                    \n",
    "                \n",
    "def create_nugget_df():\n",
    "    \"\"\"Dataframe containing nugget data and its appearances in corpus\"\"\"\n",
    "    def create_entry(row, reg_cols, multi_col_vals=None):\n",
    "        entry_dict = {}\n",
    "        for col in reg_cols:\n",
    "            entry_dict[col] = row[col]\n",
    "        if multi_cols is not None:\n",
    "            for k,v in multi_col_vals.items():\n",
    "                entry_dict[k] = v\n",
    "        return entry_dict\n",
    "        \n",
    "    nuggets_tsv = pd.read_csv(nuggets_path, \"\\t\")\n",
    "    entry_list = []\n",
    "    reg_cols = ['query_id', 'nugget_id', 'importance', 'nugget_len', 'nugget_text']\n",
    "    multi_cols = ['docid', 'streamid', 'epoch', 'yyyymmddhh']  # multiindex cols\n",
    "    num_cols = ['query_id', 'importance', 'nugget_len', 'epoch']\n",
    "    \n",
    "    pbar = tqdm(total=len(nuggets_tsv), position=0, leave=True)\n",
    "    for index, row in nuggets_tsv.iterrows():\n",
    "        # find where nugget appears in text\n",
    "        nug_text = row['nugget_text']\n",
    "        topic_id = 0\n",
    "        try:\n",
    "            topic_id = int(row['query_id'])  # make sure pattern match in correct topic\n",
    "        except ValueError:\n",
    "            pbar.update()\n",
    "            continue  # topic_id is unknown string in tsv file, e.g. \"TS13.07\"\n",
    "        appears = corpus[corpus['topic_id'] == topic_id]\n",
    "        appears = appears[appears['text'].str.contains(re.escape(nug_text))]  # make sure no accidental regex pattern\n",
    "        \n",
    "        # gather information on docs it appears in\n",
    "        dups = find_duplicates(appears)  # get docids where nugget appears\n",
    "        for docid in dups:\n",
    "            upd = appears[appears['docid'] == docid]  # get docs with this docid\n",
    "            for i, r in upd.iterrows():  # gather info on each doc with this docid (e.g. streamid, epoch etc.)\n",
    "                multi_col_vals = {}\n",
    "                for multi_col in multi_cols:\n",
    "                    multi_col_vals[multi_col] = r[multi_col]\n",
    "                entry = create_entry(row, reg_cols, multi_col_vals=multi_col_vals)\n",
    "                entry_list.append(entry)\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "    \n",
    "    # form multi-index nugget dataframe\n",
    "    reg_cols.extend(multi_cols)  # get new multiindex order\n",
    "    nugget_df = pd.DataFrame(entry_list)\n",
    "    nugget_df[num_cols] = nugget_df[num_cols].apply(pd.to_numeric, errors='coerce', axis=1)  # convert appropriate cols to numerical values\n",
    "    nugget_df.rename(columns={'query_id':'topic_id'}, inplace=True)  # topic_id matches other dataframes\n",
    "    return nugget_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1366/1366 [00:41<00:00, 33.29it/s] \n"
     ]
    }
   ],
   "source": [
    "nugget_df = create_nugget_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>nugget_id</th>\n",
       "      <th>importance</th>\n",
       "      <th>nugget_len</th>\n",
       "      <th>nugget_text</th>\n",
       "      <th>docid</th>\n",
       "      <th>streamid</th>\n",
       "      <th>epoch</th>\n",
       "      <th>yyyymmddhh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.052</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Hundreds injured</td>\n",
       "      <td>dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420</td>\n",
       "      <td>2012-02-23-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.052</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Hundreds injured</td>\n",
       "      <td>dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1</td>\n",
       "      <td>1330041420</td>\n",
       "      <td>2012-02-23-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.054</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>February 22, 2012</td>\n",
       "      <td>f66f6668504592a391345e012800469c</td>\n",
       "      <td>1329944400-f66f6668504592a391345e012800469c</td>\n",
       "      <td>1329944400</td>\n",
       "      <td>2012-02-22-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.054</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>February 22, 2012</td>\n",
       "      <td>f66f6668504592a391345e012800469c</td>\n",
       "      <td>1329944400-f66f6668504592a391345e012800469c</td>\n",
       "      <td>1329944400</td>\n",
       "      <td>2012-02-22-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>VMTS13.01.054</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>February 22, 2012</td>\n",
       "      <td>ecda22bcfc10da137b49f0089bd5d7f5</td>\n",
       "      <td>1329916140-ecda22bcfc10da137b49f0089bd5d7f5</td>\n",
       "      <td>1329916140</td>\n",
       "      <td>2012-02-22-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id      nugget_id  importance  nugget_len        nugget_text  \\\n",
       "0         1  VMTS13.01.052           3           2   Hundreds injured   \n",
       "1         1  VMTS13.01.052           3           2   Hundreds injured   \n",
       "2         1  VMTS13.01.054           1           3  February 22, 2012   \n",
       "3         1  VMTS13.01.054           1           3  February 22, 2012   \n",
       "4         1  VMTS13.01.054           1           3  February 22, 2012   \n",
       "\n",
       "                              docid  \\\n",
       "0  dd95d5dbbff443c3ddae4e34a5d2e9c1   \n",
       "1  dd95d5dbbff443c3ddae4e34a5d2e9c1   \n",
       "2  f66f6668504592a391345e012800469c   \n",
       "3  f66f6668504592a391345e012800469c   \n",
       "4  ecda22bcfc10da137b49f0089bd5d7f5   \n",
       "\n",
       "                                      streamid       epoch     yyyymmddhh  \n",
       "0  1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1  1330041420  2012-02-23-23  \n",
       "1  1330041420-dd95d5dbbff443c3ddae4e34a5d2e9c1  1330041420  2012-02-23-23  \n",
       "2  1329944400-f66f6668504592a391345e012800469c  1329944400  2012-02-22-21  \n",
       "3  1329944400-f66f6668504592a391345e012800469c  1329944400  2012-02-22-21  \n",
       "4  1329916140-ecda22bcfc10da137b49f0089bd5d7f5  1329916140  2012-02-22-13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(nugget_df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1245/1245 [00:02<00:00, 594.41it/s]\n"
     ]
    }
   ],
   "source": [
    "update_df = create_update_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th>streamid</th>\n",
       "      <th>epoch</th>\n",
       "      <th>yyyymmddhh</th>\n",
       "      <th>zulu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">29874677aff307363157298b13c8becf</th>\n",
       "      <th>1354014393-29874677aff307363157298b13c8becf</th>\n",
       "      <th>1354014393</th>\n",
       "      <th>2012-11-27-11</th>\n",
       "      <th>2012-11-27T11:06:33.0Z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1354014284-29874677aff307363157298b13c8becf</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1354014284</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2012-11-27-11</th>\n",
       "      <th>2012-11-27T11:04:44.0Z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-11-27T11:04:44.0Z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354014292-29874677aff307363157298b13c8becf</th>\n",
       "      <th>1354014292</th>\n",
       "      <th>2012-11-27-11</th>\n",
       "      <th>2012-11-27T11:04:52.0Z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354014260-29874677aff307363157298b13c8becf</th>\n",
       "      <th>1354014260</th>\n",
       "      <th>2012-11-27-11</th>\n",
       "      <th>2012-11-27T11:04:20.0Z</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(29874677aff307363157298b13c8becf, 1354014393-29874677aff307363157298b13c8becf, 1354014393, 2012-11-27-11, 2012-11-27T11:06:33.0Z), (29874677aff307363157298b13c8becf, 1354014284-29874677aff307363157298b13c8becf, 1354014284, 2012-11-27-11, 2012-11-27T11:04:44.0Z), (29874677aff307363157298b13c8becf, 1354014284-29874677aff307363157298b13c8becf, 1354014284, 2012-11-27-11, 2012-11-27T11:04:44.0Z), (29874677aff307363157298b13c8becf, 1354014292-29874677aff307363157298b13c8becf, 1354014292, 2012-11-27-11, 2012-11-27T11:04:52.0Z), (29874677aff307363157298b13c8becf, 1354014260-29874677aff307363157298b13c8becf, 1354014260, 2012-11-27-11, 2012-11-27T11:04:20.0Z)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(update_df[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (137) found smaller than n_clusters (269). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "100%|██████████| 1/1 [00:39<00:00, 39.70s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 'Cuse, reaches Big East final Woman missing for a month found with her cat 5 Irish killed in 1832 re-buried in Pa. cemetery Teen smoking an 'epidemic,' new report finds Pakistan charges Osama bin Laden 's widows Anti- Obama Facebook post tests military rules Anna Gristina: Suburban mom or high-rolling madam? The anger over the Quran burning has sparked two days of protests across Afghanistan and tapped into anti-foreign sentiment fueled by a popular perception that foreign troops disrespect Afghan culture and Islam . The Afghan Interior Ministry said in a statement that clashes during a protest in the eastern province of Parwan left four people dead. It said an investigation was under way to determine what happened. U.S.\n",
      "Gen.\n",
      "John Allen , the top commander of American and NATO forces in Afghanistan , said after the books had been mistakenly given to troops to be burned at a garbage pit without realizing it. \" The moment we found out about it we immediately stopped and we intervened.\" In the crowd there were rebels and Taliban who had weapons. Police returned fire, killing one protester, said provincial police chief Gen.\n",
      "Ghulam Sakhi Roogh Lawanay . Two protesters and two police officers were also wounded, he added. He said the protesters had come from neighboring Wardak province, an insurgent hotbed. 3 Georgians killed Meanwhile, three Georgian soldiers were killed in the southern Afghan province of Helmand on Wednesday, the Georgian defense ministry said, taking the country's death toll to 15. \" The sacrifice of Georgian servicemen is appreciated by the Georgian people... future generations will live in a united, much stronger and more successful country,\" Saakashvili said in a statement. Another deployment of 600-700 will be sent this year, making Georgia one of the largest non- NATO contributors in the war. The tragedy comes after a series of train accidents in Argentina and will likely bring about a prolonged legal battle. Argentina 's government said they would join plaintiffs against Trenes de Buenos Aires ( TBA ), the company that holds the concession on the Sarmiento line where the accident occurred. Thomson Reuters journalists are subject to an Editorial Handbook which requires fair presentation and disclosure of relevant interests. NYSE and AMEX quotes delayed by at least 20 minutes. For a complete list of exchanges and delays, please click here . By CHARLES NEWBERY and SIMON ROMERO Published: February 22, 2012 Sign In to E-Mail Print Reprints PINAMAR, Argentina — A commuter train crashed at a busy central station in Buenos Aires on Wednesday, killing 49 passengers and injuring more than 600 people, the deadliest in a series of train accidents in Argentina over the past year. He said one car pierced into another by nearly 20 feet. Passengers told the local news media that the train, which is operated by the private company Trenes de Buenos Aires , was traveling faster than normal and had struggled to slow down when braking at stations ahead of Once Station . Trenes de Buenos Aires said in a statement that the reasons for the crash had not been determined, though the company acknowledged that the train “wasn ’t able to stop.” The bus had crossed the tracks when the barrier was down. In February 2011, four people were killed in a collision of two trains. Get 50% Off The New York Times &amp ; Free All Digital Access. Get Free E -mail Alerts on These Topics Railroad Accidents and Safety Argentina Buenos Aires ( Argentina ) Getting fat but staying fit? With Your Friends Explore news, videos and much more based on what your friends are reading and watching. Publish your own activity and retain full control. YOUR FRIENDS' ACTIVITY prev next YOU ON YAHOO! SPORTS Your Activity |Social : OFFON Turn Social ONRemind me when I share |Options What is this?Not you? Barton spilled the image via his Twitter feed, presumably after attending some sort of Umbro preview (We're not ruling out a break-and-enter at this point). And now Umbro have posted more pictures on their blog: The new England shirt ( blog.umbro.com ) The new red and white design has already courted plenty of controversy — the FA 's latest contract with Umbro stipulates that the national kit will be changed roughly every 18 months, rather than the previous two-year cycle. The complaints from people who object to changing their clothes more frequently than two years at a time were so strong that the FA released an explanatory statement. and the last photo caption was brilliant too! 11 Penn St 8 … A veteran of three-plus decades on the sideline, Ohio State coach Jim Foster Full Story »No. Saturday - 2 hours 3 minutes ago Video: Please enjoy Jeremy Evans , dunk contestantBall Don't Lie - 1 hour 23 minutes ago Show More Dirty Tackle on Twitter DirtyTackleDirty Tackle DirtyTackleDirty Tackle @ JCheese23 Remember Ochocinco at Sporting KC ? co/uRD0Z6Ta 1 hr ago Reply Retweet Favorite DirtyTackleDirty Tackle @ JCheese23 Yahoo 's basketball blog covered it the other day. 1 hr ago Reply Retweet Favorite More tweets » Yahoo ! Sports Bloggers Brooks Peck Brooks Peck is a Soccer blogger for Yahoo! PostsWebsiteEmailRSS Ryan Bailey Ryan Bailey is a Soccer blogger for Yahoo! New England kit revealed by Joey Barton Latest Y! Back in 2008 \"the situation was disastrous and the braking system was terrible,\" said Despouy . State authorities have \"not taken measures since then, nor applied serious sanctions\" against the company. An angry crowd shouted \"Murderers, murderers!\" Planning Minister Julio de Vido told reporters Thursday that the government would sue the company. My son must be somewhere, alive or dead!\" cried Maria Lujan , mother to 20-year-old Lucas Menghini . The death toll reached 50, with 11 still unidentified, authorities said. The Sarmiento rail line, owned by TBA , links the center of Buenos Aires to a densely populated suburb 70 kilometers (44 miles) to the west of the city. It uses rolling stock made in Japan and acquired in the 1960s. The rail network was privatized in the 1990s. The region's transit system has been plagued with serious accidents in recent years. Argentina 's deadliest train tragedy was in 1970, an accident that killed 236 people in northern Buenos Aires. Press Press kit Jobs Help Contact us Site map Terms of use / Privacy policy © 2006 - 2012 Copyright FRANCE 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# select random selection of streams with nugs to summarise\n",
    "test_nug = nugget_df[nugget_df['topic_id'] == 1].sample(n=10)['streamid']\n",
    "test_nug = corpus[corpus['streamid'].isin(test_nug)]\n",
    "test_nug_sum = sum_handler.summarize(test_nug['text'])\n",
    "print(test_nug_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricHandler:\n",
    "    def __init__(self, rouge_scores=['rouge1']):   \n",
    "#         self.summary = summary\n",
    "        self.rouge = rouge_scorer.RougeScorer(rouge_scores)\n",
    "        self.nugget_df = None\n",
    "        self.nugget_dict = None\n",
    "        self.streamids = None\n",
    "        \n",
    "    def evaluate_summary(self, summary, streamids, rouge=True, importance=True):\n",
    "        self.streamids = streamids\n",
    "        self.nugget_df = self.nugget_frame(streamids)  # store in self.nugget_frame\n",
    "        # create nested dictionary of metrics\n",
    "        metrics = {}\n",
    "        if importance:\n",
    "            sum_nugs = self.nuggets_in_summary(summary)\n",
    "            cur_imp, total_imp = self.importance_score(sum_nugs)\n",
    "            imp_dict = {}\n",
    "            imp_dict['cur_imp'] = cur_imp\n",
    "            imp_dict['total_imp'] = total_imp\n",
    "            metrics['importance'] = imp_dict\n",
    "        if rouge:\n",
    "            target_text = self.target_nugget_text()\n",
    "            rouges = self.rouge_score(target_text, summary)\n",
    "            for k,v in rouges.items():\n",
    "                r_dict = {}\n",
    "                for label, value in v._asdict().items():\n",
    "                    # keys: precision, recall, fmeasure\n",
    "                    r_dict[label] = value\n",
    "                metrics[k] = r_dict\n",
    "        return metrics, sum_nugs  # return found nuggets to pass to db\n",
    "        \n",
    "    def rouge_score(self, target, summary):\n",
    "        scores = self.rouge.score(target, summary)\n",
    "        return scores\n",
    "        \n",
    "    def target_nugget_text(self, str_divider=\" \"):\n",
    "        t_nugs = list(self.nugget_df['nugget_text'])\n",
    "        t_nugs = str_divider.join(t_nugs)\n",
    "        return t_nugs\n",
    "        \n",
    "    def importance_score(self, sum_nugs):\n",
    "        cur_score = sum_nugs['importance'].sum()  # actual summary score\n",
    "        total_score = self.nugget_df['importance'].sum() # potential score\n",
    "        return cur_score, total_score\n",
    "        \n",
    "    def nuggets_in_summary(self, summary):\n",
    "        # filter where nugget_text is in summary\n",
    "        sum_nugs = self.nugget_df[self.nugget_df.apply(lambda x: x['nugget_text'] in summary, axis=1)]\n",
    "        return sum_nugs\n",
    "        \n",
    "    def nugget_frame(self, streamids, keep_columns=None):\n",
    "        if keep_columns is None:\n",
    "            keep_columns = ['nugget_id', 'importance', 'nugget_text']\n",
    "        # get nuggets for each streamid\n",
    "        nug_rows = nugget_df[nugget_df['streamid'].isin(streamids)]\n",
    "        nug_rows = nug_rows.drop_duplicates('nugget_id')\n",
    "        nug_rows = nug_rows[keep_columns]\n",
    "        self.nugget_df = nug_rows\n",
    "        return self.nugget_df\n",
    "    \n",
    "    def update_summary(self, summary):\n",
    "        self.summary = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'importance': {'cur_imp': 1, 'total_imp': 12},\n",
       "  'rouge1': {'precision': 0.004761904761904762,\n",
       "   'recall': 0.38461538461538464,\n",
       "   'fmeasure': 0.009407337723424272}},\n",
       "        nugget_id  importance        nugget_text\n",
       " 2  VMTS13.01.054           1  February 22, 2012)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_handler = MetricHandler()\n",
    "metric_handler.evaluate_summary(test_nug_sum, test_nug['streamid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find difference in epoch for a day\n",
    "# def epoch_diff():\n",
    "#     # find instances in epoch where there is a day gap\n",
    "#     day_gap['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class SummaryFeeder:\n",
    "    \"\"\"\n",
    "    Need to add nugget metric info to database table and add from here\n",
    "    Then also add to nugget_instances table\n",
    "    \"\"\"\n",
    "    def __init__(self, sum_handler, tech_name, tech_descr=None, is_temporal=False):\n",
    "        self.sum_handler = sum_handler\n",
    "        self.tech_name = tech_name\n",
    "        self.is_temporal = is_temporal\n",
    "        self.tech_descr = tech_descr\n",
    "        self.metric = MetricHandler()\n",
    "        \n",
    "    def summarize_topics(self, corp_df):\n",
    "        # pre-determine some columns to be inputted into meta table\n",
    "        self.meta_columns = self.get_meta_columns(corp_df)\n",
    "        # pre-summary database operations\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        \n",
    "        start_exec = self.cur_datetime()\n",
    "        instance_id = self.store_instance(start_exec)\n",
    "        self.conn.commit()\n",
    "        \n",
    "        topic_ids = corp_df['topic_id'].unique()\n",
    "        for topic_id in tqdm(topic_ids, position=0, leave=True):\n",
    "            # currently sum all \n",
    "            topic_df = corp_df[corp_df['topic_id'] == topic_id]\n",
    "            summary = self.sum_handler.summarize(topic_df['text'])\n",
    "            self.store_topic_summary(topic_id, instance_id, summary)\n",
    "            self.conn.commit()\n",
    "        \n",
    "        end_exec = self.cur_datetime()\n",
    "        self.update_end_exec(end_exec, instance_id)\n",
    "        self.conn.commit()\n",
    "        self.conn.close()\n",
    "        print(\"summarize_topics complete\")\n",
    "        \n",
    "    def get_meta_columns(self, corp_df):\n",
    "        meta_fields = {}\n",
    "        for topic_id in corp_df['topic_id'].unique():\n",
    "            nest = {}\n",
    "            t = corp_df[corp_df['topic_id'] == topic_id]\n",
    "            t = t.sort_values(\"epoch\")\n",
    "            nest['epoch_start'] = t['epoch'].iloc[0]\n",
    "            nest['epoch_end'] = t['epoch'].iloc[-1]\n",
    "            streamids = list(t['streamid'])\n",
    "            streamids = \",\".join(streamids)\n",
    "            nest['streamids'] = streamids\n",
    "            meta_fields[topic_id] = nest\n",
    "        return meta_fields\n",
    "            \n",
    "    def store_instance(self, start_exec):\n",
    "        # get number of rows to get instance value\n",
    "        self.cursor.execute('SELECT COUNT(instance) FROM instances')\n",
    "        rowcount = self.cursor.fetchone()[0]\n",
    "        \n",
    "        # insert instance\n",
    "        self.cursor.execute('insert into instances values(?,?,?,?,?)', \n",
    "                  (rowcount, self.tech_name, self.is_temporal, start_exec, None))  # add end_exec later\n",
    "        return rowcount # return instance id for ease of later storage\n",
    "    \n",
    "    def update_end_exec(self, end_exec, instance_id):\n",
    "        self.cursor.execute('UPDATE instances SET end_exec=? WHERE instance=?', (end_exec, instance_id))\n",
    "        \n",
    "            \n",
    "    def store_topic_summary(self, topic_id, instance_id, summary):\n",
    "        self.cursor.execute('insert into meta values(?,?,?,?,?,?,?)',\n",
    "                  (topic_id, instance_id, summary, self.meta_columns[topic_id]['streamids'],\n",
    "                   self.meta_columns[topic_id]['epoch_start'], self.meta_columns[topic_id]['epoch_end'], self.is_temporal))\n",
    "        \n",
    "    def fetch_technique_entry(self):\n",
    "        self.cursor.execute('SELECT * FROM techniques WHERE name=?', (self.tech_name,))\n",
    "        entry = self.cursor.fetchone()\n",
    "        return entry\n",
    "        \n",
    "    def insert_technique(self, tech_descr=None):\n",
    "        entry = self.fetch_technique_entry()\n",
    "        if entry is None:  # technique not in database\n",
    "            if tech_descr is None:\n",
    "                raise ValueError(\"Tech description must not equal none if technique not in database\")\n",
    "            else:\n",
    "                self.cursor.execute('insert into techniques values (?, ?)', (self.tech_name, tech_descr))\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def cur_datetime(self):\n",
    "        time = datetime.now().strftime(\"%B %d, %Y %I:%M%p\")\n",
    "        return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (167) found smaller than n_clusters (205). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.45s/it]\u001b[A\u001b[A\n",
      " 11%|█         | 1/9 [00:17<02:21, 17.64s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (131) found smaller than n_clusters (173). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.43s/it]\u001b[A\u001b[A\n",
      " 22%|██▏       | 2/9 [00:32<01:57, 16.79s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (187) found smaller than n_clusters (221). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.02s/it]\u001b[A\u001b[A\n",
      " 33%|███▎      | 3/9 [00:51<01:45, 17.61s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (240) found smaller than n_clusters (447). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [01:13<00:00, 73.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:18<00:00, 18.69s/it]\u001b[A\u001b[A\n",
      " 44%|████▍     | 4/9 [02:24<03:20, 40.11s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (525) found smaller than n_clusters (946). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [04:50<00:00, 290.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [01:11<00:00, 71.98s/it]\u001b[A\u001b[A\n",
      " 56%|█████▌    | 5/9 [08:26<09:06, 136.72s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (285) found smaller than n_clusters (524). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [01:33<00:00, 93.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.67s/it]\u001b[A\u001b[A\n",
      " 67%|██████▋   | 6/9 [10:27<06:36, 132.06s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (223) found smaller than n_clusters (337). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:46<00:00, 46.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.29s/it]\u001b[A\u001b[A\n",
      " 78%|███████▊  | 7/9 [11:30<03:42, 111.28s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (273) found smaller than n_clusters (366). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:54<00:00, 54.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:16<00:00, 16.64s/it]\u001b[A\u001b[A\n",
      " 89%|████████▉ | 8/9 [12:42<01:39, 99.30s/it] \n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cluster/k_means_.py:972: ConvergenceWarning: Number of distinct clusters (141) found smaller than n_clusters (180). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarising text in 1 pieces.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.67s/it]\u001b[A\u001b[A\n",
      "100%|██████████| 9/9 [12:57<00:00, 74.04s/it]\n",
      "100%|██████████| 9/9 [12:57<00:00, 86.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Full corpus caused float divison by zero error at new_ratios.append(size_split/self.total_length(s))\n",
    "\n",
    "def small_corpus(start_topic, end_topic):\n",
    "    t_dfs = []\n",
    "    topic_ids = corpus['topic_id'].unique()\n",
    "    for topic_id in topic_ids:\n",
    "        t_dfs.append(corpus[corpus['topic_id'] == topic_id][start_topic:end_topic])\n",
    "    small = pd.concat(t_dfs)\n",
    "    return small\n",
    "\n",
    "test_feeder_name = \"bes_naive_datasplit_[0:5]\"\n",
    "test_feeder_descr = \"\"\"\n",
    "Using bert-extractive_summarizer with original naive datasplit.\n",
    "Uses first 5 documents of each topic\n",
    "Split entire topic into portions above 0.1 ratio, summarise iteratively\n",
    "\"\"\"\n",
    "test_feeder = SummaryFeeder(SummarizationHandler(sum_model), test_feeder_name, tech_descr=test_feeder_descr)\n",
    "\n",
    "first_20 = small_corpus(0, 5)\n",
    "test_feeder.summarize_topics(first_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "\n",
    "# db_dir = '/nfs/proj-repo/AAARG-dissertation'\n",
    "# db_name = 'sumresults.db'\n",
    "# db_path = db_dir + '/' + db_name\n",
    "\n",
    "# conn = sqlite3.connect(db_path)  # creates db if doesn't exist\n",
    "# c = conn.cursor()  # allows send commands to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.execute(\"\"\"CREATE TABLE results (\n",
    "#     topic_id integer,\n",
    "#     summary text\n",
    "# )\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

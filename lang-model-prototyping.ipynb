{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on current discoveries:\n",
    "\n",
    "- All files in storage claim are accessed with the base address '/nfs/'\n",
    "- Target files are HTML wrapped in .gz, use gzip to unzip\n",
    "- The inner text of <doc> is stored as the None tag where tag.string != '\\n' (is more than just a newline character\n",
    "- .string of <doc>'s inner text parses inner <a> tags etc. into just their inner text\n",
    "\n",
    "### Ideas for language model\n",
    "- NLTK\n",
    "- BERT\n",
    "\n",
    "- Load data set -> preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-langdetect\n",
      "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\n",
      "Collecting langdetect==1.0.7\n",
      "  Downloading langdetect-1.0.7.zip (998 kB)\n",
      "\u001b[K     |████████████████████████████████| 998 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest\n",
      "  Downloading pytest-6.1.2-py3-none-any.whl (272 kB)\n",
      "\u001b[K     |████████████████████████████████| 272 kB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect==1.0.7->spacy-langdetect) (1.12.0)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)\n",
      "Collecting importlib-metadata>=0.12; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-2.0.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.9.0-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (19.1.0)\n",
      "Collecting pluggy<1.0,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest->spacy-langdetect) (2.4.2)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993460 sha256=b17a1923a69513fa96ad65941c9e4d4c8dfd0ab4898c271a7631d20fa77be491\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/fc/78/21a8a2e77805c9f5d82a50188f3e10473f7028a470b6a976b1\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect, toml, packaging, zipp, importlib-metadata, iniconfig, py, pluggy, pytest, spacy-langdetect\n",
      "Successfully installed importlib-metadata-2.0.0 iniconfig-1.1.1 langdetect-1.0.7 packaging-20.4 pluggy-0.13.1 py-1.9.0 pytest-6.1.2 spacy-langdetect-0.1.2 toml-0.10.2 zipp-3.4.0\n",
      "Collecting language-detector\n",
      "  Downloading language-detector-5.0.2.tar.gz (6.6 kB)\n",
      "Building wheels for collected packages: language-detector\n",
      "  Building wheel for language-detector (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for language-detector: filename=language_detector-5.0.2-py3-none-any.whl size=7051 sha256=87761f2891981fc6dca04764808bde7062600193cf824468888193c2002f27e6\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f2/36/2022432385b7ec574674a9cb43e00f47c1dcb60d381d755d1a\n",
      "Successfully built language-detector\n",
      "Installing collected packages: language-detector\n",
      "Successfully installed language-detector-5.0.2\n",
      "Collecting symspellpy\n",
      "  Downloading symspellpy-6.7.0-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.0)\n",
      "Installing collected packages: symspellpy\n",
      "Successfully installed symspellpy-6.7.0\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-0.3.8.tar.gz (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 2.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting transformers<3.4.0,>=3.1.0\n",
      "  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.35.0)\n",
      "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.21.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.3.1)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2.22.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 20.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.10.28-cp36-cp36m-manylinux2010_x86_64.whl (666 kB)\n",
      "\u001b[K     |████████████████████████████████| 666 kB 33.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 32.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (20.4)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Downloading tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 43.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.13.2)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 4.5 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.4.2)\n",
      "Building wheels for collected packages: sentence-transformers, nltk, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-py3-none-any.whl size=101994 sha256=d1cae5280a91785cfdd3ff6e60547ba08eb31691e4f268c4644808547aeeccb6\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/cc/02/f731b61e1a4c97c248a9dbf7478f207f9ddede5d10f2ff6b34\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434677 sha256=9f7ddc1580988e64312df71ba35eb77a4186cd7459a951b89fc9a6fcc8c1d57b\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=d1a84047f291a094a0daf02aec8e9d93d0e72d9fd072cc69ee5a61bc05ccb761\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sentence-transformers nltk sacremoses\n",
      "Installing collected packages: regex, click, sacremoses, sentencepiece, dataclasses, tokenizers, transformers, nltk, sentence-transformers\n",
      "Successfully installed click-7.1.2 dataclasses-0.7 nltk-3.5 regex-2020.10.28 sacremoses-0.0.43 sentence-transformers-0.3.8 sentencepiece-0.1.94 tokenizers-0.8.1rc2 transformers-3.3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lxml\n",
      "  Downloading lxml-4.6.1-cp36-cp36m-manylinux1_x86_64.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 14.8 MB/s eta 0:00:01 eta 0:00:01MB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\"\n",
      "  Downloading soupsieve-2.0.1-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, lxml\n",
      "Successfully installed beautifulsoup4-4.9.3 lxml-4.6.1 soupsieve-2.0.1\n",
      "successfully installed packages\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy-langdetect\n",
    "!pip install language-detector\n",
    "!pip install symspellpy\n",
    "!pip install sentence-transformers\n",
    "!pip install beautifulsoup4 lxml  # html/xml parser\n",
    "print(\"successfully installed packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data set dependencies successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "print (\"loading data set dependencies successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET FILE META VARIABLES\n",
    "\n",
    "corpus_path = \"/nfs/trects-kba2014-filtered\" # directory of corpus of gzipped html files\n",
    "topics_path = corpus_path + \"/test-topics.xml\"\n",
    "doc_tags = ['topic_id','streamid', 'docid', 'yyyymmddhh', 'kbastream', 'zulu', 'epoch', 'title', 'text', 'url'] # doc fields\n",
    "topic_tags = ['id', 'title', 'description', 'start','end','query','type'] # topic fields\n",
    "#test_file_addr = \"/1/2012-02-22-15.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and get beautifulsoup object from markup file\n",
    "def open_markup_file(addr, gz=True, xml=False, verbose=False):\n",
    "    markup = None\n",
    "    f = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(addr)\n",
    "\n",
    "    if gz:\n",
    "        f = gzip.open(addr)\n",
    "    else:\n",
    "        f = open(addr)\n",
    "        \n",
    "    if xml == False:\n",
    "        markup = bs(f)  # open as html\n",
    "    else:\n",
    "        markup = bs(f, \"xml\")\n",
    "        \n",
    "    f.close()\n",
    "    return markup\n",
    "\n",
    "\n",
    "# parse markup and return 2D list [entry:tags]\n",
    "def parse_markup(markup, entry_list, find_tag=\"doc\", tag_list=doc_tags, topic_id=None):\n",
    "    for e in markup.find_all(find_tag):\n",
    "        entry = OrderedDict.fromkeys(tag_list)\n",
    "        if topic_id is not None:\n",
    "            entry['topic_id'] = topic_id\n",
    "        for c in e.children:  # children use direct children, descendants uses all\n",
    "            if c.name in entry:\n",
    "                entry[c.name] = c.string\n",
    "            elif c.name is None and c.string != '\\n':  # inner body of <doc> tag\n",
    "                entry['text'] = c.string\n",
    "#             elif c.name is not None:\n",
    "#                 print(\"Entry has unexpected field: \" + str(c.name))\n",
    "#                 print(\"parent field: \" + c.parent.name)\n",
    "#                 print(\"field content: \" + str(c))\n",
    "        \n",
    "#         missing_fields = [k for (k,v) in entry.items() if v == None]\n",
    "#         for m in missing_fields:\n",
    "#             print(\"Entry is missing field: \" + m)\n",
    "#             try:\n",
    "#                 for (k,v) in entry.items():\n",
    "#                     print(k,v)\n",
    "#             except:\n",
    "#                 print(\"could not print entry dictionary\")\n",
    "        entry_list.append(list(entry.values()))\n",
    "        \n",
    "            \n",
    "# recursively find gz html files from a directory address\n",
    "def search_dir(path):    \n",
    "    # separate the subdirectories and html files \n",
    "    # (help maintain sequential order of insertion)\n",
    "#     subdirs = []\n",
    "    gz_paths = []\n",
    "    for f in os.scandir(path):\n",
    "#         if f.is_dir():\n",
    "#             subdirs.append(f.path)\n",
    "        if os.path.splitext(f.path)[-1].lower() == \".gz\":\n",
    "            gz_paths.append(f.path)\n",
    "    \n",
    "#     # search subdirs\n",
    "#     for sd in subdirs:\n",
    "#         search_dir(sd, gz_paths)\n",
    "    return gz_paths\n",
    "\n",
    "\n",
    "def list_to_dataframe(markup_list, tags):\n",
    "    return pd.DataFrame(markup_list, columns=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topics into dataframe\n",
    "def load_topics(path):\n",
    "    topics_list = []\n",
    "    \n",
    "    parse_markup(open_markup_file(path, gz=False, xml=True), \n",
    "                    topics_list, find_tag=\"event\", tag_list=topic_tags)\n",
    "    \n",
    "    \n",
    "    return  list_to_dataframe(topics_list, topic_tags)\n",
    "\n",
    "topics = load_topics(topics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics loaded successfuly\n",
      "  id                                title  \\\n",
      "0  1      2012 Buenos Aires Rail Disaster   \n",
      "1  2  2012 Pakistan garment factory fires   \n",
      "2  3                 2012 Aurora shooting   \n",
      "3  4       Wisconsin Sikh temple shooting   \n",
      "\n",
      "                                         description       start         end  \\\n",
      "0  http://en.wikipedia.org/wiki/2012_Buenos_Aires...  1329910380  1330774380   \n",
      "1  http://en.wikipedia.org/wiki/2012_Pakistan_gar...  1347368400  1348232400   \n",
      "2  http://en.wikipedia.org/wiki/2012_Aurora_shooting  1342766280  1343630280   \n",
      "3  http://en.wikipedia.org/wiki/Wisconsin_Sikh_te...  1344180300  1345044300   \n",
      "\n",
      "                      query      type  \n",
      "0  buenos aires train crash  accident  \n",
      "1     pakistan factory fire  accident  \n",
      "2         colorado shooting  shooting  \n",
      "3      sikh temple shooting  shooting  \n"
     ]
    }
   ],
   "source": [
    "print(\"Topics loaded successfuly\")\n",
    "print(topics.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 239/241 [01:24<00:00,  4.81it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 241/241 [01:24<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# load all formatted gzipped html files into dataframe\n",
    "def load_corpus(path):\n",
    "    corpus_list = []\n",
    "    gz_paths = []\n",
    "    for topic_id in topics['id'].to_numpy():\n",
    "        id_path = corpus_path + \"/\" + topic_id + \"/\"  # every topic id correlates to subfolder named after it\n",
    "        gz_paths = search_dir(id_path)\n",
    "    for gz_path in tqdm(gz_paths, position=0, leave=True):\n",
    "        parse_markup(open_markup_file(gz_path, verbose=False),\n",
    "                        corpus_list, topic_id=topic_id)\n",
    "#     for topic_id in topics['id'].to_numpy():\n",
    "#         id_path = corpus_path + \"/\" + topic_id + \"/\"  # every topic id correlates to subfolder named after it\n",
    "#         gz_paths = search_dir(id_path)\n",
    "#         for gz_path in gz_paths:\n",
    "#             parse_markup(open_markup_file(gz_path, verbose=True),\n",
    "#                             corpus_list, topic_id=topic_id)\n",
    "    return list_to_dataframe(corpus_list, doc_tags)\n",
    "\n",
    "corpus = load_corpus(corpus_path)\n",
    "#print(\"Corpus loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded succesfully: 1578 documents loaded.\n",
      "  topic_id                                     streamid  \\\n",
      "0       10  1354113657-a4417f055ea5ae84207a4edb4dad881b   \n",
      "1       10  1354112039-110cc86ea7a8a1b58306dfade5b300ec   \n",
      "2       10  1354114192-a4417f055ea5ae84207a4edb4dad881b   \n",
      "3       10  1354114426-6c8d58d994c0e3243ee8dca8f34516a4   \n",
      "\n",
      "                              docid     yyyymmddhh        kbastream  \\\n",
      "0  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "1  110cc86ea7a8a1b58306dfade5b300ec  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "2  a4417f055ea5ae84207a4edb4dad881b  2012-11-28-14  MAINSTREAM_NEWS   \n",
      "3  6c8d58d994c0e3243ee8dca8f34516a4  2012-11-28-14           WEBLOG   \n",
      "\n",
      "                     zulu       epoch  \\\n",
      "0  2012-11-28T14:40:57.0Z  1354113657   \n",
      "1  2012-11-28T14:13:59.0Z  1354112039   \n",
      "2  2012-11-28T14:49:52.0Z  1354114192   \n",
      "3  2012-11-28T14:53:46.0Z  1354114426   \n",
      "\n",
      "                                               title  \\\n",
      "0  Morning Briefing: Support grows for bid by Pal...   \n",
      "1  BBC News - Egypt crisis: Appeals courts launch...   \n",
      "2  Morning Briefing: Support grows for bid by Pal...   \n",
      "3  Egypt News - Bodybuilder sets record for world...   \n",
      "\n",
      "                                                text  \\\n",
      "0  \\nMorning Briefing: Support grows for bid by P...   \n",
      "1  \\nBBC News - Egypt crisis: Appeals courts laun...   \n",
      "2  \\nMorning Briefing: Support grows for bid by P...   \n",
      "3  \\nEgypt News - Bodybuilder sets record for wor...   \n",
      "\n",
      "                                                 url  \n",
      "0  http://www.theglobeandmail.com/news/morning-br...  \n",
      "1  http://www.bbc.co.uk/news/world-middle-east-20...  \n",
      "2  http://www.theglobeandmail.com/news/morning-br...  \n",
      "3  http://www.egyptnews.net/index.php/sid/2110165...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Corpus loaded succesfully: \" + str(len(corpus)) + \" documents loaded.\")\n",
    "print(corpus.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing dependencies import successful\n"
     ]
    }
   ],
   "source": [
    "## IMPORT DEPENDENCIES\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from language_detector import detect_language\n",
    "\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "print(\"preprocessing dependencies import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPELL CHECKER\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
